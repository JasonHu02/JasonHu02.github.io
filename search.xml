<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>RNN，LSTM算法图解</title>
      <link href="/2020/02/08/lstm-and-rnn/"/>
      <url>/2020/02/08/lstm-and-rnn/</url>
      
        <content type="html"><![CDATA[<h4 id="RNN的工作原理："><a href="#RNN的工作原理：" class="headerlink" title="RNN的工作原理："></a>RNN的工作原理：</h4><p>RNN可以被认为是一种有记忆力的neural network。在RNN里面，每一次hidden layer的neuron产生output的时候，这个output会被存到memory里面。当下次又有input的时候，需要和上一次的memory结合起来使用。对它来说除了$x_1,x_2$以外，这些存在memory里的值$a_1,a_2$也会影响它的output。</p><img src="LSTM-and-RNN/image-20200226154228719.png" alt="image-20200226154228719" style="zoom: 25%;"><p>如上图所示, 用Recurrent Neural Network处理slot filling这件事，就像是这样，使用者说：“arrive Taipei on November 2nd”，arrive就变成了一个vector丢到neural network里面去，neural network的hidden layer的output写成$a^1$($a^1$是一排neural的output，是一个vector)，$a^1$产生$y^1$,$y^1$就是“arrive”属于每一个slot filling的几率。接下来$a^1$会被存到memory里面去，”Taipei会变为input”，这个hidden layer会同时考虑“Taipei”这个input和存在memory里面的$a^1$,得到$a^2$，根据$a^2$得到$y^2$，$y^2$是属于每一个slot filling的几率。以此类推($a^3$得到$y^2$)。</p><p>通过这样的方式，RNN就有了推断一句话意思的能力，他不仅仅看到了taibei这个词，也记住了arrive这个词，就可以做自动化做更多的事情。</p><h4 id="LSTM的网络架构："><a href="#LSTM的网络架构：" class="headerlink" title="LSTM的网络架构："></a>LSTM的网络架构：</h4><img src="LSTM-and-RNN/image-20200226154152676.png" alt="image-20200226154152676" style="zoom:25%;"><p>LSTM的网络架构，相比之下复杂了很多，有四个input，一个output，这构成了这个神经网络。分别是一个是想要存进memory的值，input-gate, forget-gate, output-gate，以及output。这个值输出，那么必须保证input_gate打开，forget_gate不会忘掉，且output_gate开启，而这三个值都会由neural network学到是1还是0. </p><p>我们看一下下面这个已经赋值好的权重的神经网络的输入和输出情况。</p><img src="LSTM-and-RNN/image-20200226174241356.png" alt="image-20200226174241356" style="zoom:25%;"><p>input的三维vector乘以linear transform以后所得到的结果($x_1$,$x_2$,$x_3$乘以权重再加上bias)，这些权重和bias是哪些值是通过train data用GD学到的。 假设我已经知道这些值是多少了，那用这样的输入会得到什么样的输出。那我们就实际的运算一下。其中绿色的就是bias，</p><p>在实际运算之前，我们先根据它的input，参数分析下可能会得到的结果。底下这个外界传入的cell，$x_1$乘以1，其他的vector乘以0，所以就直接把$x_1$当做输入。在input gate时，$x_2$乘以100，bias乘以-10(假设$x_2$是没有值的话，通常input gate是关闭的(bias等于-10)因为-10通过sigmoid函数之后会接近0，所以就代表是关闭的，若$x_2$的值大于1的话，结果会是一个正值，代表input gate会被打开) 。forget gate通常会被打开的，因为他的bias等于10(它平常会一直记得东西)，只有当$x_2$的值为一个很大的负值时，才会把forget gate关起来。output gate平常是被关闭的，因为bias是一个很大的负值，若$x_3$有一个很大的正值的话，压过bias把output打开。</p><h4 id="LSTM的原理："><a href="#LSTM的原理：" class="headerlink" title="LSTM的原理："></a>LSTM的原理：</h4><img src="LSTM-and-RNN/image-20200226174701864.png" alt="image-20200226174701864" style="zoom:33%;"><p>现在的input($x_1,x_2$)会乘以不同的weight当做LSTM不同的输入(假设我们这个hidden layer只有两个neuron，但实际上是有很多的neuron)。input($x_1,x_2$)会乘以不同的weight会去操控output gate，乘以不同的weight操控input gate，乘以不同的weight当做底下的input，乘以不同的weight当做forget gate。第二个LSTM也是一样的。所以LSTM是有四个input跟一个output，对于LSTM来说，这四个input是不一样的。在原来的neural network里是一个input一个output。在LSTM里面它需要四个input，它才能产生一个output。</p><p>LSTM的最终形态：</p><img src="LSTM-and-RNN/image-20200226174805623.png" alt="image-20200226174805623" style="zoom:33%;"><p>其代码实现如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#single lstm cell</span><span class="token keyword">def</span> <span class="token function">lstm_cell</span><span class="token punctuation">(</span>batch_dataset<span class="token punctuation">,</span> prev_activation_matrix<span class="token punctuation">,</span> prev_cell_matrix<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true">#get parameters</span>    fgw <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">'fgw'</span><span class="token punctuation">]</span>    igw <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">'igw'</span><span class="token punctuation">]</span>    ogw <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">'ogw'</span><span class="token punctuation">]</span>    ggw <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">'ggw'</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true">#concat batch data and prev_activation matrix</span>    concat_dataset <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_dataset<span class="token punctuation">,</span>prev_activation_matrix<span class="token punctuation">)</span><span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#forget gate activations</span>    fa <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>concat_dataset<span class="token punctuation">,</span>fgw<span class="token punctuation">)</span>    fa <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>fa<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#input gate activations</span>    ia <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>concat_dataset<span class="token punctuation">,</span>igw<span class="token punctuation">)</span>    ia <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>ia<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#output gate activations</span>    oa <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>concat_dataset<span class="token punctuation">,</span>ogw<span class="token punctuation">)</span>    oa <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>oa<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#gate gate activations</span>    ga <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>concat_dataset<span class="token punctuation">,</span>ggw<span class="token punctuation">)</span>    ga <span class="token operator">=</span> tanh_activation<span class="token punctuation">(</span>ga<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#new cell memory matrix</span>    cell_memory_matrix <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>fa<span class="token punctuation">,</span>prev_cell_matrix<span class="token punctuation">)</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>ia<span class="token punctuation">,</span>ga<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#current activation matrix</span>    activation_matrix <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>oa<span class="token punctuation">,</span> tanh_activation<span class="token punctuation">(</span>cell_memory_matrix<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#lets store the activations to be used in back prop</span>    lstm_activations <span class="token operator">=</span> dict<span class="token punctuation">(</span><span class="token punctuation">)</span>    lstm_activations<span class="token punctuation">[</span><span class="token string">'fa'</span><span class="token punctuation">]</span> <span class="token operator">=</span> fa    lstm_activations<span class="token punctuation">[</span><span class="token string">'ia'</span><span class="token punctuation">]</span> <span class="token operator">=</span> ia    lstm_activations<span class="token punctuation">[</span><span class="token string">'oa'</span><span class="token punctuation">]</span> <span class="token operator">=</span> oa    lstm_activations<span class="token punctuation">[</span><span class="token string">'ga'</span><span class="token punctuation">]</span> <span class="token operator">=</span> ga    <span class="token keyword">return</span> lstm_activations<span class="token punctuation">,</span>cell_memory_matrix<span class="token punctuation">,</span>activation_matrix<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">output_cell</span><span class="token punctuation">(</span>activation_matrix<span class="token punctuation">,</span>parameters<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true">#get hidden to output parameters</span>    how <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">'how'</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true">#get outputs </span>    output_matrix <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>activation_matrix<span class="token punctuation">,</span>how<span class="token punctuation">)</span>    output_matrix <span class="token operator">=</span> softmax<span class="token punctuation">(</span>output_matrix<span class="token punctuation">)</span>    <span class="token keyword">return</span> output_matrix<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://www.youtube.com/watch?v=xCGidAeyS4M" target="_blank" rel="noopener">主要参考了台大李宏毅老师的youtube视频课程。</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Few shot learning 少样本图像生成任务</title>
      <link href="/2020/02/06/few-shot-image-generation-method/"/>
      <url>/2020/02/06/few-shot-image-generation-method/</url>
      
        <content type="html"><![CDATA[<h4 id="Background："><a href="#Background：" class="headerlink" title="Background："></a>Background：</h4><p>few-shot learning问题简单来说，就是通过减少学习的次数来获得有用的知识。我们以传统的image classification为例，对于传统的问题，我们经过大量的样本对模型进行训练从而增强它的泛化能力。也就是给machine看了很多张图片，让它区分开不同类别图片的区别。这和人学习很不一样，人只需要非常少的图片就可以区分图片里的是猫还是狗。few–shot learning就是给少量的训练样本，也能够具有比较好的泛化能力。如何达到呢？这类问题我们使用解决meta-learning的方式去处理。可以参见<a href="https://jasonhu02.github.io/2020/01/12/meta-learning/">meta-learning tutorial</a>中给出的数学定义。</p><p>这里用通俗的话来讲，过去的机器学习只是教会了机器分类猫和狗(CNN结构的参数 $\theta$ )，meta-learning教会了机器如何分类，这是一个更大框架的参数 $\phi$ 。这样相当于训练了多个 $\theta$ ，然后根据 $\theta$ 的训练情况，得到最好的 $\phi$ (可以认为 $\phi$ 控制 $\theta$  , $\phi=f(\theta)$ )，这就是完整训练的过程。将得到的参数 $\phi$ 去测试集上测试，给几张原来没看过的image，丢到网络里面训练，网络生成的 $\theta$ 同时会受到 $\phi$ 的控制， ==可以发现可以很快的区分原来没有学习过的image。== 并且能够正确的识别出他们的label。</p><p>MAML和Reptiles都是解决上述问题比较好的方法。现在把meta-learning的问题用来解决生成问题。通常来说 生成image有三种方法：pixelCNN，VAE和GAN。VAE曾经在以前的文章中介绍过，实质上是衡量的是真实样本和生成样本的KL-divergence。VAE仍然有很多问题需要解决 ，而GAN有更好的生成特性。</p><h4 id="Method："><a href="#Method：" class="headerlink" title="Method："></a>Method：</h4><p>本文使用latent vector作为input，使用reptile的网络架构，将GAN的generator来生成逼真的图像。GAN由generator和discriminator组成。基本思想通俗来说，用Reptiles的方式来训练Generator和Discriminator，得到的 $\phi_{\mathcal{d}}$ 和 $\phi_{\mathcal{g}}$, $W_{d}$ 和$W_{g}$，使用Wasserstein GP loss进行训练，先进行内层循环K次，得到最佳的 $W_{d}$ 和 $W_{g}$ 。通过$\phi_{\mathcal{d}}-W_{d}$ 和 $\phi_{\mathcal{g}}-W_{g}$ 来更新参数。其loss function为<br>$$<br>\text {minimize} \sum_{T}\left(\Phi_{d}-W_{d \tau}\right)+\left(\Phi_{g}-W_{g \tau}\right)<br>$$<br>训练的流程如下：</p><img src="few shot image generation method/image-20200223142531081.png" alt="image-20200223142531081" style="zoom:33%;"><h4 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment:"></a>Experiment:</h4><p>构造一个ResNetGenerator和ResNetDiscriminator。再分别构造inner_loop和meta_training_loop</p><p>核心代码如下：</p><p>inner_loop:</p><pre class="line-numbers language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">inner_loop</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> real_batch<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>meta_g<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>        fake_batch <span class="token operator">=</span> self<span class="token punctuation">.</span>meta_g<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>z_shape<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>        training_batch <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>real_batch<span class="token punctuation">,</span> fake_batch<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Training discriminator</span>        gradient_penalty <span class="token operator">=</span> calc_gradient_penalty<span class="token punctuation">(</span>self<span class="token punctuation">.</span>meta_d<span class="token punctuation">,</span> real_batch<span class="token punctuation">,</span> fake_batch<span class="token punctuation">)</span>        discriminator_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>meta_d<span class="token punctuation">(</span>training_batch<span class="token punctuation">)</span>        discriminator_loss <span class="token operator">=</span> wassertein_loss<span class="token punctuation">(</span>discriminator_pred<span class="token punctuation">,</span> self<span class="token punctuation">.</span>discriminator_targets<span class="token punctuation">)</span>        discriminator_loss <span class="token operator">+=</span> gradient_penalty        self<span class="token punctuation">.</span>meta_d_optim<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        discriminator_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>meta_d_optim<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Training generator</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>meta_d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>meta_g<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>z_shape<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        generator_loss <span class="token operator">=</span> wassertein_loss<span class="token punctuation">(</span>output<span class="token punctuation">,</span> self<span class="token punctuation">.</span>generator_targets<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>meta_g_optim<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        generator_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>meta_g_optim<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> discriminator_loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> generator_loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>meta_training_loop:</p><pre class="line-numbers language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">meta_training_loop</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        data<span class="token punctuation">,</span> task <span class="token operator">=</span> self<span class="token punctuation">.</span>env<span class="token punctuation">.</span>sample_training_task<span class="token punctuation">(</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>        data <span class="token operator">=</span> normalize_data<span class="token punctuation">(</span>data<span class="token punctuation">)</span>        real_batch <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        discriminator_total_loss <span class="token operator">=</span> <span class="token number">0</span>        generator_total_loss <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>inner_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>            disc_loss<span class="token punctuation">,</span> gen_loss <span class="token operator">=</span> self<span class="token punctuation">.</span>inner_loop<span class="token punctuation">(</span>real_batch<span class="token punctuation">)</span>            discriminator_total_loss <span class="token operator">+=</span> disc_loss            generator_total_loss <span class="token operator">+=</span> gen_loss        self<span class="token punctuation">.</span>writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Training_discriminator_loss'</span><span class="token punctuation">,</span> discriminator_total_loss<span class="token punctuation">,</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Training_generator_loss'</span><span class="token punctuation">,</span> generator_total_loss<span class="token punctuation">,</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epochs:{},Train_D_loss:{},Train_G_loss:{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>self<span class="token punctuation">.</span>eps<span class="token punctuation">,</span>discriminator_total_loss<span class="token punctuation">,</span>generator_total_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># Updating both generator and dicriminator</span>        <span class="token keyword">for</span> p<span class="token punctuation">,</span> meta_p <span class="token keyword">in</span> zip<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>self<span class="token punctuation">.</span>meta_g<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            diff <span class="token operator">=</span> p <span class="token operator">-</span> meta_p<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span>            p<span class="token punctuation">.</span>grad <span class="token operator">=</span> diff        self<span class="token punctuation">.</span>g_optim<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> p<span class="token punctuation">,</span> meta_p <span class="token keyword">in</span> zip<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>meta_d<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            diff <span class="token operator">=</span> p <span class="token operator">-</span> meta_p<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span>            p<span class="token punctuation">.</span>grad <span class="token operator">=</span> diff        self<span class="token punctuation">.</span>d_optim<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到meta_training_loop调用了inner_loop，计算了p - meta_p和p - meta_p的loss，并进行了更新。</p><p>真实的training过程</p><pre class="line-numbers language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">training</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">while</span> self<span class="token punctuation">.</span>eps <span class="token operator">&lt;=</span> <span class="token number">10000</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>reset_meta_model<span class="token punctuation">(</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>meta_training_loop<span class="token punctuation">(</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>eps <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token comment" spellcheck="true"># Validation run every 10000 training loop</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>eps <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                self<span class="token punctuation">.</span>reset_meta_model<span class="token punctuation">(</span><span class="token punctuation">)</span>                gan_score<span class="token operator">=</span>self<span class="token punctuation">.</span>validation_run<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>gan_score_total<span class="token punctuation">.</span>append<span class="token punctuation">(</span>gan_score<span class="token punctuation">)</span>                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"valscore_total:{}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gan_score_total<span class="token punctuation">)</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>checkpoint_model<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#            self.eps += 1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Result："><a href="#Result：" class="headerlink" title="Result："></a>Result：</h4><p>最后生成的效果非常好。Mnist上的测试结果：</p><img src="few shot image generation method/image-20200223144618093.png" alt="image-20200223144618093" style="zoom:50%;"><p>Omniglot上的测试结果：</p><img src="few shot image generation method/image-20200223144638634.png" alt="image-20200223144638634" style="zoom:50%;">]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> meta-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VAE-GAN的一些理解</title>
      <link href="/2020/02/02/vae-gan/"/>
      <url>/2020/02/02/vae-gan/</url>
      
        <content type="html"><![CDATA[<p>CVAE-GAN这篇论文有一些不错的insight可以拿过来使用。</p><p>VAE中的encoder是一个神经网路结构用来生成数据样本X中的方差和均值。根据方差和均值得到一个正态分布，从这个分布中采样一个z，该正态分布与标准正态分布之间有一个转换关系，$z=uz<em>+sigma $, 通过decoder这个神经网络生成新的$x</em>$。这个$x*$可以用来表示新的图像。</p><p>衡量decoder和encoder之间的metric是通过用KL-divergence来表示Loss function，具体的推导过程如下，</p><img src="VAE_GAN/image-20200218140108115.png" alt="image-20200218140108115" style="zoom:33%;"><img src="VAE_GAN/image-20200218140121734.png" alt="image-20200218140121734" style="zoom:33%;"><p> 理解VAE的本质：</p><p><strong>它本质上就是在我们常规的自编码器的基础上，对 encoder 的结果（在VAE中对应着计算均值的网络）加上了“高斯噪声”，使得结果 decoder 能够对噪声有鲁棒性；而那个额外的 KL loss（目的是让均值为 0，方差为 1），事实上就是相当于对 encoder 的一个正则项，希望 encoder 出来的东西均有零均值。</strong> </p><p>​    那另外一个 encoder（对应着计算方差的网络）的作用呢？它是用来<strong>动态调节噪声的强度</strong>的。直觉上来想，<strong>当 decoder 还没有训练好时（重构误差远大于 KL loss），就会适当降低噪声（KL loss 增加），使得拟合起来容易一些（重构误差开始下降）</strong>。反之，<strong>如果 decoder 训练得还不错时（重构误差小于 KL loss），这时候噪声就会增加（KL loss 减少），使得拟合更加困难了（重构误差又开始增加），这时候 decoder 就要想办法提高它的生成能力了</strong>。</p><p>在GAN中，其中的discriminator本质上也是一个metric的判别器，表示生成样本和真实样本之间的JS-divergence。但是GAN存在的问题是</p><p>对于cVAE来说，就是存在一个有监督的标签，通过这个标签，只需要将KL-divergence进行一下变换即可：</p><img src="VAE_GAN/image-20200218140135714.png" alt="image-20200218140135714" style="zoom:33%;"><hr><p>正式说VAE和GAN的结合：</p><p>论文链接： <a href="https://arxiv.org/pdf/1703.10155.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.10155.pdf</a> </p><img src="VAE_GAN/image-20200218140159343.png" alt="image-20200218140159343" style="zoom:50%;"><p>cvae-gan的具体实施架构</p><img src="VAE_GAN/image-20200218140217148.png" alt="image-20200218140217148" style="zoom: 50%;">]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> representation-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VAE的一些理解</title>
      <link href="/2020/02/02/shen-ru-li-jie-vae/"/>
      <url>/2020/02/02/shen-ru-li-jie-vae/</url>
      
        <content type="html"><![CDATA[<p>仔细去看了ICML2014的文章Auto-Encoding Variational Bayes，想重新把VAE在理论上推导一遍。</p><p>先了解auto-encoder的组成。 q(z) 是标准正态分布, p(z|x), q(x|z) 是条件正态分布，分别对应编码器、解码器。</p><p>这里通过直接对联合分布进行近似的方式，简明快捷地给出了 VAE 的理论框架。 出发点依然没变，这里再重述一下。首先我们有一批数据样本 {<em>x</em>1,…,<em>x</em>n}，其整体用 <em>x</em> 来描述，我们希望<strong>借助隐变量</strong> <strong><em>z</em></strong> <strong>描述</strong> <strong><em>x</em></strong> <strong>的分布</strong> $p(x)$：<br>$$<br>p(x)=\int p(x | z) p(z) d z, \quad p(x, z)=p(x | z) p(z)<br>$$</p><p>$$<br>KL(p(x, z) | q(x, z))=\iint p(x, z) \ln \frac{p(x, z)}{q(x, z)} d z d x<br>$$</p><p>这样（理论上）我们既描述了 <em>p</em>(<em>x</em>)，又得到了生成模型 <em>p</em>(<em>x</em>|<em>z</em>)，一举两得。</p><p>KL 散度是我们的终极目标，因为我们希望两个分布越接近越好，所以 KL 散度越小越好。由于我们手头上只有 <em>x</em> 的样本，因此利用 <em>p</em>(<em>x</em>,<em>z</em>)=<em>p</em>(<em>x</em>)<em>p</em>(<em>z</em>|<em>x</em>) 对上式进行改写：<br>$$<br>\begin{aligned} K L(p(x, z) | q(x, z)) &amp;=\int p(x)\left[\int p(z | x) \ln \frac{p(x, z)}{q(x, z)} d z\right] d x \\ &amp;=\mathbb{E}_{x \sim p(x)}\left[\int p(z | x) \ln \frac{p(z | x) p(x)}{q(x, z)} d z\right] \end{aligned}<br>$$<br>这样一来利用 (4) 式，把各个 <em>xi</em> 代入就可以进行计算了，这个式子还可以进一步简化，因为：<br>$$<br>\ln \frac{p(z | x) p(x)}{q(x, z)}=\ln \frac{p(z | x)}{q(x, z)}+\ln p(x)<br>$$<br>而<br>$$<br>\begin{aligned} \mathbb{E}_{x \sim p(x)}\left[\int p(z | x) \ln p(x) d z\right] &amp;=\mathbb{E}_{x \sim p(x)}\left[\ln p(x) \int p(z | x) d z\right] \\ &amp;=\mathbb{E}_{x \sim p(x)}[\ln p(x)] \end{aligned}<br>$$<br> 注意这里的 <em>p</em>(<em>x</em>) 是根据样本 <em>x</em>1,<em>x</em>2,…,<em>x</em>n 确定的关于 <em>x</em> 的先验分布（更常见的写法是 <em>p̃</em>(<em>x</em>)），尽管我们不一定能准确写出它的形式，但它是确定的、存在的，因此这一项只是一个常数，所以可以写出：<br>$$<br>\mathcal{L}=K L(p(x, z) | q(x, z))-constant=\mathbb{E}_{x \sim p(x)}\left[\int p(z | x) \ln \frac{p(z | x)}{q(x, z)} d z\right]<br>$$<br> 目前最小化 KL(<em>p</em>(<em>x</em>,<em>z</em>)‖<em>q</em>(<em>x</em>,<em>z</em>)) 也就等价于最小化 L。注意减去的常数一般是负数（概率小于 1，取对数就小于 0），而 KL 散度本来就非负，非负数减去一个负数，结果会是一个正数，所以 L 恒大于一个某个正数。 </p><p>到这里，我们回顾初衷——为了得到生成模型，所以我们把 <em>q</em>(<em>x</em>,<em>z</em>) 写成 <em>q</em>(<em>x</em>|<em>z</em>)<em>q</em>(<em>z</em>)，于是就有：<br>$$<br>\begin{aligned} \mathcal{L} &amp;=\mathbb{E}_{x \sim p(x)}\left[\int p(z | x) \ln \frac{p(z | x)}{q(x | z) q(z)} d z\right] \\ &amp;=\mathbb{E}_{x \sim p(x)}\left[-\int p(z | x) \ln q(x | z) d z+\int p(z | x) \ln \frac{p(z | x)}{q(z)} d z\right] \end{aligned}<br>$$<br> 再简明一点，那就是：<br>$$<br>\begin{aligned} \mathcal{L} &amp;=\mathbb{E}_{x \sim p(x)}\left[\mathbb{E}_{z \sim p(z | x)}[-\ln q(x | z)]+\mathbb{E}_{z \sim p(z | x)}\left[\ln \frac{p(z | x)}{q(z)}\right]\right] \\ &amp;=\mathbb{E}_{x \sim p(x)}\left[\mathbb{E}_{z \sim p(z | x)}[-\ln q(x | z)]+K L(p(z | x) | q(z))\right] \end{aligned}<br>$$<br>看，括号内的不就是 VAE 的损失函数吗？只不过我们换了个符号而已。我们就是要想办法找到适当的 <em>q</em>(<em>x</em>|<em>z</em>) 和 <em>q</em>(<em>z</em>) 使得 L 最小化。 </p><p>简单来说，由于直接描述复杂分布是难以做到的，<strong>所以我们通过引入隐变量来将它变成条件分布的叠加</strong>。而这时候我们对隐变量的分布和条件分布都可以做适当的简化（比如都假设为正态分布），并且在条件分布的参数可以跟深度学习模型结合起来（用深度学习来算隐变量的参数），至此，“深度概率图模型”就可见一斑了。 </p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> representation-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ICLR2020论文解读__Learning robust representations via multi-view information</title>
      <link href="/2020/01/22/information-bottleneck-paper/"/>
      <url>/2020/01/22/information-bottleneck-paper/</url>
      
        <content type="html"><![CDATA[<p>本文是对于信息瓶颈方法的原始公式应用于在学习时可以使用任务特定标签的监督设置中。提出了通过利用multi-view的方式 提供一种实体的两类视图，可以将方法扩展到无监督的设置。本文通过理论分析得到了多视图的定义。还通过利用标准数据增强技术将理论扩展到单视图，比传统的无监督学习的方法相比，有更好的泛化能力。并在数据集Sketchy和MIR-flickr上进行了实验。</p><p><img src="image/image-20200217022428246.png" alt="image-20200217022428246"></p><p>表示了x和z之间的互信息分成三部份，<strong>第一部分表示为x和z之间的互信息在没有预测为y的情况下</strong>；第二部分<strong>x，y之间的互信息，是一个常数，该常数由原始观测值标签的信息来决定</strong>；减去<strong>第三部分，表示x编码为z丢失的和y有关的互信息</strong>。第二部分和第三部分的区别就是考虑了z情况下的x，y互信息，包含于没有考虑x，y情况下的，所以减去。</p><h4 id="理论"><a href="#理论" class="headerlink" title="理论:"></a>理论:</h4><p>其中涉及到了几个定理和推论，文中均给出了详尽的证明，涉及到大量的信息论和概率的知识，必须借助维基百科()加上自己的手推才能完全理解本文的思路。在此直接列出所有结论。</p><p>定义1，充分性：</p><img src="image/image-20200217143840828.png" alt="image-20200217143840828" style="zoom: 50%;"><p>z对y充分定义为，I(x;y|z)在z条件下的y对x的互信息为0. 由该定义可以推出如下结论:<br>$$<br>I(\mathbf{x} ; \mathbf{y} | \mathbf{z})=0 \Longleftrightarrow I(\mathbf{x} ; \mathbf{y})=I(\mathbf{y} ; \mathbf{z})<br>$$<br>命题2.1：</p><p><img src="image/image-20200217150059055.png" alt="image-20200217150059055"></p><p>定义2，冗余性</p><p><img src="image/image-20200217150236253.png" alt="image-20200217150236253"></p><p>推论1：</p><p><img src="image/image-20200217150331149.png" alt="image-20200217150331149"></p><p>附录部分给出了非常详尽的证明，对任何理解上不到位的地方都可以去看数学推导。</p><h4 id="Related-Work："><a href="#Related-Work：" class="headerlink" title="Related Work："></a>Related Work：</h4><img src="image/image-20200217142805245.png" alt="image-20200217142805245" style="zoom: 50%;"><p>为了比较和其他模型的区别，文中这个图对作者使用的情况进行了详细的解释说明。infomax最大化互信息，来实现无监督学习。理想情况下，良好的表示形式将最大程度地提供有关标签的信息，同时保留来自观察结果的最少信息。也就是图中平行四边形左上方的顶点。从图中可以看到MIB模型是最接近最优解的，本文是第一篇明确指出在多视角无监督学习中丢弃冗余信息的一篇文章。</p><h4 id="实施方法"><a href="#实施方法" class="headerlink" title="实施方法:"></a>实施方法:</h4><p>论文的核心思想在这个图上：</p><img src="image/image-20200217024601501.png" alt="image-20200217024601501" style="zoom: 33%;"><p>在v1和v2两个视图上，分别得到编码得到z1和z2，通过比较两者的分布之间的平均KL散度，以及z1和z2之间的互信息来更新loss。</p><p>它的loss为全文核心：</p><p><img src="image/image-20200217024749741.png" alt="image-20200217024749741"></p><p>散度减去互信息，其表达了用冗余信息减去预测y充分性下的z1和z2的互信息，我们使得在z1|v1和z2|v2下的的KL散度最大化，即v1，v2呈现不同的视角使其给的信息更加无关，而最大化z1和z2之间的互信息，使得z1和z2的信息更加相关。这样的目的都是消除两个变量之间的相关性，也就是信息瓶颈的意思，让最有用的信息通过去，留下对预测没用的多余信息。本文的意图就是想方设法的使得两个不同分布的数据集关联度尽可能小，简单来讲就是让互信息尽可能小。</p><p>最后作者将MIB方法在Sketchy和Flickr数据集上与先前的多视图算法做比较。Sketchy数据集包含来自125个类别的12,500张图像和75,471张手绘草图，是两种信息量上差别很大的图。MIR-Flicker则是通过图像和文字结合，提供两种视角。最后的效果如图所示：分别在Sketchy和Flickr上的效果如下：</p><p>可以看到mv-infomax的实力也非常不错，所以文章主要就是和它在做对比。</p><img src="image/image-20200217030932182.png" alt="image-20200217030932182" style="zoom:33%;"><img src="image/image-20200217025835301.png" alt="image-20200217025835301" style="zoom:33%;"><p>我特地去view了代码，发现代码实现的方法非常简单，说明该方法从某一些理论性的角度解决了模型鲁棒性的问题，训练起来速度很快，有一定的参考价值。且论文作者丝毫不避讳地把实验中所有数据全部公开在论文附录里，看来是对论文地实验效果非常有信心，有足够地把握给读者看。总之，可以借鉴地点非常多。之后工作可以围绕他的思路做一些扩展了。</p><p>核心代码在这里：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> v_1<span class="token punctuation">,</span> v_2<span class="token punctuation">,</span> _ <span class="token keyword">in</span> train_loader<span class="token punctuation">:</span>        <span class="token keyword">if</span> cuda<span class="token punctuation">:</span>            v_1 <span class="token operator">=</span> v_1<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>            v_2 <span class="token operator">=</span> v_2<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Encode a batch of data</span>        p_z_1_given_v_1 <span class="token operator">=</span> encoder_v_1<span class="token punctuation">(</span>v_1<span class="token punctuation">)</span>        p_z_2_given_v_2 <span class="token operator">=</span> encoder_v_2<span class="token punctuation">(</span>v_2<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Sample from the posteriors with reparametrization</span>        z_1 <span class="token operator">=</span> p_z_1_given_v_1<span class="token punctuation">.</span>rsample<span class="token punctuation">(</span><span class="token punctuation">)</span>        z_2 <span class="token operator">=</span> p_z_2_given_v_2<span class="token punctuation">.</span>rsample<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Mutual information estimation</span>        mi_gradient<span class="token punctuation">,</span> mi_estimation <span class="token operator">=</span> mi_estimator<span class="token punctuation">(</span>z_1<span class="token punctuation">,</span>z_2<span class="token punctuation">)</span>        mi_gradient <span class="token operator">=</span> mi_gradient<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        mi_estimation <span class="token operator">=</span> mi_estimation<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Symmetrized Kullback-Leibler divergence</span>        kl_1_2 <span class="token operator">=</span> p_z_1_given_v_1<span class="token punctuation">.</span>log_prob<span class="token punctuation">(</span>z_1<span class="token punctuation">)</span> <span class="token operator">-</span> p_z_2_given_v_2<span class="token punctuation">.</span>log_prob<span class="token punctuation">(</span>z_1<span class="token punctuation">)</span>        kl_2_1 <span class="token operator">=</span> p_z_2_given_v_2<span class="token punctuation">.</span>log_prob<span class="token punctuation">(</span>z_2<span class="token punctuation">)</span> <span class="token operator">-</span> p_z_1_given_v_1<span class="token punctuation">.</span>log_prob<span class="token punctuation">(</span>z_2<span class="token punctuation">)</span>        skl <span class="token operator">=</span> <span class="token punctuation">(</span>kl_1_2 <span class="token operator">+</span> kl_2_1<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">.</span>        <span class="token comment" spellcheck="true"># Update the value of beta according to the policy</span>        beta <span class="token operator">=</span> beta_scheduler<span class="token punctuation">(</span>iterations<span class="token punctuation">)</span>        iterations <span class="token operator">+=</span><span class="token number">1</span>        <span class="token comment" spellcheck="true"># Computing the loss function</span>        loss <span class="token operator">=</span> <span class="token operator">-</span> mi_gradient <span class="token operator">+</span> beta <span class="token operator">*</span> skl        <span class="token comment" spellcheck="true"># Logging</span>        mi_over_time<span class="token punctuation">.</span>append<span class="token punctuation">(</span>mi_estimation<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        skl_over_time<span class="token punctuation">.</span>append<span class="token punctuation">(</span>skl<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Backward pass and update</span>        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Plot the loss components every 5 epochs</span>    <span class="token keyword">if</span> epoch <span class="token operator">%</span> plot_every <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'$I(z_1;z_2)$'</span><span class="token punctuation">)</span>        ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'$D_{SKL}(p(z_1|v_1)||p(z_2|v_2))$'</span><span class="token punctuation">)</span>        ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_yscale<span class="token punctuation">(</span><span class="token string">'log'</span><span class="token punctuation">)</span>        ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>mi_over_time<span class="token punctuation">,</span> <span class="token string">'.'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>        ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>skl_over_time<span class="token punctuation">,</span> <span class="token string">'.r'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>        ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylim<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span>        ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylim<span class="token punctuation">(</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>        f<span class="token punctuation">.</span>suptitle<span class="token punctuation">(</span><span class="token string">'Epoch: %d'</span><span class="token operator">%</span>epoch<span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">15</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Compute train and test_accuracy of a logistic regression</span>        train_accuracy<span class="token punctuation">,</span> test_accuracy <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>encoder<span class="token operator">=</span>encoder_v_1<span class="token punctuation">,</span> train_on<span class="token operator">=</span>train_subset<span class="token punctuation">,</span> test_on<span class="token operator">=</span>test_set<span class="token punctuation">,</span> cuda<span class="token operator">=</span>cuda<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Train Accuracy: %f'</span><span class="token operator">%</span> train_accuracy<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Test Accuracy: %f'</span><span class="token operator">%</span> test_accuracy<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="点评："><a href="#点评：" class="headerlink" title="点评："></a>点评：</h4><p>这是一篇很好的多视角表示学习论文，具有新的见解。learn variable z_1和z_2，它们是一致的，包含视角不变信息，但应尽可能丢弃特定于视角的信息。<br>本文依赖于相互信息估计，并且无需重构。在先前的一些工作中（例如Aaron van den Oord等人2018）中提到，重建损失会引入偏见，对学习的表征产生负面影响。与现有的尝试最大化学习的表示和视图之间的相互信息的多视图表示学习方法相比，本文明确定义了多余的信息，我们应该尝试抛弃这些多余的信息，并弄清楚如何获得足够的学习的表示用于输出。作者还得出了一些现有的（多视图）表示学习方法与他们提出的方法之间的明确联系。</p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> representation-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch-多GPU操作的方法</title>
      <link href="/2020/01/19/pytorch-parallel/"/>
      <url>/2020/01/19/pytorch-parallel/</url>
      
        <content type="html"><![CDATA[<h5 id="完整的步骤："><a href="#完整的步骤：" class="headerlink" title="完整的步骤："></a>完整的步骤：</h5><ol><li>设定好os.environment(“CUDA_VISIBEL_DEVICES”)=’0,1,2,3,4,5’</li><li>pytorch中定义一个变量device = torch.device(“cuda:0,1,2,3,4,5”）</li><li>把model使用model = nn.DataParallel(model)，用来分配数据，并且model.to(device)，将model加载到gpu上</li><li>将要训练的数据放入到device中，然后训练这个model就可以了！</li></ol><h5 id="实际操作："><a href="#实际操作：" class="headerlink" title="实际操作："></a>实际操作：</h5><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> os os<span class="token punctuation">.</span>environment<span class="token punctuation">(</span><span class="token string">"CUDA_VISIBEL_DEVICES"</span><span class="token punctuation">)</span><span class="token operator">=</span><span class="token string">'0,1,2,3,4,5'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>上述步骤相当于在shell 命令里面调用了 CUDA_VISIBEL_DEVICES 命令, 相当于打开了6个gpu的入口。分别指定了cuda0,cuda1,cuda2三个gpu进行计算。这里相当于调用了6个gpu</p><p>note：使用torch.cuda.device_count()看到的就是这里调用了几个gpu。</p><pre class="line-numbers language-python"><code class="language-python">device<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0,1,2,3,4,5"</span><span class="token punctuation">,</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这里必须和上面的数值保持一致，否则会报错。device的输出是 type: “cuda” ，index=0， 用index的值表示了cuda的个数</p><pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> Model<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span><span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Let's use"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"GPUs!"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs</span>  model <span class="token operator">=</span> nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>model<span class="token punctuation">)</span>model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用nn.DataParallel对model这个网络模型进行data上的自动分配</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> data <span class="token keyword">in</span> rand_loader<span class="token punctuation">:</span>    input <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    output <span class="token operator">=</span> model<span class="token punctuation">(</span>input<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Outside: input size"</span><span class="token punctuation">,</span> input<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          <span class="token string">"output_size"</span><span class="token punctuation">,</span> output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>将input从dataset中得到batch_size的数据，将data传给GPU中，对模型进行了训练(forward function这里省去)</p><p>这个时候cuda上就会被分配上数据。可以看到最后输出的结果：</p><pre><code>Let&#39;s use 2 GPUs!    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre><pre><code>Let&#39;s use 8 GPUs!    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre><p>可以看到model上的data被平均分配到了8个gpu上。</p><p>input_size=2, output_size=5, batch_size=30, data_size=100。</p><p>由于每次batch_size=30, 因此跑了4次，数据量为30+30+30+10</p><p>每次在模型中，30个input被分配到8个gpu上，4+4+4+4+4+2+4+4个，每个cuda上跑了4或2个input，output则相应输出对应的大小。[4,5]表示4个input，每个input的大小为5。</p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch-data处理的方法</title>
      <link href="/2020/01/18/pytorch-data/"/>
      <url>/2020/01/18/pytorch-data/</url>
      
        <content type="html"><![CDATA[<p>pytorch对数据处理的一些基本用法总结：</p><p>为了更好地分配数据，通常在训练中会用到pytorch的几个库，<code>torchvision.transforms</code> 和<code>torchvision.dataset.Imagefolder</code> 和 <code>torch.util.data.Dataloader</code> </p><p>用代码解释这三个函数的作用：</p><h4 id="transforms："><a href="#transforms：" class="headerlink" title="transforms："></a>transforms：</h4><p>transforms的作用一句话概括就是使得数据集里的数据统一化，比如对于图像数据，可能很多图像的尺寸不一样，需要对图像的大小进行裁剪和缩放，并对图像的大小尺寸进行统一。</p><p>tutorial中的几个参数：</p><ul><li><p><code>Rescale</code>: to scale the image</p></li><li><p><code>RandomCrop</code>: to crop from image randomly. This is data augmentation.</p></li><li><p><code>ToTensor</code>: to convert the numpy images to torch images (we need to swap axes).</p><p>note: numpy中的图像数据是 H × W × C ; 而torch.tensor的数据是C × H × W</p></li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Rescale</span><span class="token punctuation">(</span>obejct<span class="token punctuation">)</span>：     <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">assert</span> isinstance<span class="token punctuation">(</span>output_size<span class="token punctuation">,</span> <span class="token punctuation">(</span>int<span class="token punctuation">,</span> tuple<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>output_size <span class="token operator">=</span> output_size    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sample<span class="token punctuation">)</span><span class="token punctuation">:</span>        image<span class="token punctuation">,</span> landmarks <span class="token operator">=</span> sample<span class="token punctuation">[</span><span class="token string">'image'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sample<span class="token punctuation">[</span><span class="token string">'landmarks'</span><span class="token punctuation">]</span>        h<span class="token punctuation">,</span> w <span class="token operator">=</span> image<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_size<span class="token punctuation">,</span> int<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> h <span class="token operator">></span> w<span class="token punctuation">:</span>                new_h<span class="token punctuation">,</span> new_w <span class="token operator">=</span> self<span class="token punctuation">.</span>output_size <span class="token operator">*</span> h <span class="token operator">/</span> w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_size            <span class="token keyword">else</span><span class="token punctuation">:</span>                new_h<span class="token punctuation">,</span> new_w <span class="token operator">=</span> self<span class="token punctuation">.</span>output_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_size <span class="token operator">*</span> w <span class="token operator">/</span> h        <span class="token keyword">else</span><span class="token punctuation">:</span>            new_h<span class="token punctuation">,</span> new_w <span class="token operator">=</span> self<span class="token punctuation">.</span>output_size        new_h<span class="token punctuation">,</span> new_w <span class="token operator">=</span> int<span class="token punctuation">(</span>new_h<span class="token punctuation">)</span><span class="token punctuation">,</span> int<span class="token punctuation">(</span>new_w<span class="token punctuation">)</span>        img <span class="token operator">=</span> transform<span class="token punctuation">.</span>resize<span class="token punctuation">(</span>image<span class="token punctuation">,</span> <span class="token punctuation">(</span>new_h<span class="token punctuation">,</span> new_w<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># h and w are swapped for landmarks because for images,</span>        <span class="token comment" spellcheck="true"># x and y axes are axis 1 and 0 respectively</span>        landmarks <span class="token operator">=</span> landmarks <span class="token operator">*</span> <span class="token punctuation">[</span>new_w <span class="token operator">/</span> w<span class="token punctuation">,</span> new_h <span class="token operator">/</span> h<span class="token punctuation">]</span>        <span class="token keyword">return</span> <span class="token punctuation">{</span><span class="token string">'image'</span><span class="token punctuation">:</span> img<span class="token punctuation">,</span> <span class="token string">'landmarks'</span><span class="token punctuation">:</span> landmarks<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>_call_</code>函数的作用是可以直接调用rescale这个类，不需要每次调用时都需要传递参数。 We will write them as callable classes instead of simple functions so that parameters of the transform need not be passed everytime it’s called. </p><h5 id="transformer的用法："><a href="#transformer的用法：" class="headerlink" title="transformer的用法："></a>transformer的用法：</h5><pre class="line-numbers language-python"><code class="language-python">transformed_dataset <span class="token operator">=</span> FaceLandmarksDataset<span class="token punctuation">(</span>csv_file<span class="token operator">=</span><span class="token string">'data/faces/face_landmarks.csv'</span><span class="token punctuation">,</span>                                           root_dir<span class="token operator">=</span><span class="token string">'data/faces/'</span><span class="token punctuation">,</span>                                           transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>                                               Rescale<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                               RandomCrop<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                               ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>                                           <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>FaceLandmarksDataset是之前定义的一个类，transformed_dataset是它的实例化，等于对目录下的data经过了处理，保存到了transformed_dataset这个实例化的类中。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>transformed_dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    sample <span class="token operator">=</span> transformed_dataset<span class="token punctuation">[</span>i<span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> sample<span class="token punctuation">[</span><span class="token string">'image'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> sample<span class="token punctuation">[</span><span class="token string">'landmarks'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>然后通过for 循环，用sample一个一个取出这个类中的数据。</p><p>输出如下:</p><pre class="line-numbers language-html"><code class="language-html">0 torch.Size([3, 224, 224]) torch.Size([68, 2])1 torch.Size([3, 224, 224]) torch.Size([68, 2])2 torch.Size([3, 224, 224]) torch.Size([68, 2])3 torch.Size([3, 224, 224]) torch.Size([68, 2])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="dataloader"><a href="#dataloader" class="headerlink" title="dataloader:"></a>dataloader:</h4><p>由此，这不是深度学习中常用的方式，</p><p>1.我们对数据集通常会进行打乱，不是顺序读取的。shffle data</p><p>2.我们会选择一个batch作为数据，而不是每次都train一个图像。batch data</p><p>3.用上述的代码不适合在多线程运行。muilt-processing</p><p>此题背景是有69张人脸图像+其面部轮廓的框图。我们使用上面的方法，那么就是先标准化处理每张图像，并且将框图和人脸融合，作为一个深度学习标准的数据集，这些是transform做的事情。接下来载入数据，我们想通过数据集<strong>载入</strong>到图像数据中</p><pre><code>dataloader=torch.util.data.Dataloader{    transforms_dataset,batch_size=4,shuffle=true, num_workers=4}</code></pre><p>通过这样的方式，就解决了上述的三个问题。num_workers是读取数据的线程数目。</p><p>note: 补充一下这个函数的实现方式，有一个collate_fn这个函数，是核心，<strong>如果后面想用到其他数据里面</strong>，重写collate_fn函数，在dataloader=torch.util.data.Dataloader{collate_fn=collate_fn}来得到自己想要的加载数据的结果：</p><blockquote><p>collate_fn这个函数的输入就是一个list，list的长度是一个batch size，list中的每个元素都是<strong>getitem</strong>得到的结果。 </p></blockquote><h4 id="ImageFolder"><a href="#ImageFolder" class="headerlink" title="ImageFolder"></a>ImageFolder</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms<span class="token punctuation">,</span> datasetsdata_transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>        transforms<span class="token punctuation">.</span>RandomSizedCrop<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.485</span><span class="token punctuation">,</span> <span class="token number">0.456</span><span class="token punctuation">,</span> <span class="token number">0.406</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                             std<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.229</span><span class="token punctuation">,</span> <span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span>hymenoptera_dataset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'hymenoptera_data/train'</span><span class="token punctuation">,</span>                                           transform<span class="token operator">=</span>data_transform<span class="token punctuation">)</span>dataset_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>hymenoptera_dataset<span class="token punctuation">,</span>                                             batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                                             num_workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从torchvision中导入这两个函数，先写transforms定义了随机裁图片，变成tensor，进行标准化处理。</p><p>使用ImageFolder读取在train下面的图片，其输出如下:  {‘cat’: 0, ‘dog’: 1} 这样的形式，将label和input做了分离 。</p><p>最后使用dataloader导入数据。</p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transfer-learning tutorial</title>
      <link href="/2020/01/16/transfer-learning/"/>
      <url>/2020/01/16/transfer-learning/</url>
      
        <content type="html"><![CDATA[<h4 id="迁移学习概述："><a href="#迁移学习概述：" class="headerlink" title="迁移学习概述："></a>迁移学习概述：</h4><p>最近开始学习迁移学习，主要还是由于在想从迁移学习中再获得一些启发来指导研究少样本学习的一些概念。</p><p>迁移学习主要解决的问题是通过预训练好的模型，用于其他任务的一类问题。总之，在我看来，迁移学习实际上是解决一类问题的。通俗来说就是使用从解决一个问题中获得的知识来解决一个不同的但是相关的问题。</p><p>此外，迁移学习实际上做的是一类问题，可见最后给出的知识框图，给出了迁移学习领域所有问题的一个说明。</p><h4 id="数学符号："><a href="#数学符号：" class="headerlink" title="数学符号："></a>数学符号：</h4><p>先了解下迁移学习定义所使用的符号： $\mathcal{D}$ 领域(domain)，包含两部分：参数空间 $ \mathcal{X} $ 和边缘概率分布 $P(X)$ 组成，其中 $X=\left\{x_{1}, \ldots, x_{n}\right\} \in \mathcal{X}$ , $X$表示领域中的所有数据，也就是表示在 $X$ 中的第 $x_{i}$ 的样本数据都属于参数空间中。</p><p>通常来说两个不同的domain，他们有不同的特征空间和不同的边缘概率分布。给定一个特定的domain，$\mathcal{D}=\{\mathcal{X}, P(X)\}$ ，一个任务 $\mathcal{T}$ 可以分为两个部分，标签空间 $\mathcal{Y}$ 和目标函数 $f(.)$ 。目标函数 $f(.)$ 对于x的预测，我们将 $f(.)$ 改写成 $f(x)$。 在迁移学习中，对数据集我们将其分为source dataset $D_{s}= \left\{({x_{S_{1}}},{y_{S_{1}}}),({x_{S_{2}}},{y_{s_{2}}}),({X_{s_{3}}},{Y_{s_{3}}})\right\} $ 和target dataset $D_{t}=\left\{\left\{x_{T_{1}},y_{T_{1}}\right\},\left\{x_{T_{2}},y_{T_{2}}\right\},\left\{x_{T_{n}},y_{T_{n}}\right\}\right\}$ 。表示在source dataset中，每一个在S<del>1</del> 中的x对应着一个label-y in S<del>1</del>。 而target 同样如此。通常情况下，$n_{T}&lt;&lt;n_{S}$ 。</p><h4 id="数学定义："><a href="#数学定义：" class="headerlink" title="数学定义："></a>数学定义：</h4><p>用数学对迁移学习做定义如下：</p><img src="迁移学习在计算机视觉中的应用/image-20200220184841364.png" alt="image-20200220184841364" style="zoom:50%;"><p>其中最后一项都相等的话，那就是机器学习的研究。因此迁移学习实际上是对机器学习的一个扩展。其研究的范围非常广。</p><p>对定义做详细说明：</p><ul><li>当 $\mathcal{D}_{S} \neq \mathcal{D}_{T}$ ，$\mathcal{T}_{S}=\mathcal{T}_{T}$ 时，此类迁移学习问题是领域上的不同，可以来自：1. 两个domain的X特征空间不一样；or 2. 两个domain的边缘概率分布不一样。</li><li>当 $\mathcal{D}_{S} = \mathcal{D}_{T}$，$\mathcal{T}_{S} \neq \mathcal{T}_{T}$ 时，此类迁移学习问题是在任务上的不同，可以来自：1. 两个task的 $Y$的特征空间不一样；or 2. 两个task的条件概率不一样 $P_{S}(y|x) \neq P_{T}(y|x)$ </li></ul><p>在论文中，以文本分类为例，分别对上面的两种情况举了两个例子：</p><blockquote><ul><li><p>case 1 corresponds to when the two sets of documents are described in different languages, and case 2 may correspond to when the source domain documents and the targetdomain documents focus on different topics. （当 $\mathcal{D}_{S} \neq \mathcal{D}_{T}$ ，$\mathcal{T}_{S}=\mathcal{T}_{T}$ 时）</p></li><li><p>case 1 corresponds to the situation where source domain has binary document classes, whereas the target domain has 10 classes to classify the documents to. Case 2 corresponds to the situation where the source and target documents are very unbalanced in terms of the userdefined classes. 当 $\mathcal{D}_{S} = \mathcal{D}_{T}$，$\mathcal{T}_{S} \neq \mathcal{T}_{T}$ 时</p></li></ul></blockquote><p>根据此，还可以引入另一个非常热的方向：领域自适应问题实际上解决的是两个特征空间相同的数据集边缘分布不同(正常的数据集应该是独立同分布的)，用现有的数据去学习一个function来预测label的准确性。</p><img src="迁移学习在计算机视觉中的应用/image-20200221002110406.png" alt="image-20200221002110406" style="zoom: 67%;"><h4 id="迁移学习的研究领域："><a href="#迁移学习的研究领域：" class="headerlink" title="迁移学习的研究领域："></a>迁移学习的研究领域：</h4><img src="迁移学习在计算机视觉中的应用/image-20200221002150365.png" alt="image-20200221002150365" style="zoom:50%;"><h4 id="迁移学习的分类"><a href="#迁移学习的分类" class="headerlink" title="迁移学习的分类"></a>迁移学习的分类</h4><p>Different Settings of Transfer Learning</p><p><img src="%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/image-20200220225555100.png" alt="image-20200220225555100"></p><img src="迁移学习在计算机视觉中的应用/image-20200220225419493.png" alt="image-20200220225419493" style="zoom: 25%;"><h4 id="深度迁移学习的实现——finetune"><a href="#深度迁移学习的实现——finetune" class="headerlink" title="深度迁移学习的实现——finetune"></a>深度迁移学习的实现——finetune</h4><p>NIPS2014的一篇文章成功地将迁移学习引入到CNN卷积神经网络中，使用了大量地实验告诉我们AlexNet实际是如何对图像进行特征提取的。在了解了迁移学习的方式，通过对其中层中的参数进行调整，就可以得到更好的结果。</p><p>现在用于计算机视觉的finetune已经可以在很大程度上使得结果变得更好：可以参见 <a href>斯坦福CS231_transfer-learning_toturial</a></p><p>用pytorch上的一个例子来学习下迁移学习具体的实现方式。对迁移学习最主要的理解就是通过在大型的数据集上进行训练得到一个网络结构，重置其中的参数，经过微调用来满足在其他数据集上的学习。</p><p>接下来看代码：一个二分类问题，想训练一个resnet18，train_model是方便后面直接调用模型来train CNN这个model。需要输入的参数有model，criterion判别，optimizer优化器，epochs。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_model</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> scheduler<span class="token punctuation">,</span> num_epochs<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    since <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    best_model_wts <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    best_acc <span class="token operator">=</span> <span class="token number">0.0</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch {}/{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> num_epochs <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'-'</span> <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Each epoch has a training and validation phase</span>        <span class="token keyword">for</span> phase <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>                model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Set model to training mode</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># Set model to evaluate mode</span>            running_loss <span class="token operator">=</span> <span class="token number">0.0</span>            running_corrects <span class="token operator">=</span> <span class="token number">0</span>            <span class="token comment" spellcheck="true"># Iterate over data.</span>            <span class="token keyword">for</span> inputs<span class="token punctuation">,</span> labels <span class="token keyword">in</span> dataloaders<span class="token punctuation">[</span>phase<span class="token punctuation">]</span><span class="token punctuation">:</span>                inputs <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>                labels <span class="token operator">=</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># zero the parameter gradients</span>                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># forward</span>                <span class="token comment" spellcheck="true"># track history if only in train</span>                <span class="token keyword">with</span> torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span>phase <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>                    _<span class="token punctuation">,</span> preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>                    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>                    <span class="token comment" spellcheck="true"># backward + optimize only if in training phase</span>                    <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>                        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>                        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># statistics</span>                running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> inputs<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>                running_corrects <span class="token operator">+=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>preds <span class="token operator">==</span> labels<span class="token punctuation">.</span>data<span class="token punctuation">)</span>            <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>                scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>            epoch_loss <span class="token operator">=</span> running_loss <span class="token operator">/</span> dataset_sizes<span class="token punctuation">[</span>phase<span class="token punctuation">]</span>            epoch_acc <span class="token operator">=</span> running_corrects<span class="token punctuation">.</span>double<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> dataset_sizes<span class="token punctuation">[</span>phase<span class="token punctuation">]</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'{} Loss: {:.4f} Acc: {:.4f}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>                phase<span class="token punctuation">,</span> epoch_loss<span class="token punctuation">,</span> epoch_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># deep copy the model</span>            <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'val'</span> <span class="token operator">and</span> epoch_acc <span class="token operator">></span> best_acc<span class="token punctuation">:</span>                best_acc <span class="token operator">=</span> epoch_acc                best_model_wts <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    time_elapsed <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> since    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Training complete in {:.0f}m {:.0f}s'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>        time_elapsed <span class="token operator">//</span> <span class="token number">60</span><span class="token punctuation">,</span> time_elapsed <span class="token operator">%</span> <span class="token number">60</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Best val Acc: {:4f}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>best_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># load best model weights</span>    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>best_model_wts<span class="token punctuation">)</span>    <span class="token keyword">return</span> model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Finetuning-the-convnet"><a href="#Finetuning-the-convnet" class="headerlink" title="Finetuning the convnet"></a>Finetuning the convnet</h4><p> Load a pretrained model and reset final fully connected layer. </p><pre class="line-numbers language-python"><code class="language-python">model_ft <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>num_ftrs <span class="token operator">=</span> model_ft<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>in_features<span class="token comment" spellcheck="true"># Here the size of each output sample is set to 2.</span><span class="token comment" spellcheck="true"># Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).</span>model_ft<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_ftrs<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>model_ft <span class="token operator">=</span> model_ft<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Observe that all parameters are being optimized</span>optimizer_ft <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model_ft<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Decay LR by a factor of 0.1 every 7 epochs</span>exp_lr_scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>StepLR<span class="token punctuation">(</span>optimizer_ft<span class="token punctuation">,</span> step_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">model_ft <span class="token operator">=</span> train_model<span class="token punctuation">(</span>model_ft<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> optimizer_ft<span class="token punctuation">,</span> exp_lr_scheduler<span class="token punctuation">,</span>                       num_epochs<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>model_conv是finetune的网络结构，单独设置了全连接层，最后输出两个label出来。在训练的过程中，conv层和fc层都会发生参数更新，等于在resneet18的参数基础上训练了整个网络，参数不是从初始开始更新的。</p><h4 id="ConvNet-as-fixed-feature-extractor"><a href="#ConvNet-as-fixed-feature-extractor" class="headerlink" title="ConvNet as fixed feature extractor"></a>ConvNet as fixed feature extractor</h4><p> Here, we need to freeze all the network except the final layer.  </p><p>训练的时候model_conv是不会发生变化的，只是fc层会发生变化。只训练了fc层的参数，conv层作为提取器不发生任何变化。</p><pre class="line-numbers language-python"><code class="language-python">model_conv <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">for</span> param <span class="token keyword">in</span> model_conv<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span><span class="token comment" spellcheck="true"># Parameters of newly constructed modules have requires_grad=True by default</span>num_ftrs <span class="token operator">=</span> model_conv<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>in_featuresmodel_conv<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_ftrs<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>model_conv <span class="token operator">=</span> model_conv<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Observe that only parameters of final layer are being optimized as</span><span class="token comment" spellcheck="true"># opposed to before.</span>optimizer_conv <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model_conv<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Decay LR by a factor of 0.1 every 7 epochs</span>exp_lr_scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>StepLR<span class="token punctuation">(</span>optimizer_conv<span class="token punctuation">,</span> step_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">model_conv <span class="token operator">=</span> train_model<span class="token punctuation">(</span>model_conv<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> optimizer_conv<span class="token punctuation">,</span>                         exp_lr_scheduler<span class="token punctuation">,</span> num_epochs<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>最终结果是fc层的结果更精确，更快速。</p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transfer-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Meta-learning tutorial</title>
      <link href="/2020/01/12/meta-learning/"/>
      <url>/2020/01/12/meta-learning/</url>
      
        <content type="html"><![CDATA[<p>乘着假期期间，把meta-learning部分的内容全部过一遍。把最近的工作做一些简单的总结，顺便做了一份tutorial方便更多的同学对这个领域quickview。以及提供一部分代码进行实践，深入了解目前元学习领域解决的各种问题。以及非常流行的MAML和Reptile两种模型无关的元学习方法。并尝试从理论上对元学习模型进行解释，learn to learn对于few-shot，zero-shot，one-shot learning都有很好的应用，同时需要掌握的概念有迁移学习和表示学习的概念，了解神经网络学习的主要特征有哪些。本tutorial的参考资料都列到了最后的附录中。</p><h4 id="Meta-learning的两类观点"><a href="#Meta-learning的两类观点" class="headerlink" title="Meta-learning的两类观点"></a>Meta-learning的两类观点</h4><ul><li>一种是Mechanistic view: 可以读取整个数据集并预测新数据点的深度神经网络模型。训练该网络使用一个元数据集，该元数据集本身包含许多数据集，每个数据集用于不同的任务。此观点使实现meta-learning算法更加容易。</li><li>另一种是Probabilistic view：从一组（元训练meta-training）任务中提取先验信息，从而高效地学习新任务学习新任务时, 使用此先验和（小）训练集来推断最有可能的后验参数。此观点使理解元学习算法更加容易。</li></ul><h4 id="元学习的数学定义"><a href="#元学习的数学定义" class="headerlink" title="元学习的数学定义"></a>元学习的数学定义</h4><p>通过学习一类任务，得到一个通用的参数$\theta ^*$,通过固定参数$\theta$ ，在每个训练集的任务上用该task运用该task可以得到$\phi$的训练参数。</p><h5 id="监督学习："><a href="#监督学习：" class="headerlink" title="监督学习："></a>监督学习：</h5><p>目标是:$\arg \max _{\phi} \log p(\phi | D)$ ，其中$\mathcal{D}=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{k}, y_{k}\right)\right\}$ </p><p>其中，xi表示input，比如image，而y表示label。</p><p>$=\arg \max \log p(D | \phi)+\log p(\phi)$</p><p>$=\arg \max \sum_{i} \log p\left(y_{i} | x_{i}, \phi\right)+\log p(\phi)$</p><p>上式可以理解为寻找最大的$\phi$ , 使得学习出来的输出和真实的输出在相似度(极大似然)上最大。其中$\log p(\phi)$ 可以理解为在训练过程中加的正则项，为了防止训练的时候出现过拟合。</p><p>对于监督学习来说，存在的问题是</p><ul><li><p>如果想训练出一个非常强大的模型需要大量的数据，</p></li><li><p>对于一些任务来说label data 非常有限</p></li></ul><h5 id="元学习"><a href="#元学习" class="headerlink" title="元学习"></a>元学习</h5><p>实际上，我们可以将数据集分成多个dataset，其中每一个dataset含有不同张图片，我们把这个新的dataset称作是meta-train数据集。</p><p><img src="\image\图片1.png" alt="图片1" style="zoom: 50%;"><img src="meta-learning/image-20200220032921498.png" alt="image-20200220032921498"></p><p>这样就是在每一个Di上都可以学习到需要的参数，最后综合在不同的dataset上学习到的参数作为$\phi$使得其最大。数学上的定义就是：</p><p>$\arg \max _{\phi} \log p\left(\phi | \mathcal{D}, \mathcal{D}_{\text {meta-train }}\right)$</p><p>meta-learning的问题就建立在这样的数据集的基础上，其中$\mathcal{D}=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{k}, y_{k}\right)\right\}$ $\mathcal{D}_{\text {meta-train }}=\left\{\mathcal{D}_{1}, \ldots, \mathcal{D}_{n}\right\}$ ，$\mathcal{D}_{i}=\left\{\left(x_{1}^{i}, y_{1}^{i}\right), \ldots,\left(x_{k}^{i}, y_{k}^{i}\right)\right\}$ .</p><p>因此, 假定$\phi \perp \mathcal{D}_{\text {meta-train }} | \theta$, 表示$ \phi$ 与 $\mathcal{D}_{meta-train}$ 和 $\theta $ 是独立无关的, 可以推导出下面的式子 </p><p>$\log p\left(\phi | \mathcal{D}, \mathcal{D}_{\mathrm{meta}-\mathrm{train}}\right)=\log \int_{\Theta} p(\phi | \mathcal{D}, \theta) p\left(\theta | \mathcal{D}_{\mathrm{meta}-\mathrm{train}}\right) d \theta$  </p><p> $\approx \log p\left(\phi | \mathcal{D}, \theta^{\star}\right)+\log p\left(\theta^{\star} | \mathcal{D}_{\text {meta-train }}\right)$ </p><p>第二行是找到一个平均值$\theta^{\star}$对积分做一个平均得到。其中，第二项中不包含变量$\phi$，为常数项，因此最后问题就变成了：</p><p>$\arg \max _{\phi} \log p\left(\phi | \mathcal{D}, \mathcal{D}_{\text {meta-train }}\right) \approx \arg \max _{\phi} \log p\left(\phi | \mathcal{D}, \theta^{\star}\right)$ </p><p>数学定义完全可以看明白。上式称为是<strong>元学习(meta-learning)需要解决的问题</strong>。要先找到一个$\theta^{*}$ ，再找到最好的$\phi$ 使得能够得到最大的p。</p><h5 id="元学习解决思路及术语介绍："><a href="#元学习解决思路及术语介绍：" class="headerlink" title="元学习解决思路及术语介绍："></a>元学习解决思路及术语介绍：</h5><p>要想使得上式能够得到求解，我们看一下如何进行完整的optimization 过程。优化问题可以分为两部分：</p><ul><li><p>1.第一部分：meta-learning，即$\theta^{\star}=\arg \max _{\theta} \log p\left(\theta | \mathcal{D}_{\text {meta-train }}\right)$ </p></li><li><p>2.第二部分：adaptation. 即 $\phi^{\star}=\arg \max _{\phi} \log p\left(\phi | \mathcal{D}^{\operatorname{tr}}, \theta^{\star}\right)$ </p></li></ul><p>我们将meta-train数据集进行完整的划分：</p><p>$\begin{array}{l}{\mathcal{D}_{\text {meta-train }}=\left\{\left(\mathcal{D}_{1}^{\text {tr }}, \mathcal{D}_{1}^{\text {ts }}\right), \ldots,\left(\mathcal{D}_{n}^{\text {tr }}, \mathcal{D}_{n}^{\text {ts }}\right)\right\}} \\ {D_{i}^{\text {tr }}=\left\{\left(x_{1}^{i}, y_{1}^{i}\right), \ldots,\left(x_{k}^{i}, y_{k}^{i}\right)\right\}} \\ {\mathcal{D}_{i}^{\text {ts }}=\left\{\left(x_{1}^{i}, y_{1}^{i}\right), \ldots,\left(x_{l}^{i}, y_{l}^{i}\right)\right\}}\end{array}$ </p><p>因此，可以将$\phi^{\star}=\arg \max _{\phi} \log p\left(\phi | \mathcal{D}^{\operatorname{tr}}, \theta^{\star}\right)$ 进行训练，得到的参数$\phi$ 即可以表示$\phi^{\star}=f_{\theta^{\star}}\left(\mathcal{D}^{\mathrm{tr}}\right)$ 下的$\theta$ ，通过进行更新参数$\theta$ ，验证在$D^{tr}$得到结果。</p><p>以上可以认为是一个完整的训练过程，其训练$\theta$ 优化数学表达为：</p><p>$\theta^{\star}=\max _{\theta} \sum_{i=1}^{n} \log p\left(\phi_{i} | \mathcal{D}_{\mathcal{i}}^{(\mathrm{ts})}\right)$ </p><p>where $\phi_{i}= f_{\theta}(D_{i}^{tr})$</p><p>要在训练集上最优，使得$\phi=f_{\theta}\left(\mathcal{D}^{\mathrm{tr}}\right)$ 学习到的$\theta$。</p><p>总结来说， 直观上$\phi$ 是在训练任务$D^{tr}$ 中学习得到的通用的经验，我们称它为$f_{\theta}(D_{i}^{tr})$ , 下标表示含有$\theta$ 这个参数，最终目的是要使得在$D_{i}^{ts}$ 上学习到最好的结果——p最大。这样就完成了在少样本测试的情况下给出了最好的分类性能。实现了meta-learning的目的。</p><p>对于数据集中的训练集和测试集，需要重新定义如下图：</p><p><img src="meta-learning/image-20200220032944265.png" alt="image-20200220032944265"></p><p>note：meta-learning的范式也有一些与其非常相关的问题，比如多任务学习(multi-task learning)，或者超参数优化和automl问题。</p><h5 id="元学习常用的几类方法："><a href="#元学习常用的几类方法：" class="headerlink" title="元学习常用的几类方法："></a>元学习常用的几类方法：</h5><p>有四类方法，并不打算过多地展开讲，可以参见我另外一篇博客中使用模型无关的思想做了图像生成的工作，可以说对maml的理解比较深入了。也确实明白这是一个非常不错的工作。这里参考了ICML2019 tutorial的分类，也可以参考 <a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html</a> 的分类，主流方法还是MAML的各种改进。</p><h6 id="1-Black-box-adaptation"><a href="#1-Black-box-adaptation" class="headerlink" title="1.Black-box adaptation"></a>1.Black-box adaptation</h6><p><img src="meta-learning/image-20200220033004659.png" alt="image-20200220033004659"></p><h6 id="2-Opamizaaon-based-inference"><a href="#2-Opamizaaon-based-inference" class="headerlink" title="2.Opamizaaon-based inference"></a>2.Opamizaaon-based inference</h6><p>其中，最经典的莫过于MAML。</p><p>其思路可以直接用下图表示，即让这个loss最小。深入的理解，需要从论文出发。</p><p><img src="meta-learning/image-20200220033024315.png" alt="image-20200220033024315"></p><p><img src="\image\2020-02-16_00-46-53.jpg" alt="2020-02-16_00-46-53" style="zoom: 50%;"><img src="meta-learning/image-20200220033039938.png" alt="image-20200220033039938"></p><h6 id="3-Non-parametric-methods"><a href="#3-Non-parametric-methods" class="headerlink" title="3.Non-parametric methods"></a>3.Non-parametric methods</h6><p><img src="meta-learning/image-20200220033058725.png" alt="image-20200220033058725"></p><h6 id="4-Bayesian-meta-learning"><a href="#4-Bayesian-meta-learning" class="headerlink" title="4.Bayesian meta-learning"></a>4.Bayesian meta-learning</h6><p><img src="meta-learning/image-20200220033114720.png" alt="image-20200220033114720"></p><h5 id="元学习的应用："><a href="#元学习的应用：" class="headerlink" title="元学习的应用："></a>元学习的应用：</h5><p>参考ICML2019tutorial，在语言和图像上都有应用，包括现在比较火的reinforcement learning以及robtic learning。</p><img src="meta-learning/image-20200220033126015.png" alt="image-20200220033126015" style="zoom:50%;"><p><img src="meta-learning/image-20200220033139928.png" alt="image-20200220033139928">：</p><p>上述应用还是在比较传统的领域，实际上，更多的工作开始围绕着强化学习展开，由于强化学习方面的了解的不够深入，今后会继续了解imitation learning相关的工作成果。</p><p>很喜欢这个图：元学习的终极目标，作为本篇文章的结尾！</p><p><img src="meta-learning/image-20200220033150499.png" alt="image-20200220033150499"></p><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul><li>ICML2019tutorial上的，Chelsea Finn和Sergey Levine做的lecuture。 <a href="https://sites.google.com/view/icml19metalearning" target="_blank" rel="noopener">https://sites.google.com/view/icml19metalearning</a> (google site)  <a href="https://youtube.videoken.com/embed/DijI4XrhqNo" target="_blank" rel="noopener">https://youtube.videoken.com/embed/DijI4XrhqNo</a> </li><li>hungyi-Lee，台大，<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pptx" target="_blank" rel="noopener">http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pptx</a> ppt, <a href="https://www.youtube.com/watch?v=EkAqYbpCYAc&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=33&amp;t=0s" target="_blank" rel="noopener">https://www.youtube.com/watch?v=EkAqYbpCYAc&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=33&amp;t=0s</a> (video)</li><li>meta-learning比较好的overview: <a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html</a> </li><li>meta-transfer-learning:  <a href="https://yyliu.net/files/meta-transfer-learning-slides.pdf" target="_blank" rel="noopener">https://yyliu.net/files/meta-transfer-learning-slides.pdf</a> (slides) <a href="https://github.com/yaoyao-liu/meta-transfer-learning" target="_blank" rel="noopener">https://github.com/yaoyao-liu/meta-transfer-learning</a> (code)</li><li>few-shot image generation with reptile: <a href="https://github.com/LuEE-C/FIGR" target="_blank" rel="noopener">https://github.com/LuEE-C/FIGR</a> (code) <a href="https://arxiv.org/abs/1901.02199" target="_blank" rel="noopener">https://arxiv.org/abs/1901.02199</a> (paper)</li><li>meta-transfer-learning-gan:  <a href="https://yyliu.net/files/meta-transfer-learning-slides.pdf" target="_blank" rel="noopener">https://yyliu.net/files/meta-transfer-learning-slides.pdf</a> (pdf) <a href="https://arxiv.org/pdf/1812.02391" target="_blank" rel="noopener">https://arxiv.org/pdf/1812.02391</a> (paper)</li></ul>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> meta-learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

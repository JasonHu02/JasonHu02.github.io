<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>leetcode题解</title>
      <link href="2020/11/25/leetcode-1/"/>
      <url>2020/11/25/leetcode-1/</url>
      
        <content type="html"><![CDATA[<p>最小的K个数</p><p>剑指offer的面试题</p><p>输入整数数组 <code>arr</code> ，找出其中最小的 <code>k</code> 个数。例如，输入4、5、1、6、2、7、3、8这8个数字，则最小的4个数字是1、2、3、4。</p><p><strong>示例 1：</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：arr = [3,2,1], k = 2</span><br><span class="line">输出：[1,2] 或者 [2,1]</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：arr = [0,1,2,1], k = 1</span><br><span class="line">输出：[0]</span><br></pre></td></tr></table></figure><p><strong>限制：</strong></p><ul><li><code>0 &lt;= k &lt;= arr.length &lt;= 10000</code></li><li><code>0 &lt;= arr[i] &lt;= 10000</code></li></ul><p>非常简单让人想到的方法就是排序，从小到大排好序之后，然后直接返回前k个元素，此题求解。</p><p>实际代码也很简单：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getLeastNumbers</span><span class="params">(self, arr, k:int)</span> :</span></span><br><span class="line">        a=arr.sort()</span><br><span class="line">        <span class="keyword">return</span> arr[:k]</span><br></pre></td></tr></table></figure><p>list.sort() 方法排序时直接修改原列表，返回None； sorted() 使用的范围更为广泛，但是如果不需要保留原列表。</p><p>函数形式如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sorted(iterable[, key][, reverse]) </span><br><span class="line">list.sort(*, key=<span class="literal">None</span>, reverse=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><ul><li><p>key 是带一个参数的<strong>函数</strong>，返回一个值用来排序，默认为 None。这个函数只调用一次，所以fast。</p></li><li><p>reverse 表示排序结果是否反转</p><p>另外就是一种新方法可以用来了解Top-K算法的一种高级用法。改进的快速排序方法， <strong>BFPRT算法</strong>。</p><p>在BFPTR算法中，仅仅是改变了快速排序<strong>Partion</strong>中的<strong>pivot</strong>值的选取，在快速排序中，我们始终选择第一个元素或者最后一个元素作为<strong>pivot</strong>，而在BFPTR算法中，每次选择五分中位数的中位数作为<strong>pivot</strong>，这样做的目的就是使得划分比较合理，从而避免了最坏情况的发生。算法步骤如下 ：</p><ol type="1"><li>将n 个元素划为 [n/5]组，每组5个，至多只有一组由 n mod 5 个元素组成。</li><li>寻找这 [n/5]个组中每一个组的中位数，这个过程可以用插入排序。</li></ol></li></ul><ol start="3" type="1"><li>对步骤2中的[n/5] 个中位数，重复步骤1和步骤2，递归下去，直到剩下一个数字。<ol start="4" type="1"><li>最终剩下的数字即为pivot，把大于它的数全放左边，小于等于它的数全放右边。</li></ol></li><li>判断pivot的位置与k的大小，有选择的对左边或右边递归。</li></ol><p>求第k大就是求第n-k+1小，这两者等价。</p><p>TopK问题最佳的解决思路是使用堆排序:</p><p>维护一个小顶堆，保持堆中的元素为k个，依次遍历整个数组中的元素，如果元素超过堆中的数，把顶端的数推出去，直到遍历完整个数组，则堆顶的元素即为topK。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findKthLargest</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;nums, <span class="keyword">int</span> k)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> numsSize = <span class="keyword">int</span>(nums.size());</span><br><span class="line">        <span class="keyword">if</span> (numsSize == <span class="number">0</span> || k &gt; numsSize)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        priority_queue&lt;<span class="keyword">int</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;, greater&lt;<span class="keyword">int</span>&gt;&gt; store;</span><br><span class="line">        <span class="comment">//堆中维持k个最大数</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numsSize; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            store.push(nums[i]);</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">int</span>(store.size()) &gt; k)</span><br><span class="line">            &#123;</span><br><span class="line">                store.pop();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        result = store.top();</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>其他方法：</p><ul><li><strong>全局排序</strong>，O(n*lg(n))</li><li><strong>局部排序</strong>，只排序TopK个数，O(n*k)</li><li><strong>堆</strong>，TopK个数也不排序了，O(n*lg(k))</li><li>分治法，每个分支“都要”递归，例如：快速排序，O(n*lg(n))</li><li>减治法，“只要”递归一个分支，例如：二分查找O(lg(n))，随机选择O(n)</li><li>TopK的另一个解法：<strong>随机选择</strong>+partition</li></ul>]]></content>
      
      
      <categories>
          
          <category> leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>回溯算法总结</title>
      <link href="2020/11/25/backtracking/"/>
      <url>2020/11/25/backtracking/</url>
      
        <content type="html"><![CDATA[<h5 id="回溯算法">回溯算法：</h5><p>解决一个回溯问题，实际上就是一个决策树的遍历过程。</p><p>1、路径：也就是已经做出的选择。2、选择列表：也就是你当前可以做的选择。3、结束条件：也就是到达决策树底层，无法再做选择的条件。</p><p>代码方面，回溯算法的框架：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = []</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backtrack</span><span class="params">(路径, 选择列表)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> 满足结束条件:</span><br><span class="line">        result.add(路径)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> 选择 <span class="keyword">in</span> 选择列表:</span><br><span class="line">        做选择</span><br><span class="line">        backtrack(路径, 选择列表)</span><br><span class="line">        撤销选择</span><br></pre></td></tr></table></figure><p><strong>其核心就是 for 循环里面的递归，在递归调用之前「做选择」，在递归调用之后「撤销选择」</strong>，特别简单。</p><p>全排列问题:</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">List&lt;List&lt;Integer&gt;&gt; res = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 主函数，输入一组不重复的数字，返回它们的全排列 */</span></span><br><span class="line"><span class="function">List&lt;List&lt;Integer&gt;&gt; <span class="title">permute</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 记录「路径」</span></span><br><span class="line">    LinkedList&lt;Integer&gt; track = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">    backtrack(nums, track);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 路径：记录在 track 中</span></span><br><span class="line"><span class="comment">// 选择列表：nums 中不存在于 track 的那些元素</span></span><br><span class="line"><span class="comment">// 结束条件：nums 中的元素全都在 track 中出现</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">backtrack</span><span class="params">(<span class="keyword">int</span>[] nums, LinkedList&lt;Integer&gt; track)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 触发结束条件</span></span><br><span class="line">    <span class="keyword">if</span> (track.size() == nums.length) &#123;</span><br><span class="line">        res.add(<span class="keyword">new</span> LinkedList(track));</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">        <span class="comment">// 排除不合法的选择</span></span><br><span class="line">        <span class="keyword">if</span> (track.contains(nums[i]))</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        <span class="comment">// 做选择</span></span><br><span class="line">        track.add(nums[i]);</span><br><span class="line">        <span class="comment">// 进入下一层决策树</span></span><br><span class="line">        backtrack(nums, track);</span><br><span class="line">        <span class="comment">// 取消选择</span></span><br><span class="line">        track.removeLast();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>N 皇后问题:</p><p>给你一个 N×N 的棋盘，让你放置 N 个皇后，使得它们不能互相攻击。皇后可以攻击同一行、同一列、左上左下右上右下四个方向的任意单位。</p><p>这个问题本质上跟全排列问题差不多，决策树的每一层表示棋盘上的每一行；每个节点可以做出的选择是，在该行的任意一列放置一个皇后。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt; res;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 输入棋盘边长 n，返回所有合法的放置 */</span></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt; <span class="title">solveNQueens</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// '.' 表示空，'Q' 表示皇后，初始化空棋盘。</span></span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; <span class="title">board</span><span class="params">(n, <span class="built_in">string</span>(n, <span class="string">'.'</span>))</span></span>;</span><br><span class="line">    backtrack(board, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 路径：board 中小于 row 的那些行都已经成功放置了皇后</span></span><br><span class="line"><span class="comment">// 选择列表：第 row 行的所有列都是放置皇后的选择</span></span><br><span class="line"><span class="comment">// 结束条件：row 超过 board 的最后一行</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">backtrack</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; board, <span class="keyword">int</span> row)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 触发结束条件</span></span><br><span class="line">    <span class="keyword">if</span> (row == board.size()) &#123;</span><br><span class="line">        res.push_back(board);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> n = board[row].size();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> col = <span class="number">0</span>; col &lt; n; col++) &#123;</span><br><span class="line">        <span class="comment">// 排除不合法选择</span></span><br><span class="line">        <span class="keyword">if</span> (!isValid(board, row, col)) </span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        <span class="comment">// 做选择</span></span><br><span class="line">        board[row][col] = <span class="string">'Q'</span>;</span><br><span class="line">        <span class="comment">// 进入下一行决策</span></span><br><span class="line">        backtrack(board, row + <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 撤销选择</span></span><br><span class="line">        board[row][col] = <span class="string">'.'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数 <code>backtrack</code> 依然像个在决策树上游走的指针，通过 <code>row</code> 和 <code>col</code> 就可以表示函数遍历到的位置，通过 <code>isValid</code> 函数可以将不符合条件的情况剪枝：</p><p>这部分主要代码，其实跟全排列问题差不多，<code>isValid</code> 函数的实现也很简单:</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* 是否可以在 board[row][col] 放置皇后？ */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isValid</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; board, <span class="keyword">int</span> row, <span class="keyword">int</span> col)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = board.size();</span><br><span class="line">    <span class="comment">// 检查列是否有皇后互相冲突</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (board[i][col] == <span class="string">'Q'</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 检查右上方是否有皇后互相冲突</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = row - <span class="number">1</span>, j = col + <span class="number">1</span>; </span><br><span class="line">            i &gt;= <span class="number">0</span> &amp;&amp; j &lt; n; i--, j++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (board[i][j] == <span class="string">'Q'</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 检查左上方是否有皇后互相冲突</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = row - <span class="number">1</span>, j = col - <span class="number">1</span>;</span><br><span class="line">            i &gt;= <span class="number">0</span> &amp;&amp; j &gt;= <span class="number">0</span>; i--, j--) &#123;</span><br><span class="line">        <span class="keyword">if</span> (board[i][j] == <span class="string">'Q'</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>数独：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 函数找到一个答案后就返回 true</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">backtrack</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; board, <span class="keyword">int</span> row)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 触发结束条件</span></span><br><span class="line">    <span class="keyword">if</span> (row == board.size()) &#123;</span><br><span class="line">        res.push_back(board);</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> col = <span class="number">0</span>; col &lt; n; col++) &#123;</span><br><span class="line">        ...</span><br><span class="line">        board[row][col] = <span class="string">'Q'</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (backtrack(board, row + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">        board[row][col] = <span class="string">'.'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其实想想看，回溯算法和动态规划是不是有点像呢？我们在动态规划系列文章中多次强调，动态规划的三个需要明确的点就是「状态」「选择」和「base case」，是不是就对应着走过的「路径」，当前的「选择列表」和「结束条件」？</p><p>某种程度上说，动态规划的暴力求解阶段就是回溯算法。只是有的问题具有重叠子问题性质，可以用 dp table 或者备忘录优化，将递归树大幅剪枝，这就变成了动态规划。而今天的两个问题，都没有重叠子问题，也就是回溯算法问题了，复杂度非常高是不可避免的。</p>]]></content>
      
      
      <categories>
          
          <category> leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> backtracking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>广度优先遍历(BFS)算法框架</title>
      <link href="2020/10/25/bfs-suan-fa-kuang-jia/"/>
      <url>2020/10/25/bfs-suan-fa-kuang-jia/</url>
      
        <content type="html"><![CDATA[<p>DFS算法可以被认为是回溯算法，BFS算法出现的场景：在一副图中找到从起点 <code>start</code>到终点 <code>target</code> 的最近距离。比如两个单词通过替换某些字母，把其中一个变为另一个，每次只能替换一个字母。下面是BFS的算法伪代码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 计算从起点 start 到终点 target 的最近距离</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">DFS</span><span class="params">(Node start, Node target)</span></span>&#123;</span><br><span class="line">Queue&lt;Node&gt; q; <span class="comment">//队列</span></span><br><span class="line">Set&lt;Node&gt; visited; <span class="comment">//避免走回头路</span></span><br><span class="line">  </span><br><span class="line">q.offer(start); <span class="comment">//将起点加入队列</span></span><br><span class="line">  visited.add(start); </span><br><span class="line">  <span class="keyword">int</span> step=<span class="number">0</span>; <span class="comment">// 记录扩散的步数，就是我们要返回的值</span></span><br><span class="line"><span class="keyword">while</span> (q not empty)&#123;</span><br><span class="line">    <span class="keyword">int</span> sz=q.size();</span><br><span class="line">    <span class="comment">//将当前队列中的所有节点向四周扩散</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i;i&lt;sz;i++)&#123;</span><br><span class="line">      Node cur=q.poll();</span><br><span class="line">    <span class="comment">// 判断是否到达终点</span></span><br><span class="line">      <span class="keyword">if</span> (cur is target) </span><br><span class="line">        <span class="keyword">return</span> step;</span><br><span class="line">    <span class="comment">// 将cur的相邻节点加入队列</span></span><br><span class="line">      <span class="keyword">for</span> (Node x: cur.adj())&#123;</span><br><span class="line">        <span class="keyword">if</span> (x not in visited)&#123;</span><br><span class="line">          q.offer(x);</span><br><span class="line">          visited.add(x);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//在这里更新步数</span></span><br><span class="line">    step++;</span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>cur.adj</code> 泛指与cur相邻的节点，整个代码可以描述成为了计算起点和终点的最近距离，从根节点开始遍历整个结构，使用队列q来存储每次的节点，step存储进行的步数。遍历的过程，将cur的相邻的点加入到队列里面， 如果达到了设置的终止条件，就返回，否则就继续step计数。</p><p>为什么要用队列存储？因为本质上树的节点，用linkedlist存储的。</p><p>二叉树的最小高度:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">minDepth</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    Queue&lt;TreeNode&gt; q = <span class="keyword">new</span> LinkedList&lt;&gt;(); </span><br><span class="line">  <span class="comment">//说明: Java的Queue是一个接口,LinkedList实现了Queue的接口，因此不用强制转型。如果给的是ArrayList则必须强制转型。例如Queue&lt;Integer&gt; qq = (Queue&lt;Integer&gt;) new ArrayList&lt;Integer&gt;();</span></span><br><span class="line">    q.offer(root);</span><br><span class="line">    <span class="comment">// root 本身就是一层，depth 初始化为 1</span></span><br><span class="line">    <span class="keyword">int</span> depth = <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (!q.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">int</span> sz = q.size();</span><br><span class="line">        <span class="comment">/* 将当前队列中的所有节点向四周扩散 */</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; sz; i++) &#123;</span><br><span class="line">            TreeNode cur = q.poll();</span><br><span class="line">            <span class="comment">/* 判断是否到达终点 */</span></span><br><span class="line">            <span class="keyword">if</span> (cur.left == <span class="keyword">null</span> &amp;&amp; cur.right == <span class="keyword">null</span>) <span class="comment">//在具体的场景中就是写对应的目标条件</span></span><br><span class="line">                <span class="keyword">return</span> depth;</span><br><span class="line">            <span class="comment">/* 将 cur 的相邻节点加入队列 */</span></span><br><span class="line">            <span class="keyword">if</span> (cur.left != <span class="keyword">null</span>) <span class="comment">//二叉树中的相邻节点就是left和right</span></span><br><span class="line">                q.offer(cur.left);</span><br><span class="line">            <span class="keyword">if</span> (cur.right != <span class="keyword">null</span>) </span><br><span class="line">                q.offer(cur.right);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/* 这里增加步数 */</span></span><br><span class="line">        depth++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> depth;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解开密码锁的次数</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">openLock</span><span class="params">(String[] deadends, String target)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 记录需要跳过的死亡密码</span></span><br><span class="line">    Set&lt;String&gt; deads = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (String s : deadends) deads.add(s);</span><br><span class="line">    <span class="comment">// 记录已经穷举过的密码，防止走回头路</span></span><br><span class="line">    Set&lt;String&gt; visited = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">    Queue&lt;String&gt; q = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">    <span class="comment">// 从起点开始启动广度优先搜索</span></span><br><span class="line">    <span class="keyword">int</span> step = <span class="number">0</span>;</span><br><span class="line">    q.offer(<span class="string">"0000"</span>);</span><br><span class="line">    visited.add(<span class="string">"0000"</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (!q.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">int</span> sz = q.size();</span><br><span class="line">        <span class="comment">/* 将当前队列中的所有节点向周围扩散 */</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; sz; i++) &#123;</span><br><span class="line">            String cur = q.poll();</span><br><span class="line">            </span><br><span class="line">            <span class="comment">/* 判断密码是否合法，是否到达终点 */</span></span><br><span class="line">            <span class="keyword">if</span> (deads.contains(cur))</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">if</span> (cur.equals(target))</span><br><span class="line">                <span class="keyword">return</span> step;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">/* 将一个节点的未遍历相邻节点加入队列 */</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; j++) &#123;</span><br><span class="line">                String up = plusOne(cur, j);</span><br><span class="line">                <span class="keyword">if</span> (!visited.contains(up)) &#123;</span><br><span class="line">                    q.offer(up);</span><br><span class="line">                    visited.add(up);</span><br><span class="line">                &#125;</span><br><span class="line">                String down = minusOne(cur, j);</span><br><span class="line">                <span class="keyword">if</span> (!visited.contains(down)) &#123;</span><br><span class="line">                    q.offer(down);</span><br><span class="line">                    visited.add(down);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/* 在这里增加步数 */</span></span><br><span class="line">        step++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果穷举完都没找到目标密码，那就是找不到了</span></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以使用双向 BFS 算法来提高效率，代码稍加修改即可：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">openLock</span><span class="params">(String[] deadends, String target)</span> </span>&#123;</span><br><span class="line">    Set&lt;String&gt; deads = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (String s : deadends) deads.add(s);</span><br><span class="line">    <span class="comment">// 用集合不用队列，可以快速判断元素是否存在</span></span><br><span class="line">    Set&lt;String&gt; q1 = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">    Set&lt;String&gt; q2 = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">    Set&lt;String&gt; visited = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">    <span class="comment">// 初始化起点和终点</span></span><br><span class="line">    q1.add(<span class="string">"0000"</span>);</span><br><span class="line">    q2.add(target);</span><br><span class="line">    <span class="keyword">int</span> step = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (!q1.isEmpty() &amp;&amp; !q2.isEmpty()) &#123;</span><br><span class="line">        <span class="comment">// 哈希集合在遍历的过程中不能修改，</span></span><br><span class="line">        <span class="comment">// 用 temp 存储 q1 的扩散结果</span></span><br><span class="line">        Set&lt;String&gt; temp = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 将 q1 中的所有节点向周围扩散 */</span></span><br><span class="line">        <span class="keyword">for</span> (String cur : q1) &#123;</span><br><span class="line">            <span class="comment">/* 判断是否到达终点 */</span></span><br><span class="line">            <span class="keyword">if</span> (deads.contains(cur))</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">if</span> (q2.contains(cur))</span><br><span class="line">                <span class="keyword">return</span> step;</span><br><span class="line">            visited.add(cur);</span><br><span class="line"></span><br><span class="line">            <span class="comment">/* 将一个节点的未遍历相邻节点加入集合 */</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; j++) &#123;</span><br><span class="line">                String up = plusOne(cur, j);</span><br><span class="line">                <span class="keyword">if</span> (!visited.contains(up))</span><br><span class="line">                    temp.add(up);</span><br><span class="line">                String down = minusOne(cur, j);</span><br><span class="line">                <span class="keyword">if</span> (!visited.contains(down))</span><br><span class="line">                    temp.add(down);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/* 在这里增加步数 */</span></span><br><span class="line">        step++;</span><br><span class="line">        <span class="comment">// temp 相当于 q1</span></span><br><span class="line">        <span class="comment">// 这里交换 q1 q2，下一轮 while 就是扩散 q2</span></span><br><span class="line">        q1 = q2;</span><br><span class="line">        q2 = temp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>再谈动态规划框架</title>
      <link href="2020/09/30/dong-tai-gui-hua-kuang-jia/"/>
      <url>2020/09/30/dong-tai-gui-hua-kuang-jia/</url>
      
        <content type="html"><![CDATA[<h2 id="动态规划框架">动态规划框架</h2><p>动态规划问题一般是求最值，比如求最长子序列，最小编辑距离等。</p><p>动态规划三要素：</p><p>1.存在重叠”子问题“， 如果暴力穷举，效率会非常低下，因此需要具备备忘录，DPtabel来优化穷举过程；</p><ol start="2" type="1"><li><p>具备最优子结构，才能通过子问题的最值得到原问题的最值。</p></li><li><p>正确的”状态转移方程“，穷举所以可行解并不是一件容易的事情，这一步最为重要。</p><p>如何列举出状态转移方程，可以遵守以下步骤：</p><ol type="1"><li>这个问题的base case (最简单的情况) 是什么？</li><li>这个问题有什么状态？</li><li>对于每个”状态“，可以做什么”选择“ 使得”状态“发生改变？</li><li>如何定义 <code>dp</code> 数组/函数 的含义来表现”状态“和“选择”？</li></ol></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#初始化base case</span></span><br><span class="line">dp[<span class="number">0</span>][<span class="number">0</span>][...]=base case</span><br><span class="line"><span class="keyword">for</span> 状态<span class="number">1</span> <span class="keyword">in</span> 状态<span class="number">1</span>的所有数值:</span><br><span class="line"><span class="keyword">for</span> 状态<span class="number">2</span> <span class="keyword">in</span> 状态<span class="number">2</span>的所有数值:</span><br><span class="line"><span class="keyword">for</span> ...:</span><br><span class="line">dp[状态<span class="number">1</span>][状态<span class="number">2</span>][...]=求最值(选择<span class="number">1</span>,选择<span class="number">2</span>,...)</span><br><span class="line">      </span><br><span class="line"><span class="comment"># 在每个列举出来的状态中通过求最值的方法得到当前状态的最大/最小选择。for循环列举出所有的状态。状态转移体现在f(z)=min(f(x),f(y))中，z=x+y z和x，y有关系，这样可以得到最后的min值</span></span><br></pre></td></tr></table></figure><p>斐波那契数列: https://leetcode-cn.com/problems/fei-bo-na-qi-shu-lie-lcof/</p><p>写一个函数，输入 n ，求斐波那契（Fibonacci）数列的第 n 项。斐波那契数列的定义如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">F(0) = 0,   F(1) = 1</span><br><span class="line">F(N) = F(N - 1) + F(N - 2), 其中 N &gt; 1.</span><br></pre></td></tr></table></figure><p>斐波那契数列由0和1开始，之后的斐波那契数就是由之前的两数相加而得出。</p><p>答案需要取模1e9+7(1000000007), 如计算初始结果为:1000000008, 请返回1。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">fib</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (n==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">if</span> (n==<span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">      <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; dp;</span><br><span class="line">      dp[<span class="number">1</span>]=<span class="number">1</span>;</span><br><span class="line">      dp[<span class="number">2</span>]=<span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">3</span>;i&lt;=n;i++)&#123;</span><br><span class="line">        dp[i]=(dp[i<span class="number">-1</span>]+dp[i<span class="number">-2</span>])%<span class="number">10000007</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> dp[n<span class="number">-1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">fib</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (n==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">if</span> (n==<span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">      <span class="keyword">int</span> prev=<span class="number">1</span>;</span><br><span class="line">      <span class="keyword">int</span> curr=<span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">3</span>;i&lt;=n;i++)&#123;</span><br><span class="line"><span class="keyword">int</span> sums= prev+curr;</span><br><span class="line">        prev=curr;</span><br><span class="line">        curr=sums;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> sums;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>零钱交换: https://leetcode-cn.com/problems/coin-change/</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">coinChange</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; coins, <span class="keyword">int</span> amount)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> Max = amount + <span class="number">1</span>;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(amount + <span class="number">1</span>, Max)</span></span>;</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= amount; ++i) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; (<span class="keyword">int</span>)coins.size(); ++j) &#123;</span><br><span class="line">                <span class="keyword">if</span> (coins[j] &lt;= i) &#123;</span><br><span class="line">                    dp[i] = min(dp[i], dp[i - coins[j]] + <span class="number">1</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[amount] &gt; amount ? <span class="number">-1</span> : dp[amount];</span><br><span class="line">      <span class="comment">//如果条件满足，则返回-1，否则返回dp[amount]</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态规划 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vim操作总结</title>
      <link href="2020/06/30/vim-chang-yong-ming-ling/"/>
      <url>2020/06/30/vim-chang-yong-ming-ling/</url>
      
        <content type="html"><![CDATA[<p>vim常用命令主要分为增删改查以及移动：</p><h2 id="vim插件的使用">vim插件的使用</h2><p>主要的配置全部在~/.vimrc 中，在使用之前，先安装插件管理工具。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim</span><br><span class="line"><span class="meta">#</span>如果没有安装成功，sudo /etc/hosts 中添加 151.101.108.133 raw.githubusercontent.com</span><br></pre></td></tr></table></figure><p>安装这个在<code>.vim/autoload</code>这个目录下之后，在vimrc里面使用下述命令即可安装各类插件，列出我常用的几个插件:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">call plug#begin('~/.vim/plugged')</span><br><span class="line">Plug 'vim-airline/vim-airline'</span><br><span class="line">Plug 'preservim/nerdcommenter'</span><br><span class="line">Plug 'jiangmiao/auto-pairs'</span><br><span class="line">Plug 'neoclide/coc.nvim'</span><br><span class="line">Plug 'neoclide/coc-python'</span><br><span class="line">Plug 'neoclide/coc-highlight'</span><br><span class="line">Plug 'preservim/nerdtree'</span><br><span class="line">Plug 'tomasr/molokai'</span><br><span class="line">Plug 'Yggdroot/LeaderF'</span><br><span class="line">Plug 'preservim/nerdcommenter'</span><br><span class="line">call plug#end()</span><br><span class="line"></span><br><span class="line">" theme</span><br><span class="line">let g:molokai_original=1</span><br><span class="line">let g:rehash256=1</span><br><span class="line">colorscheme molokai</span><br><span class="line"></span><br><span class="line">let g:airline_powerline_fonts = 1</span><br><span class="line">let g:airline#extensions#tabline#enabled = 1</span><br><span class="line">if !exists('g:airline_symbols')</span><br><span class="line">        let g:airline_symbols = &#123;&#125;</span><br><span class="line">endif</span><br><span class="line">let g:airline_right_sep = '◀'</span><br><span class="line">let g:airline_left_sep = '▶'</span><br><span class="line">let g:airline_left_alt_sep = '❯'</span><br><span class="line">let g:airline_right_alt_sep = '❮'</span><br><span class="line">let g:airline_symbols.linenr = '¶'</span><br><span class="line">let g:airline_symbols.branch = '⎇'</span><br><span class="line"></span><br><span class="line">set  mouse = r</span><br><span class="line"></span><br><span class="line">" NERDTree</span><br><span class="line">map &lt;C-n&gt; :NERDTreeToggle&lt;CR&gt;</span><br><span class="line"></span><br><span class="line">" Add spaces after comment delimiters by default</span><br><span class="line">let g:NERDSpaceDelims = 1</span><br><span class="line">" Use compact syntax for prettified multi-line comments</span><br><span class="line">let g:NERDCompactSexyComs = 1</span><br><span class="line">" Align line-wise comment delimiters flush left instead of following code indentation</span><br><span class="line">let g:NERDDefaultAlign = 'left'</span><br><span class="line">" Set a language to use its alternate delimiters by default</span><br><span class="line">let g:NERDAltDelims_java = 1</span><br><span class="line">" Add your own custom formats or override the defaults</span><br><span class="line">let g:NERDCustomDelimiters = &#123; 'c': &#123; 'left': '/**','right': '*/' &#125; &#125;</span><br><span class="line">" Allow commenting and inverting empty lines (useful when commenting a region)</span><br><span class="line">let g:NERDCommentEmptyLines = 1</span><br><span class="line">" Enable trimming of trailing whitespace when uncommenting</span><br><span class="line">let g:NERDTrimTrailingWhitespace = 1</span><br><span class="line">" Enable NERDCommenterToggle to check all selected lines is commented or not</span><br><span class="line">let g:NERDToggleCheckAllLines = 1</span><br></pre></td></tr></table></figure><p>直接复制粘贴到vimrc中，然后使用先保存整个文本， 然后再PlugInstall即可配置好整个vim了。</p><p>note: 'neoclide/coc.nvim'中的一些坑，首先要求安装好nodejs，但是会报错，这个时候在<code>.vim/coc.vim</code>下，使用yarn install 安装好，然后会提示在 <code>[coc.nvim] creating data directory: /Users/zhehu/.config/coc</code></p><h2 id="normal的模式下">Normal的模式下：</h2><h3 id="移动">移动</h3><p><code>:14</code>表示跳到14行；<code>gg</code>直接跳到行首 <code>:G</code>表示到最后一行 <code>y(yank)</code>表示复制 <code>p</code>表示粘贴</p><p><code>14gg</code>同样表示14行；<code>hjkl</code>是左上下右四个方向。<code>0</code>快速跳转到本行行首，<code>$</code>表示跳转到本行行尾</p><p><code>w</code>移动到下一个单词的开头，<code>b</code>移动回上一个单词。</p><p><code>ctrl+o</code> 表示跳到上一个位置</p><p><code>ctrl+f</code> 向下翻页</p><p><code>ctrl+u</code>向上翻页</p><h3 id="查-f-find">查: f (find)</h3><p><code>fs</code>f表示在某一行查找，这里就是表示查找一个出现s的字母。<code>Fs</code>和<code>fs</code>正好相反，表示反向查。使用 <code>；</code>查找下一个。</p><p><code>/</code>全文查单词某个</p><p><code>?</code> 全文逆向查某个单词</p><h3 id="改-c-change">改: c (change)</h3><p><code>ciw</code>(change inner word ) 会把这个单词删掉，然后在单词首处insert模式。</p><p><code>ct { ) ""</code> = <code>ci")&gt;</code> change to ""表示把包含在这个里面的单词改变，在首处进入到insert模式</p><h4 id="删-d-delete-note所有的删除操作实际上都是剪切操作都可以使用p剪切进去">删: d (delete note:所有的删除操作实际上都是剪切操作，都可以使用p剪切进去</h4><p><code>u</code>表示撤销操作；<code>dw</code>表示删除一个单词；<code>daw</code>表示把单词对应的空格也删掉，<code>diw</code>表示只删除不包含空格的单词</p><p><code>x</code>删除光标所对应的这个字母</p><p><code>dd</code>表示的是剪切整行</p><h2 id="在insert模式下">在insert模式下</h2><p>增加文本：</p><p>a append A</p><p>i insert I</p><p>o open new line O</p><p>大写表示反向</p><p><code>ctrl+w</code>在insert 模式下删除单词</p><h2 id="vim多窗口模式">vim多窗口模式</h2><h4 id="目录树使用">目录树使用</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">?: 快速帮助文档</span><br><span class="line">o: 打开一个目录或者打开文件，创建的是 buffer，也可以用来打开书签</span><br><span class="line">go: 打开一个文件，但是光标仍然留在 NERDTree，创建的是 buffer</span><br><span class="line">t: 打开一个文件，创建的是Tab，对书签同样生效</span><br><span class="line">T: 打开一个文件，但是光标仍然留在 NERDTree，创建的是 Tab，对书签同样生效</span><br><span class="line">i: 水平分割创建文件的窗口，创建的是 buffer</span><br><span class="line">gi: 水平分割创建文件的窗口，但是光标仍然留在 NERDTree</span><br><span class="line">s: 垂直分割创建文件的窗口，创建的是 buffer</span><br><span class="line">gs: 和 gi，go 类似</span><br><span class="line">x: 收起当前打开的目录</span><br><span class="line">X: 收起所有打开的目录</span><br><span class="line">e: 以文件管理的方式打开选中的目录</span><br><span class="line">D: 删除书签</span><br><span class="line"></span><br><span class="line">ctrl+w+w 光标在左右窗口切换</span><br><span class="line">ctrl+w+r 切换当前窗口左右布局</span><br><span class="line">gT 切换到前一个tab</span><br><span class="line">gt 切换到后一个tab</span><br><span class="line">g t新打开一个tab</span><br></pre></td></tr></table></figure><h4 id="vim下的命令">vim下的命令</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">:e file</span><br><span class="line"># 可以再打开一个文件，并且此时vim里会显示出file文件的内容。</span><br><span class="line">同时显示多个文件：</span><br><span class="line">:sp         //水平切分窗口</span><br><span class="line">:vsplit     //垂直切分窗口</span><br><span class="line">二、在文件之间切换：</span><br><span class="line">1.文件间切换</span><br><span class="line">Ctrl+6  //两文件间的切换</span><br><span class="line">:bn      //下一个文件</span><br><span class="line">:bp      //上一个文件</span><br><span class="line">:ls       //列出打开的文件，带编号</span><br><span class="line">:b1~n  //切换至第n个文件</span><br><span class="line">对于用(v)split在多个窗格中打开的文件，这种方法只会在当前窗格中切换不同的文件。</span><br><span class="line">2.在窗格间切换的方法</span><br><span class="line">Ctrl+w+方向键——切换到前／下／上／后一个窗格</span><br><span class="line">Ctrl+w+h/j/k/l ——同上</span><br><span class="line">Ctrl+ww——依次向后切换到下一个窗格中</span><br></pre></td></tr></table></figure><h4 id="other">Other:</h4><p>如何在vim中使用跳转功能。</p><p><strong>打开多个文件：</strong> 1.vim还没有启动的时候： 在终端里输入 vim file1 file2 ... filen便可以打开所有想要打开的文件 2.vim已经启动</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入</span><br><span class="line">:open file</span><br><span class="line">可以再打开一个文件，并且此时vim里会显示出file文件的内容。</span><br><span class="line"></span><br><span class="line">:e ../myFile.pl</span><br><span class="line"></span><br><span class="line">1. vim 文档名  普通方式打开文档   </span><br><span class="line">2. vim +n 文档名   打开文档后，定位第n行</span><br><span class="line">3. vim ，进入vim界面之后使用命令 :e 文档名 打开文档，此方式可以在编辑一个文档的同时打开另外一个文档</span><br></pre></td></tr></table></figure><p><strong>同时显示多个文件：</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">:split  简写 :sp</span><br><span class="line">:vsplit 简写 :vsp</span><br><span class="line">显示缓存  :ls</span><br></pre></td></tr></table></figure><p><strong>在文件之间切换：</strong> 1.文件间切换</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Ctrl+6—下一个文件</span><br><span class="line">:bn—下一个文件</span><br><span class="line">:bp—上一个文件</span><br></pre></td></tr></table></figure><p>对于用(v)split在多个窗格中打开的文件，这种方法只会在当前窗格中切换不同的文件。</p><p>对于用(v)split在多个窗格中打开的文件，这种方法只会在当前窗格中切换不同的文件。 2.在窗格间切换的方法</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Ctrl+w+方向键——切换到前／下／上／后一个窗格</span><br><span class="line">Ctrl+w+h/j/k/l ——同上</span><br><span class="line">Ctrl+ww——依次向后切换到下一个窗格中</span><br></pre></td></tr></table></figure><p>3.多文档编辑的命令如下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">:n     编辑下一个文档。</span><br><span class="line">:2n    编辑下两个文档。</span><br><span class="line">:N     编辑上一个文档。注意，该方法只能用于同时打开多个文档。</span><br><span class="line">:e 文档名        这是在进入vim后，不离开 vim 的情形下打开其他文档。</span><br><span class="line">:e# 或 Ctrl+ˆ   编辑上一个文档,用于两个文档相互交换编辑时使用。?# 代表的是编辑前一次编辑的文档</span><br><span class="line">:files 或 :buffers 或 :ls   可以列出目前 缓冲区 中的所有文档。加号 + 表示 缓冲区已经被修改过了。＃代表上一次编辑的文档，%是目前正在编辑中的文档</span><br><span class="line">:b 文档名或编号   移至该文档。</span><br><span class="line">:f 或 Ctrl+g   显示当前正在编辑的文档名称。</span><br><span class="line">:f 檔名     改变编辑中的文档名。(file)</span><br></pre></td></tr></table></figure><p><strong>多文件切换</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 通过vim打开多个文件（可以通过ctags或者cscope）</span><br><span class="line">2. &quot;:ls&quot;查看当前打开的buffer（文件）</span><br><span class="line">3. &quot;:b num&quot;切换文件（其中num为buffer list中的编号）</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vim </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shell语法常见总结</title>
      <link href="2020/06/02/shell-yu-fa-zong-jie/"/>
      <url>2020/06/02/shell-yu-fa-zong-jie/</url>
      
        <content type="html"><![CDATA[<p>在服务器端运行，离不开shell的使用。今天主要总结一下shell的用法：</p><p><strong>Shell 是什么？</strong> Shell 是一个命令解释权，它为用户提供了一个向 Linux 内核发送请求以便运行程序界面系统级程序，用户可以用 Shell 来启动、挂起、停止甚至是编写一些程序。</p><p><img src="Shell语法总结/image-20200602232857092.png" alt="image-20200602232857092" style="zoom:33%;"></p><p>写一个shell文件，第一行表示用bash执行，第二行打印出“hello world”</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash </span><br><span class="line">echo 'hello world!'</span><br></pre></td></tr></table></figure><p>执行的方法有两种，一种是 <code>sh hello.sh</code> 另外一种方法是 <code>chmod +x hello.sh</code> 之后，这样<code>./hello.sh</code> 就变成了可执行文件，可以直接运行。</p><p>说明:</p><ul><li><code>#!</code> 告诉系统这个脚本需要什么解释器来执行。</li><li>文件扩展名 <code>.sh</code> 不是强制要求的。</li><li>方法1 直接运行解释器，<code>hello.sh</code> 作为 Shell 解释器的参数。此时 Shell 脚本就不需要指定解释器信息，第一行可以去掉。</li><li>方法2 hello.sh 作为可执行程序运行，Shell 脚本第一行一定要指定解释器。</li></ul><p><strong>Shell 变量</strong> <strong>定义</strong> Shell 变量分为<strong>系统变量</strong>和<strong>自定义变量</strong>。系统变量有<span class="math inline">\(HOME、\)</span>PWD、$USER等，显示当前 Shell 中所有变量：<code>set</code> 。 变量名可以由字母、数字、下划线组成，不能以数字开头。 <strong>基本语法</strong></p><ul><li><strong>定义变量：</strong>变量名=变量值，等号两侧不能有空格，变量名一般习惯用大写。</li><li><strong>删除变量：</strong>unset 变量名 。</li><li><strong>声明静态变量：</strong>readonly 变量名，静态变量不能unset。</li><li><strong>使用变量：</strong>$变量名</li></ul><p><strong>将命令返回值赋给变量（重点）</strong></p><ul><li>A= `ls` 反引号,执行里面的命令</li><li>A=$(ls) 等价于反引号</li></ul><p><strong>Shell 环境变量</strong> （常用） <strong>定义</strong></p><p><img src="Shell语法总结/image-20200602233553542.png" alt="image-20200602233553542" style="zoom: 33%;"></p><p><strong>基本语法</strong></p><ol type="1"><li>export 变量名=变量值，将 Shell 变量输出为环境变量。</li><li>source 配置文件路径，让修改后的配置信息立即生效。</li><li>echo $变量名，检查环境变量是否生效</li></ol><p><strong>位置参数变量</strong>（常用） <strong>基本语法</strong></p><ul><li>$n ：$0 代表命令本身、$1-$9 代表第1到9个参数，10以上参数用花括号，如 ${10}。</li><li>$* ：命令行中所有参数，且把所有参数看成一个整体。</li><li>$@ ：命令行中所有参数，且把每个参数区分对待。</li><li>$# ：所有参数个数。</li></ul><p><strong>预定义变量</strong> <strong>定义</strong> 在赋值定义之前，事先在 Shell 脚本中直接引用的变量。 <strong>基本语法</strong></p><ul><li>$$ ：当前进程的 PID 进程号。</li><li>$! ：后台运行的最后一个进程的 PID 进程号。</li><li>$? ：最后一次执行的命令的返回状态，0为执行正确，非0执行失败。</li></ul><p><strong>运算符</strong> <strong>基本语法</strong></p><ul><li>$((运算式)) 或 $[运算式]</li><li>expr m + n 注意 expr 运算符间要有空格</li><li>expr m - n</li><li>expr \*，/，% 分别代表乘，除，取余</li></ul><h2 id="流程控制"><strong>流程控制</strong></h2><h3 id="if-判断"><strong>if 判断</strong></h3><h3 id="基本语法"><strong>基本语法</strong></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ 条件判断式 ];then   </span><br><span class="line">    程序   </span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span> 或者（推荐）</span><br><span class="line">if [ 条件判断式 ]</span><br><span class="line">then</span><br><span class="line">    程序</span><br><span class="line">elif [ 条件判断式 ]</span><br><span class="line">then</span><br><span class="line">    程序</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="for-循环"><strong>for 循环</strong></h3><h3 id="基本语法-1"><strong>基本语法</strong></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> 语法1</span><br><span class="line">for 变量名 in 值1 值2 值3...</span><br><span class="line">do</span><br><span class="line">    程序</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 语法2</span><br><span class="line">for ((初始值;循环控制条件;变量变化))</span><br><span class="line">do</span><br><span class="line">    程序</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="while-循环"><strong>while 循环</strong></h3><h3 id="基本语法-2"><strong>基本语法</strong></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while [ 条件判断式 ]</span><br><span class="line">do</span><br><span class="line">    程序</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h2 id="读取控制台输入"><strong>读取控制台输入</strong></h2><h3 id="基本语法-3"><strong>基本语法</strong></h3><p>read (选项) (参数) <strong>选项</strong></p><ul><li>-p：指定读取值时的提示符</li><li>-t：指定读取值时等待的时间（秒），如果没有在指定时间内输入，就不再等待了。</li></ul><p><strong>参数</strong></p><ul><li>变量名：读取值的变量名</li></ul><h2 id="函数"><strong>函数</strong></h2><p>和其它编程语言一样，Shell 编程有系统函数和自定义函数，本文只举两个常用系统函数。</p><h3 id="系统函数"><strong>系统函数</strong></h3><ul><li><p>basename，删掉路径最后一个 / 前的所有部分（包括/），常用于获取文件名。</p><p><strong>基本语法</strong></p></li><li><ul><li><p>basename [pathname] [suffx]</p></li><li><p>basename [string] [suffx]</p></li><li><p>如果指定 suffx，也会删掉pathname或string的后缀部分。</p></li></ul></li><li><p>dirname，删掉路径最后一个 / 后的所有部分（包括/），常用于获取文件路径。 <strong>基本语法</strong></p></li><li><p>dirname pathname</p></li><li><p>如果路径中不含 / ，则返回 '.' （当前路径）。</p></li></ul><h3 id="自定义函数"><strong>自定义函数</strong></h3><h3 id="基本语法-4"><strong>基本语法</strong></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[ function ] funname[()]</span><br><span class="line">&#123;</span><br><span class="line">    Action;</span><br><span class="line">    [return int;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 调用</span><br><span class="line">funname 参数1 参数2...</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> program </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>leetcode题解</title>
      <link href="2020/05/08/leetcode-283/"/>
      <url>2020/05/08/leetcode-283/</url>
      
        <content type="html"><![CDATA[<p>这道题是也是我在面试的过程中碰到的一个原题，这里写一下解法。</p><p>题目描述是这样的， 给定一个数组 nums，编写一个函数将所有 0 移动到数组的末尾，同时保持非零元素的相对顺序。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">示例:</span><br><span class="line">输入: [0,1,0,3,12]</span><br><span class="line">输出: [1,3,12,0,0]</span><br></pre></td></tr></table></figure><p><strong>说明</strong>:</p><ol type="1"><li>必须在原数组上操作，不能拷贝额外的数组。</li><li>尽量减少操作次数。</li></ol><p>初次看，是想新开一个数组，然后把所有的零丢到最后，但是这样不满足题目中给定的要求。</p><p>或者用哈希表把0元素存储起来，然后再把元素丢到后面。但是这样同样需要新开一个数组，需要的空间复杂度为O(n).</p><p>最佳解法，双指针，解题思路可以选择在数组中用指针进行数组的index交换。其中一个数组必须指向第一个index，另外一个数组从零开始进行遍历，如果在首端的指针碰到了零，那么就选择与末端的指针索引对应的数进行交换。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">2</span>]</span><br><span class="line">first=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">    <span class="keyword">if</span> nums[i]!=<span class="number">0</span>:</span><br><span class="line">        nums[i],nums[first]=nums[first],nums[i]</span><br><span class="line">        first+=<span class="number">1</span></span><br><span class="line">print(nums)</span><br></pre></td></tr></table></figure><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> movezeros=<span class="function"><span class="keyword">function</span>(<span class="params">nums</span>)</span>&#123;</span><br><span class="line"><span class="keyword">let</span> anchor=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">let</span> explorer=<span class="number">0</span>;explorer&lt;nums.length;explorer++)&#123;</span><br><span class="line">     <span class="keyword">if</span> (nums[explorer]!==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">let</span> temp=nums[anchor];</span><br><span class="line">            nums[anchor]=nums[explorer];</span><br><span class="line">            nums[explorer]=temp;</span><br><span class="line">            </span><br><span class="line">            anchor++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>leetcode-二叉树题解I</title>
      <link href="2020/03/22/leetcode-2/"/>
      <url>2020/03/22/leetcode-2/</url>
      
        <content type="html"><![CDATA[<h4 id="二叉树的特点">二叉树的特点</h4><p>树是一种十分重要的数据结构，是一种抽象数据类型或是实现这种抽象数据类型的数据结构，用来模拟具有树状结构性质的数据集合。<strong>它是由n(n&gt;0)个有限节点组成一个具有层次关系的集合。</strong></p><p>它具有以下的特点：</p><ul><li><p>每个节点都只有有限个子节点或无子节点；</p></li><li><p>没有父节点的节点称为根节点；</p></li><li><p>每一个非根节点有且只有一个父节点；</p></li><li><p>除了根节点外，每个子节点可以分为多个不相交的子树；</p></li><li><p>树里面没有环路。</p><p>因此，树的定义为：</p></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span>：</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">__init__</span><span class="params">(self,x)</span>:</span></span><br><span class="line">self.val=x</span><br><span class="line">         self.left=Null</span><br><span class="line">         self.right=Null</span><br><span class="line"> <span class="comment"># NULL可以占位，但是表示空值</span></span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TreeNode</span> &#123;</span></span><br><span class="line"><span class="keyword">int</span> val;</span><br><span class="line">TreeNode *left;</span><br><span class="line">TreeNode *right;</span><br><span class="line">TreeNode  (<span class="keyword">int</span> x)：val(x),left(<span class="literal">NULL</span>),right(<span class="literal">NULL</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>了解一个数据结构，必须先学会如何遍历和查找。</p><p>遍历二叉树的数据结构有三种方式，分别是前序遍历，中序遍历和后序遍历。代码上的实现非常类似，但要了解里面的原理。</p><p>前序遍历：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>；</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">solution</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">preorderTravsal</span><span class="params">(Treenode* root)</span></span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">        __preorderTraversal(root,res);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">FindBianryTree</span><span class="params">(TreeNode* Tree, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;res )</span></span>&#123;</span><br><span class="line"><span class="keyword">if</span>(node)&#123;</span><br><span class="line">            res.push_back(node-&gt;val);</span><br><span class="line">            __preorderTraversal(node-&gt;left,res);</span><br><span class="line">            __preorderTraversal(node-&gt;right,res);</span><br><span class="line">        &#125;    </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>中序遍历:</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>；</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">solution</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">midorderTravsal</span><span class="params">(Treenode* root)</span></span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">        __midorderTraversal(root,res);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">FindBianryTree</span><span class="params">(TreeNode* Tree, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;res )</span></span>&#123;</span><br><span class="line"><span class="keyword">if</span>(node)&#123;</span><br><span class="line">            res.push_back(node-&gt;val);</span><br><span class="line">            __midorderTraversal(node-&gt;left,res);</span><br><span class="line">            __midorderTraversal(node-&gt;right,res);</span><br><span class="line">        &#125;    </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>后序遍历：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>；</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">solution</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">aftorderTravsal</span><span class="params">(Treenode* root)</span></span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">        __aftorderTraversal(root,res);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">FindBianryTree</span><span class="params">(TreeNode* Tree, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;res )</span></span>&#123;</span><br><span class="line"><span class="keyword">if</span>(node)&#123;</span><br><span class="line">            res.push_back(node-&gt;val);</span><br><span class="line">            __aftorderTraversal(node-&gt;left,res);</span><br><span class="line">            __aftorderTraversal(node-&gt;right,res);</span><br><span class="line">        &#125;    </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>了解了遍历的基本特点之后，可以解决相关的遍历的算法问题了。</p><h4 id="leetcode404计算给定二叉树的所有左叶子之和">Leetcode404:计算给定二叉树的所有左叶子之和：</h4><p>计算给定二叉树的所有左叶子之和。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">    3</span><br><span class="line">   / \</span><br><span class="line">  9  20</span><br><span class="line">    /  \</span><br><span class="line">   15   7</span><br><span class="line"></span><br><span class="line">在这个二叉树中，有两个左叶子，分别是 9 和 15，所以返回 24</span><br></pre></td></tr></table></figure><p>使用递归的方法，总和等于左子树的值加上右子树的值，终止条件是当前节点为左子叶或者为空节点。代码如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sumOfLeftLeaves</span><span class="params">(self, root: TreeNode)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">and</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.left.left <span class="keyword">and</span> <span class="keyword">not</span> root.left.right:</span><br><span class="line">            <span class="keyword">return</span> root.left.val+self.sumOfLeftLeaves(root.right)</span><br><span class="line">        <span class="keyword">return</span> self.sumOfLeftLeaves(root.left) + self.sumOfLeftLeaves(root.right)</span><br></pre></td></tr></table></figure><h4 id="leetcode100相同的树">Leetcode100:相同的树</h4><p>给定两个二叉树，编写一个函数来检验它们是否相同。如果两个树在结构上相同，并且节点具有相同的值，则认为它们是相同的。</p><p>示例1:</p><pre><code>  输入:    1         1          / \       / \         2   3     2   3  [1,2,3],   [1,2,3]  输出: true</code></pre><p>示例2：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入:      1          1</span><br><span class="line">          /           \</span><br><span class="line">         2             2</span><br><span class="line"></span><br><span class="line">        [1,2],     [1,null,2]</span><br><span class="line">输出: false</span><br></pre></td></tr></table></figure><p>使用递归。首先判断 <code>p</code> 和 <code>q</code> 是不是 <code>None</code>，然后判断它们的值是否相等。若以上判断通过，则递归对子结点做同样操作。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSameTree</span><span class="params">(self, p, q)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type p: TreeNode</span></span><br><span class="line"><span class="string">        :type q: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        """</span> </span><br><span class="line">        <span class="comment"># p and q are both None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> p <span class="keyword">and</span> <span class="keyword">not</span> q:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="comment"># one of p and q is None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> q <span class="keyword">or</span> <span class="keyword">not</span> p:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> p.val != q.val:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> self.isSameTree(p.right, q.right) <span class="keyword">and</span> \</span><br><span class="line">               self.isSameTree(p.left, q.left)</span><br></pre></td></tr></table></figure><h4 id="leetcode474-路径总和ii">leetcode474: 路径总和II</h4><pre><code>给定一个二叉树，它的每个结点都存放着一个整数值。找出路径和等于给定数值的路径总数。路径不需要从根节点开始，也不需要在叶子节点结束，但是路径方向必须是向下的（只能从父节点到子节点）。二叉树不超过1000个节点，且节点数值范围是 [-1000000,1000000] 的整数。</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">示例：</span><br><span class="line">root = [10,5,-3,3,2,null,11,3,-2,null,1], sum = 8</span><br><span class="line"></span><br><span class="line">      10</span><br><span class="line">     /  \</span><br><span class="line">    5   -3</span><br><span class="line">   / \    \</span><br><span class="line">  3   2   11</span><br><span class="line"> / \   \</span><br><span class="line">3  -2   1</span><br><span class="line"></span><br><span class="line">返回 3。和等于 8 的路径有:</span><br><span class="line"></span><br><span class="line">1.  5 -&gt; 3</span><br><span class="line">2.  5 -&gt; 2 -&gt; 1</span><br><span class="line">3.  -3 -&gt; 11</span><br></pre></td></tr></table></figure><p>使用单递归dfs：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pathSum</span><span class="params">(self, root: TreeNode, sum: int)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># sums为node的父节点已能构成的和，返回最长可延伸到node结束的所有路径所能构成的和列表</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(node, sums)</span>:</span></span><br><span class="line">        left = right = <span class="number">0</span>  <span class="comment"># 左右的值默认为0</span></span><br><span class="line">        <span class="comment"># 之前的和加当前结点值能构成的新和，以及从当前结点开始算的新和</span></span><br><span class="line">        temp = [num + node.val <span class="keyword">for</span> num <span class="keyword">in</span> sums] + [node.val]</span><br><span class="line">        <span class="keyword">if</span> node.left:</span><br><span class="line">            left = dfs(node.left, temp)  <span class="comment"># 递归</span></span><br><span class="line">        <span class="keyword">if</span> node.right:</span><br><span class="line">            right = dfs(node.right, temp)  <span class="comment"># 递归</span></span><br><span class="line">        <span class="keyword">return</span> temp.count(sum) + left + right</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dfs(root, [])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态规划简单归纳</title>
      <link href="2020/03/17/leetcode/"/>
      <url>2020/03/17/leetcode/</url>
      
        <content type="html"><![CDATA[<p>动态规划题主要的思路步骤就是: 将问题做拆解，找到问题之间具体的联系；状态定义；递推方程推导；实现。</p><p>这里还是拿 Quora 上面的例子来讲解，“1+1+1+1+1+1+1+1” 得出答案是 8，那么如何快速计算 “1+ 1+1+1+1+1+1+1+1”，我们首先可以对这个大的问题进行拆解，这里我说的大问题是 9 个 1 相加，这个问题可以拆解成 1 + “8 个 1 相加的答案”，8 个 1 相加继续拆，可以拆解成 1 + “7 个 1 相加的答案”，… 1 + “0 个 1 相加的答案”，到这里，<strong>第一个步骤</strong> 已经完成。【初始化定义为1，后面的都是+1】</p><p>定义好了状态，递推方程就变得非常简单，就是 <code>dp[i] = dp[i - 1] + 1</code>，这里的 <code>dp[i]</code> 记录的是当前问题的答案，也就是当前的状态，<code>dp[i - 1]</code> 记录的是之前相邻的问题的答案，也就是之前的状态，它们之间通过 +1 来实现状态的变更。</p><p>最后一步就是实现了，有了状态表示和递推方程，实现这一步上需要重点考虑的其实是初始化，就是用什么样的数据结构，根据问题的要求需要做那些初始值的设定。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">dpExample</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[n + <span class="number">1</span>];  <span class="comment">// 多开一位用来存放 0 个 1 相加的结果</span></span><br><span class="line"></span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">0</span>;      <span class="comment">// 0 个 1 相加等于 0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; ++i) &#123;</span><br><span class="line">        dp[i] = dp[i - <span class="number">1</span>] + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dp[n];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你可以看到，动态规划这四个步骤其实是相互递进的，状态的定义离不开问题的拆解，递推方程的推导离不开状态的定义，最后的实现代码的核心其实就是递推方程，这中间如果有一个步骤卡壳了则会导致问题无法解决，当问题的复杂程度增加的时候，这里面的思维复杂程度会上升。</p><h5 id="leetcode70爬楼梯">Leetcode70：爬楼梯：</h5><p>假设你正在爬楼梯。需要 <em>n</em> 阶你才能到达楼顶。每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？注意：给定 <em>n</em> 是一个正整数。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：2</span><br><span class="line">输出：2</span><br><span class="line">解释： 有两种方法可以爬到楼顶。</span><br><span class="line"></span><br><span class="line">1. 1 阶 + 1 阶</span><br><span class="line">2. 2 阶</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：3</span><br><span class="line">输出：3</span><br><span class="line">解释： 有三种方法可以爬到楼顶。</span><br><span class="line"></span><br><span class="line">1. 1 阶 + 1 阶 + 1 阶</span><br><span class="line">2. 1 阶 + 2 阶</span><br><span class="line">3. 2 阶 + 1 阶</span><br></pre></td></tr></table></figure><p><strong>问题拆解</strong>：</p><p>我们到达第 n 个楼梯可以从第 n – 1 个楼梯和第 n – 2 个楼梯到达，因此第 n 个问题可以拆解成第 n – 1 个问题和第 n – 2 个问题，第 n – 1 个问题和第 n – 2 个问题又可以继续往下拆，直到第 0 个问题，也就是第 0 个楼梯 (起点)</p><p><strong>状态定义</strong></p><p>“问题拆解” 中已经提到了，第 n 个楼梯会和第 n – 1 和第 n – 2 个楼梯有关联，那么具体的联系是什么呢？你可以这样思考，第 n – 1 个问题里面的答案其实是从起点到达第 n – 1 个楼梯的路径总数，n – 2 同理，从第 n – 1 个楼梯可以到达第 n 个楼梯，从第 n – 2 也可以，并且路径没有重复，因此我们可以把第 i 个状态定义为 “<strong>从起点到达第 i 个楼梯的路径总数</strong>”，状态之间的联系其实是相加的关系。</p><p><strong>递推方程</strong></p><p>“状态定义” 中我们已经定义好了状态，也知道第 i 个状态可以由第 i – 1 个状态和第 i – 2 个状态通过相加得到，因此递推方程就出来了 <code>dp[i] = dp[i - 1] + dp[i - 2]</code></p><p><strong>实现</strong></p><p>你其实可以从递推方程看到，我们需要有一个初始值来方便我们计算，起始位置不需要移动 <code>dp[0] = 0</code>，第 1 层楼梯只能从起始位置到达，因此 <code>dp[1] = 1</code>，第 2 层楼梯可以从起始位置和第 1 层楼梯到达，因此 <code>dp[2] = 2</code>，有了这些初始值，后面就可以通过这几个初始值进行递推得到。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="leetcode-第-120-号问题三角形最小路径和">LeetCode 第 120 号问题：三角形最小路径和。</h5><p>给定一个三角形，找出自顶向下的最小路径和。每一步只能移动到下一行中相邻的结点上。</p><p>例如，给定三角形：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">     [2],</span><br><span class="line">    [3,4],</span><br><span class="line">   [6,5,7],</span><br><span class="line">  [4,1,8,3]</span><br><span class="line">]</span><br></pre></td></tr></table></figure><ul><li>问题拆解：</li></ul><p>这里的总问题是求出最小的路径和，路径是这里的分析重点，路径是由一个个元素组成的，和之前爬楼梯那道题目类似，<code>[i][j]</code> 位置的元素，经过这个元素的路径肯定也会经过 <code>[i - 1][j]</code> 或者 <code>[i - 1][j - 1]</code>，因此经过一个元素的路径和可以通过这个元素上面的一个或者两个元素的路径和得到。</p><ul><li><p>状态定义</p><p>状态的定义一般会和问题需要求解的答案联系在一起，这里其实有两种方式，一种是考虑路径从上到下，另外一种是考虑路径从下到上，因为元素的值是不变的，所以路径的方向不同也不会影响最后求得的路径和，如果是从上到下，你会发现，在考虑下面元素的时候，起始元素的路径只会从<code>[i - 1][j]</code> 获得，每行当中的最后一个元素的路径只会从 <code>[i - 1][j - 1]</code> 获得，中间二者都可，这样不太好实现，因此这里考虑从下到上的方式，状态的定义就变成了 “<strong>最后一行元素到当前元素的最小路径和</strong>”，对于 <code>[0][0]</code> 这个元素来说，最后状态表示的就是我们的最终答案。</p></li><li><p>递推方程</p><p>“状态定义” 中我们已经定义好了状态，递推方程就出来了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dp[i][j] = Math.min(dp[i + 1][j], dp[i + 1][j + 1]) + triangle[i][j]</span><br></pre></td></tr></table></figure><p>实现</p><p>这里初始化时，我们需要将最后一行的元素填入状态数组中，然后就是按照前面分析的策略，从下到上计算即可</p></li></ul><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minimumTotal</span><span class="params">(List&lt;List&lt;Integer&gt;&gt; triangle)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = triangle.size();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[n][n];</span><br><span class="line"></span><br><span class="line">    List&lt;Integer&gt; lastRow = triangle.get(n - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        dp[n - <span class="number">1</span>][i] = lastRow.get(i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = n - <span class="number">2</span>; i &gt;= <span class="number">0</span>; --i) &#123;</span><br><span class="line">        List&lt;Integer&gt; row = triangle.get(i);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; i + <span class="number">1</span>; ++j) &#123;</span><br><span class="line">            dp[i][j] = Math.min(dp[i + <span class="number">1</span>][j], dp[i + <span class="number">1</span>][j + <span class="number">1</span>]) + row.get(j);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dp[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>LeetCode 第 53 号问题：最大子序和。</p><p>给定一个整数数组 <em>nums</em> ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: [-2,1,-3,4,-1,2,1,-5,4],</span><br><span class="line">输出: 6</span><br><span class="line">解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。</span><br></pre></td></tr></table></figure><p>问题拆解：</p><p>问题的核心是子数组，子数组可以看作是一段区间，因此可以由起始点和终止点确定一个子数组，两个点中，我们先确定一个点，然后去找另一个点，比如说，如果我们确定一个子数组的截止元素在 i 这个位置，这个时候我们需要思考的问题是 “<strong>以 i 结尾的所有子数组中，和最大的是多少？</strong>”，然后我们去试着拆解，这里其实只有两种情况：</p><p>i 这个位置的元素自成一个子数组;</p><p>i 位置的元素的值 + <strong>以 i – 1 结尾的所有子数组中的子数组和最大的值</strong></p><p>你可以看到，我们把第 i 个问题拆成了第 i – 1 个问题，之间的联系也变得清晰</p><ul><li><p>状态定义</p><p>通过上面的分析，其实状态已经有了，<code>dp[i]</code> 就是 “<strong>以 i 结尾的所有子数组的最大值</strong>”</p></li><li><p>递推方程</p><p>拆解问题的时候也提到了，有两种情况，即当前元素自成一个子数组，另外可以考虑前一个状态的答案，于是就有了。</p></li><li><p>实现</p></li></ul><p>题目要求子数组不能为空，因此一开始需要初始化，也就是 <code>dp[0] = array[0]</code>，保证最后答案的可靠性，另外我们需要用一个变量记录最后的答案，因为子数组有可能以数组中任意一个元素结尾</p><p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (nums == null || nums.length == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> n = nums.length;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line"></span><br><span class="line">    dp[<span class="number">0</span>] = nums[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> result = dp[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        dp[i] = Math.max(dp[i - <span class="number">1</span>], <span class="number">0</span>) + nums[i];</span><br><span class="line">        result = Math.max(result, dp[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dynamic programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN，LSTM算法图解</title>
      <link href="2020/02/08/lstm-and-rnn/"/>
      <url>2020/02/08/lstm-and-rnn/</url>
      
        <content type="html"><![CDATA[<h4 id="rnn的工作原理">RNN的工作原理：</h4><p>RNN可以被认为是一种有记忆力的neural network。在RNN里面，每一次hidden layer的neuron产生output的时候，这个output会被存到memory里面。当下次又有input的时候，需要和上一次的memory结合起来使用。对它来说除了<span class="math inline">\(x_1,x_2\)</span>以外，这些存在memory里的值<span class="math inline">\(a_1,a_2\)</span>也会影响它的output。</p><p><img src="LSTM-and-RNN/image-20200226154228719.png" alt="image-20200226154228719" style="zoom: 25%;"></p><p>如上图所示, 用Recurrent Neural Network处理slot filling这件事，就像是这样，使用者说：“arrive Taipei on November 2nd”，arrive就变成了一个vector丢到neural network里面去，neural network的hidden layer的output写成<span class="math inline">\(a^1\)</span>(<span class="math inline">\(a^1\)</span>是一排neural的output，是一个vector)，<span class="math inline">\(a^1\)</span>产生<span class="math inline">\(y^1\)</span>,<span class="math inline">\(y^1\)</span>就是“arrive”属于每一个slot filling的几率。接下来<span class="math inline">\(a^1\)</span>会被存到memory里面去，"Taipei会变为input"，这个hidden layer会同时考虑“Taipei”这个input和存在memory里面的<span class="math inline">\(a^1\)</span>,得到<span class="math inline">\(a^2\)</span>，根据<span class="math inline">\(a^2\)</span>得到<span class="math inline">\(y^2\)</span>，<span class="math inline">\(y^2\)</span>是属于每一个slot filling的几率。以此类推(<span class="math inline">\(a^3\)</span>得到<span class="math inline">\(y^2\)</span>)。</p><p>通过这样的方式，RNN就有了推断一句话意思的能力，他不仅仅看到了taibei这个词，也记住了arrive这个词，就可以做自动化做更多的事情。</p><h4 id="lstm的网络架构">LSTM的网络架构：</h4><p><img src="LSTM-and-RNN/image-20200226154152676.png" alt="image-20200226154152676" style="zoom:25%;"></p><p>LSTM的网络架构，相比之下复杂了很多，有四个input，一个output，这构成了这个神经网络。分别是一个是想要存进memory的值，input-gate, forget-gate, output-gate，以及output。这个值输出，那么必须保证input_gate打开，forget_gate不会忘掉，且output_gate开启，而这三个值都会由neural network学到是1还是0.</p><p>我们看一下下面这个已经赋值好的权重的神经网络的输入和输出情况。</p><p><img src="LSTM-and-RNN/image-20200226174241356.png" alt="image-20200226174241356" style="zoom:25%;"></p><p>input的三维vector乘以linear transform以后所得到的结果(<span class="math inline">\(x_1\)</span>,<span class="math inline">\(x_2\)</span>,<span class="math inline">\(x_3\)</span>乘以权重再加上bias)，这些权重和bias是哪些值是通过train data用GD学到的。 假设我已经知道这些值是多少了，那用这样的输入会得到什么样的输出。那我们就实际的运算一下。其中绿色的就是bias，</p><p>在实际运算之前，我们先根据它的input，参数分析下可能会得到的结果。底下这个外界传入的cell，<span class="math inline">\(x_1\)</span>乘以1，其他的vector乘以0，所以就直接把<span class="math inline">\(x_1\)</span>当做输入。在input gate时，<span class="math inline">\(x_2\)</span>乘以100，bias乘以-10(假设<span class="math inline">\(x_2\)</span>是没有值的话，通常input gate是关闭的(bias等于-10)因为-10通过sigmoid函数之后会接近0，所以就代表是关闭的，若<span class="math inline">\(x_2\)</span>的值大于1的话，结果会是一个正值，代表input gate会被打开) 。forget gate通常会被打开的，因为他的bias等于10(它平常会一直记得东西)，只有当<span class="math inline">\(x_2\)</span>的值为一个很大的负值时，才会把forget gate关起来。output gate平常是被关闭的，因为bias是一个很大的负值，若<span class="math inline">\(x_3\)</span>有一个很大的正值的话，压过bias把output打开。</p><h4 id="lstm的原理">LSTM的原理：</h4><p><img src="LSTM-and-RNN/image-20200226174701864.png" alt="image-20200226174701864" style="zoom:33%;"></p><p>现在的input <span class="math inline">\(x_1,x_2\)</span>会乘以不同的weight当做LSTM不同的输入假设我们这个hidden layer只有两个neuron，但实际上是有很多的neuron。input<span class="math inline">\(x_1,x_2\)</span>会乘以不同的weight会去操控output gate，乘以不同的weight操控input gate，乘以不同的weight当做底下的input，乘以不同的weight当做forget gate。第二个LSTM也是一样的。所以LSTM是有四个input跟一个output，对于LSTM来说，这四个input是不一样的。在原来的neural network里是一个input一个output。在LSTM里面它需要四个input，它才能产生一个output。</p><p>LSTM的最终形态：</p><p><img src="LSTM-and-RNN/image-20200226174805623.png" alt="image-20200226174805623" style="zoom:33%;"></p><p>其代码实现如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#single lstm cell</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell</span><span class="params">(batch_dataset, prev_activation_matrix, prev_cell_matrix, parameters)</span>:</span></span><br><span class="line">    <span class="comment">#get parameters</span></span><br><span class="line">    fgw = parameters[<span class="string">'fgw'</span>]</span><br><span class="line">    igw = parameters[<span class="string">'igw'</span>]</span><br><span class="line">    ogw = parameters[<span class="string">'ogw'</span>]</span><br><span class="line">    ggw = parameters[<span class="string">'ggw'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#concat batch data and prev_activation matrix</span></span><br><span class="line">    concat_dataset = np.concatenate((batch_dataset,prev_activation_matrix),axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#forget gate activations</span></span><br><span class="line">    fa = np.matmul(concat_dataset,fgw)</span><br><span class="line">    fa = sigmoid(fa)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#input gate activations</span></span><br><span class="line">    ia = np.matmul(concat_dataset,igw)</span><br><span class="line">    ia = sigmoid(ia)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#output gate activations</span></span><br><span class="line">    oa = np.matmul(concat_dataset,ogw)</span><br><span class="line">    oa = sigmoid(oa)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#gate gate activations</span></span><br><span class="line">    ga = np.matmul(concat_dataset,ggw)</span><br><span class="line">    ga = tanh_activation(ga)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#new cell memory matrix</span></span><br><span class="line">    cell_memory_matrix = np.multiply(fa,prev_cell_matrix) + np.multiply(ia,ga)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#current activation matrix</span></span><br><span class="line">    activation_matrix = np.multiply(oa, tanh_activation(cell_memory_matrix))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#lets store the activations to be used in back prop</span></span><br><span class="line">    lstm_activations = dict()</span><br><span class="line">    lstm_activations[<span class="string">'fa'</span>] = fa</span><br><span class="line">    lstm_activations[<span class="string">'ia'</span>] = ia</span><br><span class="line">    lstm_activations[<span class="string">'oa'</span>] = oa</span><br><span class="line">    lstm_activations[<span class="string">'ga'</span>] = ga</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> lstm_activations,cell_memory_matrix,activation_matrix</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">output_cell</span><span class="params">(activation_matrix,parameters)</span>:</span></span><br><span class="line">    <span class="comment">#get hidden to output parameters</span></span><br><span class="line">    how = parameters[<span class="string">'how'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#get outputs </span></span><br><span class="line">    output_matrix = np.matmul(activation_matrix,how)</span><br><span class="line">    output_matrix = softmax(output_matrix)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output_matrix</span><br></pre></td></tr></table></figure><p><a href="https://www.youtube.com/watch?v=xCGidAeyS4M" target="_blank" rel="noopener">主要参考了台大李宏毅老师的youtube视频课程。</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型压缩方法总结</title>
      <link href="2020/02/08/model-yasuo/"/>
      <url>2020/02/08/model-yasuo/</url>
      
        <content type="html"><![CDATA[<h2 id="一方法总结">一、方法总结</h2><ul><li>Network Pruning</li><li>Knowledge Distillation</li><li>Parameter Quantization</li><li>Architecture Design</li><li>Dynamic Computation</li></ul><h2 id="二network-pruning">二、Network Pruning</h2><p>模型通常是过参数的，即很多参数或者neuron是冗余的(例如非常接近0),因此我们可以移除这些参数来对模型进行压缩。</p><h2 id="重要性判断">1. 重要性判断</h2><p>那么怎么判断哪些参数是冗余或者不重要的呢？</p><ul><li>对权重(weight)而言，我们可以通过计算它的<code>l1</code>,<code>l2</code>值来判断重要程度</li><li>对neuron而言，我们可以给出一定的数据集，然后查看在计算这些数据集的过程中neuron参数为0的次数，如果次数过多，则说明该neuron对数据的预测结果并没有起到什么作用，因此可以去除。</li></ul><h2 id="为什么要pruning">2. 为什么要pruning？</h2><p>那我们不禁要问，既然最后要得到一个小的network，那<strong>为什么不直接在数据集上训练小的模型，而是先训练大模型?</strong></p><ul><li>解释一</li></ul><p>一个比较普遍接受的解释是因为模型越大，越容易在数据集上找到一个局部最优解，而小模型比较难训练，有时甚至无法收敛。</p><ul><li>解释二</li></ul><p>2018年的一个发表在ICLR的<strong>大乐透假设</strong>(<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.03635" target="_blank" rel="noopener">Lottery Ticket Hypothesis</a>)观察到下面的现象：</p><p>首先看最左边的网络，它表示大模型，我们随机初始化它权重参数（红色）。然后我们训练这个大模型得到训练后的模型以及权重参数（紫色）。最后我们对训练好的大模型做pruning得到小模型。</p><h2 id="实际操作分析">3. 实际操作分析</h2><p>前面提到模型pruning可以从weight和neuron两个角度进行，下面就分别介绍实际可操作性：</p><ul><li>weight pruning</li></ul><p>如上图示，每个节点的输出和输出节点数都会变得不规则，这样一来有两个方面的问题： - 使用Pytorch，Keras实现起来不方便 - GPU是对矩阵运算做加速，现在都变得不规则，看起来似乎GPU面对这种情况也无能为力。2016年的一篇<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1608.03665" target="_blank" rel="noopener">文章</a>对AlexNet就做了这样的实验，实验结果是模型参数去掉了将近90%，最后的准确率只降低了2%左右，说明weight pruning的确能够在保证模型准确率的同时减少了模型大小，but！！！最后实验发现模型计算的速度似乎并没有提速，甚至对有的结构的修改使得速度降低了。</p><ul><li>neuron pruning</li></ul><p>如下面示意图所示，删减neuron之后网络结构能够保持一定的规则，实现起来方便，而且也能起到一定的加速作用</p><h2 id="三knowledge-distillation">三、Knowledge Distillation</h2><p>直接看下面的图应该很好理解。整个知识蒸馏过程中会用到两个模型：大模型（Teacher Net）和小模型（Student Net）。</p><p>具体方法是我们先用大模型在数据集上学习到收敛，并且这个大模型要学的还不错，因为后面我们要用大模型当老师来教小模型学习嘛，如果大模型本身都没学好还教个锤子，对吧？</p><p>我们以MNIST数据集为例，假设大模型训练好了，现在对于一张数字为“1”的图像，大模型的输出结果是由0.7的概率是1,0.2的概率是7,0.1的概率是9，这是不是有一定的道理？相比如传统的one-hot格式的label信息，这样的label包含更多的信息，所以Student Net要做的事情就是对于这张数字为“1”的图像，它的输出结果也要尽量接近Teacher Net的预测结果。</p><p>那Student Net到底如何学习呢？首先回顾一下在多类别分类任务中，我们用到的是softmax来计算最终的概率，即</p><p>但是这样有一个缺点，因为使用了指数函数，如果在使用softmax之前的预测值是x1=100,x2=10,x3=1,那么使用softmax之后三者对应的概率接近于y1=1,y2=0,y3=0，那这和常规的label无异了，所以为了解决这个问题就引入了一个新的参数T,称之为<strong>Temperature</strong>,即有:</p><p>此时，如果我们令T=100,那么最后的预测概率是y1=0.56,y2=0.23,y3=0.21。（不过李宏毅老师在视频里提到说这个方法在实际使用时貌似用处不大hhhh，感觉这个方法可以回答知乎上的 <strong>什么东西看起来很厉害但是没什么用?</strong> 哈哈哈哈哈哈哈哈哈哈或或）</p><h2 id="四parameter-quantization">四、Parameter Quantization</h2><h2 id="less-bits">1. less bits</h2><p>一个很直观的方法就是使用更少bit来存储数值，例如一般默认是32位，那我们可以用16或者8位来存数据。</p><h2 id="weight-clustering">2. weight clustering</h2><p>如下图所示，最左边表示网络中正常权重矩阵，之后我们对这个权重参数做聚类，比如最后得到了4个聚类，那么为了表示这4个聚类我们只需要2个bit，即用00,01,10,11来表示不同聚类。之后每个聚类的值就用均值来表示。这样的一个缺点就是误差可能会比较大。</p><h2 id="五architecture-design">五、Architecture Design</h2><h2 id="low-rank-approximation低秩近似">1. Low Rank Approximation(低秩近似)</h2><p>下图是低秩近似的简单示意图，左边是一个普通的全连接层，可以看到权重矩阵大小为 <img src="https://www.zhihu.com/equation?tex=M%5Ctimes+N" alt="[公式]"> ，而低秩近似的原理就是在两个全连接层之间再插入一层K。是不是很反直观？插入一层后，参数还能变少？</p><p>没错，的确变少了，我们可以看看新插入一层后的参数数量为: <img src="https://www.zhihu.com/equation?tex=N%5Ctimes+K%2BK%5Ctimes+M%3DK%5Ctimes+%28M%2BN%29" alt="[公式]"> ,因为 <img src="https://www.zhihu.com/equation?tex=K%3CM%2CK%3CN" alt="[公式]"> ,所以参数减少了。</p><p>但是低秩近似之所以叫<strong>低秩</strong>，是因为原来的矩阵的秩最大可能是min(M,N),而新增一层后可以看到矩阵U和V的秩都是小于等于K的，我们知道rank(AB)≤min(rank(A),rank(B)), 所以相乘之后的矩阵的秩一定还是小于等于K。那么这样会带来什么影响呢？那就是原先全连接层能表示更大的空间，而现在只能表示小一些的空间了。</p><h2 id="depthwise-separable-convolution">2. Depthwise Separable Convolution</h2><p>首先看一下标准卷积所需要的参数量。如下图示，输入数据由两个6<em>6的feature map组成，之后用4个大小为3</em>3的卷积核做卷积，最后的输出特征图大小为4<em>4</em>4。每个卷积核参数数量为2<em>3</em>3=18,所以总共用到的参数数量为4*18=72。</p><p>而Depthwise Separable卷积分成了两步，如下图示。</p><p>首先是输入数据的每个通道只由一个二维的卷积核负责，即卷积核通道数固定为1，而不是像上面那样，每个卷积核的通道数和输入通道数保持一致。这样最后得到的输出特征图的通道数等于输入通道数。</p><p>因为第一步得到的输出特征图是用不同卷积核计算得到的，所以不同通道之间是独立的，因此我们还需要对不同通道之间进行关联。为了实现关联，在第二步中使用了1<em>1大小的卷积核，通道数量等于输入数据的通道数量。另外1</em>1卷积核的数量等于预期输出特征图的通道数，在这里等于4。最后我们可以得到和标准卷积一样的效果，而且参数数量更少：3<em>3</em>2+(1<em>1</em>2)*4=26。</p><h2 id="六dynamic-computation">六、Dynamic Computation</h2><p>该方法的主要思路是如果目前的资源充足（比如你的手机电量充足），那么算法就尽量做到最好，比如训练更久，或者训练更多模型等；反之，如果当前资源不够（如电量只剩10%），那么就先算出一个过得去的结果。</p><p>那么如何实现呢？</p><h2 id="训练更多的model">1. 训练更多的model</h2><p>比如说我们提前训练多种网络，比如大网络，中等网络和小网络，那么我们就可以根据资源情况来选择不同的网络。但是这样的缺点是我们需要保存多个模型，这在移动设备上的可操作性不高。</p><h2 id="使用中间层输出结果">2. 使用中间层输出结果</h2><p>这样的思路其实也挺直观的，就是比如说我们做分类任务，当资源有限时，我们可能只是基于前面几层提取到的特征做分类预测，但是一般而言这样得到的结果会打折扣，因为前面提取到的特征是比较细腻度的，可能只是一些纹理，而不是比较高层次抽象的特征。</p><p>左下角的图表就展示了不同中间层的结果比较，可以看到越靠近输入，预测结果越差。</p><p>右下角的图则展示了在不同中间层插入分类器对于模型训练的影响，可以看到越靠近输入层插入分类器，对模型的影响越大。其实也很好理解，因为一般而言，前面的网络结构负责提取浅层的特征，但是当我们在前面就插入分类器后，那么分类器为了得到较好的预测结果会强迫前面的网络结构提取一些抽象的特征，进而扰乱了后面特征的提取。具体的分析可以阅读这篇文章<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.09844" target="_blank" rel="noopener">Multi-Scale Dense Networks for Resource Efficient Image Classification</a>。</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Few shot learning 少样本图像生成任务</title>
      <link href="2020/02/06/few-shot-image-generation-method/"/>
      <url>2020/02/06/few-shot-image-generation-method/</url>
      
        <content type="html"><![CDATA[<h4 id="background">Background：</h4><p>few-shot learning问题简单来说，就是通过减少学习的次数来获得有用的知识。我们以传统的image classification为例，对于传统的问题，我们经过大量的样本对模型进行训练从而增强它的泛化能力。也就是给machine看了很多张图片，让它区分开不同类别图片的区别。这和人学习很不一样，人只需要非常少的图片就可以区分图片里的是猫还是狗。few--shot learning就是给少量的训练样本，也能够具有比较好的泛化能力。如何达到呢？这类问题我们使用解决meta-learning的方式去处理。可以参见<a href="https://jasonhu02.github.io/2020/01/12/meta-learning/" target="_blank" rel="noopener">meta-learning tutorial</a>中给出的数学定义。</p><p>这里用通俗的话来讲，过去的机器学习只是教会了机器分类猫和狗-CNN结构的参数 <span class="math inline">\(\theta\)</span>，meta-learning教会了机器如何分类，这是一个更大框架的参数 <span class="math inline">\(\phi\)</span> 。这样相当于训练了多个 <span class="math inline">\(\theta\)</span> ，然后根据 <span class="math inline">\(\theta\)</span> 的训练情况，得到最好的 <span class="math inline">\(\phi\)</span> . 可以认为 <span class="math inline">\(\phi\)</span> 控制 <span class="math inline">\(\theta\)</span> , <span class="math inline">\(\phi=f(\theta)\)</span> ，这就是完整训练的过程。将得到的参数 <span class="math inline">\(\phi\)</span> 去测试集上测试，给几张原来没看过的image，丢到网络里面训练，网络生成的 <span class="math inline">\(\theta\)</span> 同时会受到 <span class="math inline">\(\phi\)</span> 的控制， 可以发现可以很快的区分原来没有学习过的image。 并且能够正确的识别出他们的label。</p><p>MAML和Reptiles都是解决上述问题比较好的方法。现在把meta-learning的问题用来解决生成问题。通常来说 生成image有三种方法：pixelCNN，VAE和GAN。VAE曾经在以前的文章中介绍过，实质上是衡量的是真实样本和生成样本的KL-divergence。VAE仍然有很多问题需要解决 ，而GAN有更好的生成特性。</p><h4 id="method">Method：</h4><p>本文使用latent vector作为input，使用reptile的网络架构，将GAN的generator来生成逼真的图像。GAN由generator和discriminator组成。基本思想通俗来说，用Reptiles的方式来训练Generator和Discriminator，得到的 <span class="math inline">\(\phi_{\mathcal{d}}\)</span> 和 <span class="math inline">\(\phi_{\mathcal{g}}\)</span>, <span class="math inline">\(W_{d}\)</span> 和<span class="math inline">\(W_{g}\)</span>，使用Wasserstein GP loss进行训练，先进行内层循环K次，得到最佳的 <span class="math inline">\(W_{d}\)</span> 和 <span class="math inline">\(W_{g}\)</span> 。通过<span class="math inline">\(\phi_{\mathcal{d}}-W_{d}\)</span> 和 <span class="math inline">\(\phi_{\mathcal{g}}-W_{g}\)</span> 来更新参数。其loss function为 <span class="math display">\[\text {minimize} \sum_{T}\left(\Phi_{d}-W_{d \tau}\right)+\left(\Phi_{g}-W_{g \tau}\right)\]</span> 训练的流程如下：</p><p><img src="few shot image generation method/image-20200223142531081.png" alt="image-20200223142531081" style="zoom:33%;"></p><h4 id="experiment">Experiment:</h4><p>构造一个ResNetGenerator和ResNetDiscriminator。再分别构造inner_loop和meta_training_loop</p><p>核心代码如下：</p><p>inner_loop:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inner_loop</span><span class="params">(self, real_batch)</span>:</span></span><br><span class="line">       self.meta_g.train()</span><br><span class="line">       fake_batch = self.meta_g(torch.tensor(np.random.normal(size=(self.batch_size, self.z_shape)), dtype=torch.float, device=device))</span><br><span class="line">       training_batch = torch.cat([real_batch, fake_batch])</span><br><span class="line"></span><br><span class="line">       <span class="comment"># Training discriminator</span></span><br><span class="line">       gradient_penalty = calc_gradient_penalty(self.meta_d, real_batch, fake_batch)</span><br><span class="line">       discriminator_pred = self.meta_d(training_batch)</span><br><span class="line">       discriminator_loss = wassertein_loss(discriminator_pred, self.discriminator_targets)</span><br><span class="line">       discriminator_loss += gradient_penalty</span><br><span class="line"></span><br><span class="line">       self.meta_d_optim.zero_grad()</span><br><span class="line">       discriminator_loss.backward()</span><br><span class="line">       self.meta_d_optim.step()</span><br><span class="line"></span><br><span class="line">       <span class="comment"># Training generator</span></span><br><span class="line">       output = self.meta_d(self.meta_g(torch.tensor(np.random.normal(size=(self.batch_size, self.z_shape)),dtype=torch.float, device=device)))</span><br><span class="line">       generator_loss = wassertein_loss(output, self.generator_targets)</span><br><span class="line"></span><br><span class="line">       self.meta_g_optim.zero_grad()</span><br><span class="line">       generator_loss.backward()</span><br><span class="line">       self.meta_g_optim.step()</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> discriminator_loss.item(), generator_loss.item()</span><br></pre></td></tr></table></figure><p>meta_training_loop:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">meta_training_loop</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">    data, task = self.env.sample_training_task(self.batch_size)</span><br><span class="line">    data = normalize_data(data)</span><br><span class="line">    real_batch = data.to(device)</span><br><span class="line"></span><br><span class="line">    discriminator_total_loss = <span class="number">0</span></span><br><span class="line">    generator_total_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.inner_epochs):</span><br><span class="line">        disc_loss, gen_loss = self.inner_loop(real_batch)</span><br><span class="line">        discriminator_total_loss += disc_loss</span><br><span class="line">        generator_total_loss += gen_loss</span><br><span class="line"></span><br><span class="line">    self.writer.add_scalar(<span class="string">'Training_discriminator_loss'</span>, discriminator_total_loss, self.eps)</span><br><span class="line">    self.writer.add_scalar(<span class="string">'Training_generator_loss'</span>, generator_total_loss, self.eps)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'epochs:&#123;&#125;,Train_D_loss:&#123;&#125;,Train_G_loss:&#123;&#125;'</span>.format(self.eps,discriminator_total_loss,generator_total_loss))</span><br><span class="line"></span><br><span class="line">     <span class="comment"># Updating both generator and dicriminator</span></span><br><span class="line">    <span class="keyword">for</span> p, meta_p <span class="keyword">in</span> zip(self.g.parameters(),self.meta_g.parameters()):</span><br><span class="line">        diff = p - meta_p.cpu()</span><br><span class="line">        p.grad = diff</span><br><span class="line"></span><br><span class="line">    self.g_optim.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p, meta_p <span class="keyword">in</span> zip(self.d.parameters(), self.meta_d.parameters()):</span><br><span class="line">        diff = p - meta_p.cpu()</span><br><span class="line">        p.grad = diff</span><br><span class="line">    self.d_optim.step()</span><br></pre></td></tr></table></figure><p>可以看到meta_training_loop调用了inner_loop，计算了p - meta_p和p - meta_p的loss，并进行了更新。</p><p>真实的training过程</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">training</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">while</span> self.eps &lt;= <span class="number">10000</span>:</span><br><span class="line">            self.reset_meta_model()</span><br><span class="line">            self.meta_training_loop()</span><br><span class="line">            self.eps += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Validation run every 10000 training loop</span></span><br><span class="line">            <span class="keyword">if</span> self.eps % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                self.reset_meta_model()</span><br><span class="line">                gan_score=self.validation_run()</span><br><span class="line">                self.gan_score_total.append(gan_score)</span><br><span class="line">                print(<span class="string">"valscore_total:&#123;&#125;"</span>.format(self.gan_score_total))</span><br><span class="line">                self.checkpoint_model()</span><br><span class="line"><span class="comment">#            self.eps += 1</span></span><br></pre></td></tr></table></figure><h4 id="result">Result：</h4><p>最后生成的效果非常好。Mnist上的测试结果：</p><p><img src="few shot image generation method/image-20200223144618093.png" alt="image-20200223144618093" style="zoom:50%;"></p><p>Omniglot上的测试结果：</p><p><img src="few shot image generation method/image-20200223144638634.png" alt="image-20200223144638634" style="zoom:50%;"></p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> meta-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VAE-GAN的一些理解</title>
      <link href="2020/02/02/vae-gan/"/>
      <url>2020/02/02/vae-gan/</url>
      
        <content type="html"><![CDATA[<p>CVAE-GAN这篇论文有一些不错的insight可以拿过来使用。</p><p>VAE中的encoder是一个神经网路结构用来生成数据样本X中的方差和均值。根据方差和均值得到一个正态分布，从这个分布中采样一个z，该正态分布与标准正态分布之间有一个转换关系，$z=uz<em>+sigma <span class="math inline">\(, 通过decoder这个神经网络生成新的\)</span>x</em><span class="math inline">\(。这个\)</span>x*$可以用来表示新的图像。</p><p>衡量decoder和encoder之间的metric是通过用KL-divergence来表示Loss function，具体的推导过程如下，</p><p><img src="VAE_GAN/image-20200218140108115.png" alt="image-20200218140108115" style="zoom:33%;"></p><p><img src="VAE_GAN/image-20200218140121734.png" alt="image-20200218140121734" style="zoom:33%;"></p><p>理解VAE的本质：</p><p><strong>它本质上就是在我们常规的自编码器的基础上，对 encoder 的结果（在VAE中对应着计算均值的网络）加上了“高斯噪声”，使得结果 decoder 能够对噪声有鲁棒性；而那个额外的 KL loss（目的是让均值为 0，方差为 1），事实上就是相当于对 encoder 的一个正则项，希望 encoder 出来的东西均有零均值。</strong></p><p>​ 那另外一个 encoder（对应着计算方差的网络）的作用呢？它是用来<strong>动态调节噪声的强度</strong>的。直觉上来想，<strong>当 decoder 还没有训练好时（重构误差远大于 KL loss），就会适当降低噪声（KL loss 增加），使得拟合起来容易一些（重构误差开始下降）</strong>。反之，<strong>如果 decoder 训练得还不错时（重构误差小于 KL loss），这时候噪声就会增加（KL loss 减少），使得拟合更加困难了（重构误差又开始增加），这时候 decoder 就要想办法提高它的生成能力了</strong>。</p><p>在GAN中，其中的discriminator本质上也是一个metric的判别器，表示生成样本和真实样本之间的JS-divergence。但是GAN存在的问题是</p><p>对于cVAE来说，就是存在一个有监督的标签，通过这个标签，只需要将KL-divergence进行一下变换即可：</p><p><img src="VAE_GAN/image-20200218140135714.png" alt="image-20200218140135714" style="zoom:33%;"></p><hr><p>正式说VAE和GAN的结合：</p><p>论文链接： https://arxiv.org/pdf/1703.10155.pdf</p><p><img src="VAE_GAN/image-20200218140159343.png" alt="image-20200218140159343" style="zoom:50%;"></p><p>cvae-gan的具体实施架构</p><p><img src="VAE_GAN/image-20200218140217148.png" alt="image-20200218140217148" style="zoom: 50%;"></p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> representation-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VAE的一些理解</title>
      <link href="2020/02/02/shen-ru-li-jie-vae/"/>
      <url>2020/02/02/shen-ru-li-jie-vae/</url>
      
        <content type="html"><![CDATA[<p>仔细去看了ICML2014的文章Auto-Encoding Variational Bayes，想重新把VAE在理论上推导一遍。</p><p>先了解auto-encoder的组成。 q(z) 是标准正态分布, p(z|x), q(x|z) 是条件正态分布，分别对应编码器、解码器。</p><p>这里通过直接对联合分布进行近似的方式，简明快捷地给出了 VAE 的理论框架。 出发点依然没变，这里再重述一下。首先我们有一批数据样本 {<em>x</em>1,…,<em>x</em>n}，其整体用 <em>x</em> 来描述，我们希望<strong>借助隐变量</strong> <strong><em>z</em></strong> <strong>描述</strong> <strong><em>x</em></strong> <strong>的分布</strong> <span class="math inline">\(p(x)\)</span>：<br><span class="math display">\[p(x)=\int p(x | z) p(z) d z, \quad p(x, z)=p(x | z) p(z)\]</span></p><p><span class="math display">\[KL(p(x, z) \| q(x, z))=\iint p(x, z) \ln \frac{p(x, z)}{q(x, z)} d z d x\]</span></p><p>这样（理论上）我们既描述了 <em>p</em>(<em>x</em>)，又得到了生成模型 <em>p</em>(<em>x</em>|<em>z</em>)，一举两得。</p><p>KL 散度是我们的终极目标，因为我们希望两个分布越接近越好，所以 KL 散度越小越好。由于我们手头上只有 <em>x</em> 的样本，因此利用 <em>p</em>(<em>x</em>,<em>z</em>)=<em>p</em>(<em>x</em>)<em>p</em>(<em>z</em>|<em>x</em>) 对上式进行改写： <span class="math display">\[\begin{aligned} K L(p(x, z) \| q(x, z)) &amp;=\int p(x)\left[\int p(z | x) \ln \frac{p(x, z)}{q(x, z)} d z\right] d x \\ &amp;=\mathbb{E}_{x \sim p(x)}\left[\int p(z | x) \ln \frac{p(z | x) p(x)}{q(x, z)} d z\right] \end{aligned}\]</span> 这样一来利用 (4) 式，把各个 <em>xi</em> 代入就可以进行计算了，这个式子还可以进一步简化，因为： <span class="math display">\[\ln \frac{p(z | x) p(x)}{q(x, z)}=\ln \frac{p(z | x)}{q(x, z)}+\ln p(x)\]</span> 而 <span class="math display">\[\begin{aligned} \mathbb{E}_{x \sim p(x)}\left[\int p(z | x) \ln p(x) d z\right] &amp;=\mathbb{E}_{x \sim p(x)}\left[\ln p(x) \int p(z | x) d z\right] \\ &amp;=\mathbb{E}_{x \sim p(x)}[\ln p(x)] \end{aligned}\]</span> 注意这里的 <em>p</em>(<em>x</em>) 是根据样本 <em>x</em>1,<em>x</em>2,…,<em>x</em>n 确定的关于 <em>x</em> 的先验分布（更常见的写法是 <em>p̃</em>(<em>x</em>)），尽管我们不一定能准确写出它的形式，但它是确定的、存在的，因此这一项只是一个常数，所以可以写出： <span class="math display">\[\mathcal{L}=K L(p(x, z) \| q(x, z))-constant=\mathbb{E}_{x \sim p(x)}\left[\int p(z | x) \ln \frac{p(z | x)}{q(x, z)} d z\right]\]</span> 目前最小化 KL(<em>p</em>(<em>x</em>,<em>z</em>)‖<em>q</em>(<em>x</em>,<em>z</em>)) 也就等价于最小化 L。注意减去的常数一般是负数（概率小于 1，取对数就小于 0），而 KL 散度本来就非负，非负数减去一个负数，结果会是一个正数，所以 L 恒大于一个某个正数。</p><p>到这里，我们回顾初衷——为了得到生成模型，所以我们把 <em>q</em>(<em>x</em>,<em>z</em>) 写成 <em>q</em>(<em>x</em>|<em>z</em>)<em>q</em>(<em>z</em>)，于是就有： <span class="math display">\[\begin{aligned} \mathcal{L} &amp;=\mathbb{E}_{x \sim p(x)}\left[\int p(z | x) \ln \frac{p(z | x)}{q(x | z) q(z)} d z\right] \\ &amp;=\mathbb{E}_{x \sim p(x)}\left[-\int p(z | x) \ln q(x | z) d z+\int p(z | x) \ln \frac{p(z | x)}{q(z)} d z\right] \end{aligned}\]</span> 再简明一点，那就是： <span class="math display">\[\begin{aligned} \mathcal{L} &amp;=\mathbb{E}_{x \sim p(x)}\left[\mathbb{E}_{z \sim p(z | x)}[-\ln q(x | z)]+\mathbb{E}_{z \sim p(z | x)}\left[\ln \frac{p(z | x)}{q(z)}\right]\right] \\ &amp;=\mathbb{E}_{x \sim p(x)}\left[\mathbb{E}_{z \sim p(z | x)}[-\ln q(x | z)]+K L(p(z | x) \| q(z))\right] \end{aligned}\]</span> 看，括号内的不就是 VAE 的损失函数吗？只不过我们换了个符号而已。我们就是要想办法找到适当的 <em>q</em>(<em>x</em>|<em>z</em>) 和 <em>q</em>(<em>z</em>) 使得 L 最小化。</p><p>简单来说，由于直接描述复杂分布是难以做到的，<strong>所以我们通过引入隐变量来将它变成条件分布的叠加</strong>。而这时候我们对隐变量的分布和条件分布都可以做适当的简化（比如都假设为正态分布），并且在条件分布的参数可以跟深度学习模型结合起来（用深度学习来算隐变量的参数），至此，“深度概率图模型”就可见一斑了。</p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> representation-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ICLR2020论文解读__Learning robust representations via multi-view information</title>
      <link href="2020/01/22/information-bottleneck-paper/"/>
      <url>2020/01/22/information-bottleneck-paper/</url>
      
        <content type="html"><![CDATA[<p>本文是对于信息瓶颈方法的原始公式应用于在学习时可以使用任务特定标签的监督设置中。提出了通过利用multi-view的方式 提供一种实体的两类视图，可以将方法扩展到无监督的设置。本文通过理论分析得到了多视图的定义。还通过利用标准数据增强技术将理论扩展到单视图，比传统的无监督学习的方法相比，有更好的泛化能力。并在数据集Sketchy和MIR-flickr上进行了实验。</p><figure><img src="image/image-20200217022428246.png" alt="image-20200217022428246"><figcaption aria-hidden="true">image-20200217022428246</figcaption></figure><p>表示了x和z之间的互信息分成三部份，<strong>第一部分表示为x和z之间的互信息在没有预测为y的情况下</strong>；第二部分<strong>x，y之间的互信息，是一个常数，该常数由原始观测值标签的信息来决定</strong>；减去<strong>第三部分，表示x编码为z丢失的和y有关的互信息</strong>。第二部分和第三部分的区别就是考虑了z情况下的x，y互信息，包含于没有考虑x，y情况下的，所以减去。</p><h4 id="理论">理论:</h4><p>其中涉及到了几个定理和推论，文中均给出了详尽的证明，涉及到大量的信息论和概率的知识，必须借助维基百科()加上自己的手推才能完全理解本文的思路。在此直接列出所有结论。</p><p>定义1，充分性：</p><p><img src="image/image-20200217143840828.png" alt="image-20200217143840828" style="zoom: 50%;"></p><p>z对y充分定义为，I(x;y|z)在z条件下的y对x的互信息为0. 由该定义可以推出如下结论: <span class="math display">\[I(\mathbf{x} ; \mathbf{y} | \mathbf{z})=0 \Longleftrightarrow I(\mathbf{x} ; \mathbf{y})=I(\mathbf{y} ; \mathbf{z})\]</span> 命题2.1：</p><figure><img src="image/image-20200217150059055.png" alt="image-20200217150059055"><figcaption aria-hidden="true">image-20200217150059055</figcaption></figure><p>定义2，冗余性</p><figure><img src="image/image-20200217150236253.png" alt="image-20200217150236253"><figcaption aria-hidden="true">image-20200217150236253</figcaption></figure><p>推论1：</p><figure><img src="image/image-20200217150331149.png" alt="image-20200217150331149"><figcaption aria-hidden="true">image-20200217150331149</figcaption></figure><p>附录部分给出了非常详尽的证明，对任何理解上不到位的地方都可以去看数学推导。</p><h4 id="related-work">Related Work：</h4><p><img src="image/image-20200217142805245.png" alt="image-20200217142805245" style="zoom: 50%;"></p><p>为了比较和其他模型的区别，文中这个图对作者使用的情况进行了详细的解释说明。infomax最大化互信息，来实现无监督学习。理想情况下，良好的表示形式将最大程度地提供有关标签的信息，同时保留来自观察结果的最少信息。也就是图中平行四边形左上方的顶点。从图中可以看到MIB模型是最接近最优解的，本文是第一篇明确指出在多视角无监督学习中丢弃冗余信息的一篇文章。</p><h4 id="实施方法">实施方法:</h4><p>论文的核心思想在这个图上：</p><p><img src="image/image-20200217024601501.png" alt="image-20200217024601501" style="zoom: 33%;"></p><p>在v1和v2两个视图上，分别得到编码得到z1和z2，通过比较两者的分布之间的平均KL散度，以及z1和z2之间的互信息来更新loss。</p><p>它的loss为全文核心：</p><figure><img src="image/image-20200217024749741.png" alt="image-20200217024749741"><figcaption aria-hidden="true">image-20200217024749741</figcaption></figure><p>散度减去互信息，其表达了用冗余信息减去预测y充分性下的z1和z2的互信息，我们使得在z1|v1和z2|v2下的的KL散度最大化，即v1，v2呈现不同的视角使其给的信息更加无关，而最大化z1和z2之间的互信息，使得z1和z2的信息更加相关。这样的目的都是消除两个变量之间的相关性，也就是信息瓶颈的意思，让最有用的信息通过去，留下对预测没用的多余信息。本文的意图就是想方设法的使得两个不同分布的数据集关联度尽可能小，简单来讲就是让互信息尽可能小。</p><p>最后作者将MIB方法在Sketchy和Flickr数据集上与先前的多视图算法做比较。Sketchy数据集包含来自125个类别的12,500张图像和75,471张手绘草图，是两种信息量上差别很大的图。MIR-Flicker则是通过图像和文字结合，提供两种视角。最后的效果如图所示：分别在Sketchy和Flickr上的效果如下：</p><p>可以看到mv-infomax的实力也非常不错，所以文章主要就是和它在做对比。</p><p><img src="image/image-20200217030932182.png" alt="image-20200217030932182" style="zoom:33%;"></p><p><img src="image/image-20200217025835301.png" alt="image-20200217025835301" style="zoom:33%;"></p><p>我特地去view了代码，发现代码实现的方法非常简单，说明该方法从某一些理论性的角度解决了模型鲁棒性的问题，训练起来速度很快，有一定的参考价值。且论文作者丝毫不避讳地把实验中所有数据全部公开在论文附录里，看来是对论文地实验效果非常有信心，有足够地把握给读者看。总之，可以借鉴地点非常多。之后工作可以围绕他的思路做一些扩展了。</p><p>核心代码在这里：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(range(epochs)):</span><br><span class="line">    <span class="keyword">for</span> v_1, v_2, _ <span class="keyword">in</span> train_loader:</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> cuda:</span><br><span class="line">            v_1 = v_1.cuda()</span><br><span class="line">            v_2 = v_2.cuda()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Encode a batch of data</span></span><br><span class="line">        p_z_1_given_v_1 = encoder_v_1(v_1)</span><br><span class="line">        p_z_2_given_v_2 = encoder_v_2(v_2)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Sample from the posteriors with reparametrization</span></span><br><span class="line">        z_1 = p_z_1_given_v_1.rsample()</span><br><span class="line">        z_2 = p_z_2_given_v_2.rsample()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Mutual information estimation</span></span><br><span class="line">        mi_gradient, mi_estimation = mi_estimator(z_1,z_2)</span><br><span class="line">        mi_gradient = mi_gradient.mean()</span><br><span class="line">        mi_estimation = mi_estimation.mean()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Symmetrized Kullback-Leibler divergence</span></span><br><span class="line">        kl_1_2 = p_z_1_given_v_1.log_prob(z_1) - p_z_2_given_v_2.log_prob(z_1)</span><br><span class="line">        kl_2_1 = p_z_2_given_v_2.log_prob(z_2) - p_z_1_given_v_1.log_prob(z_2)</span><br><span class="line">        skl = (kl_1_2 + kl_2_1).mean()/ <span class="number">2.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update the value of beta according to the policy</span></span><br><span class="line">        beta = beta_scheduler(iterations)</span><br><span class="line">        iterations +=<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Computing the loss function</span></span><br><span class="line">        loss = - mi_gradient + beta * skl</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Logging</span></span><br><span class="line">        mi_over_time.append(mi_estimation.item())</span><br><span class="line">        skl_over_time.append(skl.item())</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Backward pass and update</span></span><br><span class="line">        opt.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Plot the loss components every 5 epochs</span></span><br><span class="line">    <span class="keyword">if</span> epoch % plot_every == <span class="number">0</span>:</span><br><span class="line">        f, ax = plt.subplots(<span class="number">1</span>,<span class="number">2</span>, figsize=(<span class="number">8</span>,<span class="number">3</span>))</span><br><span class="line">        ax[<span class="number">0</span>].set_title(<span class="string">'$I(z_1;z_2)$'</span>)</span><br><span class="line">        ax[<span class="number">1</span>].set_title(<span class="string">'$D_&#123;SKL&#125;(p(z_1|v_1)||p(z_2|v_2))$'</span>)</span><br><span class="line">        ax[<span class="number">1</span>].set_yscale(<span class="string">'log'</span>)</span><br><span class="line">        ax[<span class="number">0</span>].plot(mi_over_time, <span class="string">'.'</span>, alpha=<span class="number">0.1</span>)</span><br><span class="line">        ax[<span class="number">1</span>].plot(skl_over_time, <span class="string">'.r'</span>, alpha=<span class="number">0.1</span>)</span><br><span class="line">        ax[<span class="number">0</span>].set_ylim(<span class="number">0</span>,<span class="number">8</span>)</span><br><span class="line">        ax[<span class="number">1</span>].set_ylim(<span class="number">1e-3</span>)</span><br><span class="line">        </span><br><span class="line">        f.suptitle(<span class="string">'Epoch: %d'</span>%epoch, fontsize=<span class="number">15</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute train and test_accuracy of a logistic regression</span></span><br><span class="line">        train_accuracy, test_accuracy = evaluate(encoder=encoder_v_1, train_on=train_subset, test_on=test_set, cuda=cuda)</span><br><span class="line">        print(<span class="string">'Train Accuracy: %f'</span>% train_accuracy)</span><br><span class="line">        print(<span class="string">'Test Accuracy: %f'</span>% test_accuracy)</span><br></pre></td></tr></table></figure><h4 id="点评">点评：</h4><p>这是一篇很好的多视角表示学习论文，具有新的见解。learn variable z_1和z_2，它们是一致的，包含视角不变信息，但应尽可能丢弃特定于视角的信息。 本文依赖于相互信息估计，并且无需重构。在先前的一些工作中（例如Aaron van den Oord等人2018）中提到，重建损失会引入偏见，对学习的表征产生负面影响。与现有的尝试最大化学习的表示和视图之间的相互信息的多视图表示学习方法相比，本文明确定义了多余的信息，我们应该尝试抛弃这些多余的信息，并弄清楚如何获得足够的学习的表示用于输出。作者还得出了一些现有的（多视图）表示学习方法与他们提出的方法之间的明确联系。</p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> representation-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch-多GPU操作的方法</title>
      <link href="2020/01/19/pytorch-parallel/"/>
      <url>2020/01/19/pytorch-parallel/</url>
      
        <content type="html"><![CDATA[<h5 id="完整的步骤">完整的步骤：</h5><ol type="1"><li>设定好<code>os.environment(" CUDA_VISIBEL_DEVICES ")='0,1,2,3,4,5'</code></li><li>pytorch中定义一个变量<code>device = torch.device("cuda:0,1,2,3,4,5"）</code></li><li>把model使用<code>model = nn.DataParallel(model)</code>，用来分配数据，并且<code>model.to(device)</code>，将model加载到gpu上</li><li>将要训练的数据放入到device中，然后训练这个model就可以了！</li></ol><h5 id="实际操作">实际操作：</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os </span><br><span class="line">os.environment(<span class="string">"CUDA_VISIBEL_DEVICES"</span>)=<span class="string">'0,1,2,3,4,5'</span></span><br></pre></td></tr></table></figure><p>上述步骤相当于在shell 命令里面调用了 CUDA_VISIBEL_DEVICES 命令, 相当于打开了6个gpu的入口。分别指定了cuda0,cuda1,cuda2三个gpu进行计算。这里相当于调用了6个gpu</p><p>note：使用torch.cuda.device_count()看到的就是这里调用了几个gpu。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device=torch.device(<span class="string">"cuda:0,1,2,3,4,5"</span>, <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure><p>这里必须和上面的数值保持一致，否则会报错。device的输出是 type: "cuda" ，index=0， 用index的值表示了cuda的个数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Model(input_size, output_size)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">  print(<span class="string">"Let's use"</span>, torch.cuda.device_count(), <span class="string">"GPUs!"</span>)</span><br><span class="line">  <span class="comment"># dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span></span><br><span class="line">  model = nn.DataParallel(model)</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure><p>使用nn.DataParallel对model这个网络模型进行data上的自动分配</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input = data.to(device)</span><br><span class="line">    output = model(input)</span><br><span class="line">    print(<span class="string">"Outside: input size"</span>, input.size(),</span><br><span class="line">          <span class="string">"output_size"</span>, output.size())</span><br></pre></td></tr></table></figure><p>将input从dataset中得到batch_size的数据，将data传给GPU中，对模型进行了训练(forward function这里省去)</p><p>这个时候cuda上就会被分配上数据。可以看到最后输出的结果：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Let&apos;s use 2 GPUs!</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class="line">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Let&apos;s use 8 GPUs!</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure><p>可以看到model上的data被平均分配到了8个gpu上。</p><p>input_size=2, output_size=5, batch_size=30, data_size=100。</p><p>由于每次batch_size=30, 因此跑了4次，数据量为30+30+30+10</p><p>每次在模型中，30个input被分配到8个gpu上，4+4+4+4+4+2+4+4个，每个cuda上跑了4或2个input，output则相应输出对应的大小。[4,5]表示4个input，每个input的大小为5。</p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch-data处理的方法</title>
      <link href="2020/01/18/pytorch-data/"/>
      <url>2020/01/18/pytorch-data/</url>
      
        <content type="html"><![CDATA[<p>pytorch对数据处理的一些基本用法总结：</p><p>为了更好地分配数据，通常在训练中会用到pytorch的几个库，<code>torchvision.transforms</code> 和<code>torchvision.dataset.Imagefolder</code> 和 <code>torch.util.data.Dataloader</code></p><p>用代码解释这三个函数的作用：</p><h4 id="transforms">transforms：</h4><p>transforms的作用一句话概括就是使得数据集里的数据统一化，比如对于图像数据，可能很多图像的尺寸不一样，需要对图像的大小进行裁剪和缩放，并对图像的大小尺寸进行统一。</p><p>tutorial中的几个参数：</p><ul><li><p><code>Rescale</code>: to scale the image</p></li><li><p><code>RandomCrop</code>: to crop from image randomly. This is data augmentation.</p></li><li><p><code>ToTensor</code>: to convert the numpy images to torch images (we need to swap axes).</p><p>note: numpy中的图像数据是 H × W × C ; 而torch.tensor的数据是C × H × W</p></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rescale</span><span class="params">(obejct)</span>： </span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">__init__</span><span class="params">(self, output_size)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(output_size, (int, tuple))</span><br><span class="line">        self.output_size = output_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"></span><br><span class="line">        h, w = image.shape[:<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> isinstance(self.output_size, int):</span><br><span class="line">            <span class="keyword">if</span> h &gt; w:</span><br><span class="line">                new_h, new_w = self.output_size * h / w, self.output_size</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_h, new_w = self.output_size, self.output_size * w / h</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_h, new_w = self.output_size</span><br><span class="line"></span><br><span class="line">        new_h, new_w = int(new_h), int(new_w)</span><br><span class="line"></span><br><span class="line">        img = transform.resize(image, (new_h, new_w))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># h and w are swapped for landmarks because for images,</span></span><br><span class="line">        <span class="comment"># x and y axes are axis 1 and 0 respectively</span></span><br><span class="line">        landmarks = landmarks * [new_w / w, new_h / h]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: img, <span class="string">'landmarks'</span>: landmarks&#125;</span><br></pre></td></tr></table></figure><p><code>_call_</code>函数的作用是可以直接调用rescale这个类，不需要每次调用时都需要传递参数。 We will write them as callable classes instead of simple functions so that parameters of the transform need not be passed everytime it’s called.</p><h5 id="transformer的用法">transformer的用法：</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transformed_dataset = FaceLandmarksDataset(csv_file=<span class="string">'data/faces/face_landmarks.csv'</span>,</span><br><span class="line">                                           root_dir=<span class="string">'data/faces/'</span>,</span><br><span class="line">                                           transform=transforms.Compose([</span><br><span class="line">                                               Rescale(<span class="number">256</span>),</span><br><span class="line">                                               RandomCrop(<span class="number">224</span>),</span><br><span class="line">                                               ToTensor()</span><br><span class="line">                                           ]))</span><br></pre></td></tr></table></figure><p>FaceLandmarksDataset是之前定义的一个类，transformed_dataset是它的实例化，等于对目录下的data经过了处理，保存到了transformed_dataset这个实例化的类中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(transformed_dataset)):</span><br><span class="line">    sample = transformed_dataset[i]</span><br><span class="line"></span><br><span class="line">    print(i, sample[<span class="string">'image'</span>].size(), sample[<span class="string">'landmarks'</span>].size())</span><br></pre></td></tr></table></figure><p>然后通过for 循环，用sample一个一个取出这个类中的数据。</p><p>输出如下:</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">0 torch.Size([3, 224, 224]) torch.Size([68, 2])</span><br><span class="line">1 torch.Size([3, 224, 224]) torch.Size([68, 2])</span><br><span class="line">2 torch.Size([3, 224, 224]) torch.Size([68, 2])</span><br><span class="line">3 torch.Size([3, 224, 224]) torch.Size([68, 2])</span><br></pre></td></tr></table></figure><h4 id="dataloader">dataloader:</h4><p>由此，这不是深度学习中常用的方式，</p><p>1.我们对数据集通常会进行打乱，不是顺序读取的。shffle data</p><p>2.我们会选择一个batch作为数据，而不是每次都train一个图像。batch data</p><p>3.用上述的代码不适合在多线程运行。muilt-processing</p><p>此题背景是有69张人脸图像+其面部轮廓的框图。我们使用上面的方法，那么就是先标准化处理每张图像，并且将框图和人脸融合，作为一个深度学习标准的数据集，这些是transform做的事情。接下来载入数据，我们想通过数据集<strong>载入</strong>到图像数据中</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dataloader=torch.util.data.Dataloader&#123;</span><br><span class="line">transforms_dataset,batch_size=4,shuffle=true, num_workers=4</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过这样的方式，就解决了上述的三个问题。num_workers是读取数据的线程数目。</p><p>note: 补充一下这个函数的实现方式，有一个collate_fn这个函数，是核心，<strong>如果后面想用到其他数据里面</strong>，重写collate_fn函数，在dataloader=torch.util.data.Dataloader{collate_fn=collate_fn}来得到自己想要的加载数据的结果：</p><blockquote><p>collate_fn这个函数的输入就是一个list，list的长度是一个batch size，list中的每个元素都是__getitem__得到的结果。</p></blockquote><h4 id="imagefolder">ImageFolder</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"></span><br><span class="line">data_transform = transforms.Compose([</span><br><span class="line">        transforms.RandomSizedCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                             std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ])</span><br><span class="line">hymenoptera_dataset = datasets.ImageFolder(root=<span class="string">'hymenoptera_data/train'</span>,</span><br><span class="line">                                           transform=data_transform)</span><br><span class="line">dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset,</span><br><span class="line">                                             batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                             num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>从torchvision中导入这两个函数，先写transforms定义了随机裁图片，变成tensor，进行标准化处理。</p><p>使用ImageFolder读取在train下面的图片，其输出如下: {'cat': 0, 'dog': 1} 这样的形式，将label和input做了分离 。</p><p>最后使用dataloader导入数据。</p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transfer-learning tutorial</title>
      <link href="2020/01/16/transfer-learning/"/>
      <url>2020/01/16/transfer-learning/</url>
      
        <content type="html"><![CDATA[<h4 id="迁移学习概述">迁移学习概述：</h4><p>最近开始学习迁移学习，主要还是由于在想从迁移学习中再获得一些启发来指导研究少样本学习的一些概念。</p><p>迁移学习主要解决的问题是通过预训练好的模型，用于其他任务的一类问题。总之，在我看来，迁移学习实际上是解决一类问题的。通俗来说就是使用从解决一个问题中获得的知识来解决一个不同的但是相关的问题。</p><p>此外，迁移学习实际上做的是一类问题，可见最后给出的知识框图，给出了迁移学习领域所有问题的一个说明。</p><h4 id="数学符号">数学符号：</h4><p>先了解下迁移学习定义所使用的符号： <span class="math inline">\(\mathcal{D}\)</span> 领域(domain)，包含两部分：参数空间 $  $ 和边缘概率分布 <span class="math inline">\(P(X)\)</span> 组成，其中 <span class="math inline">\(X=\left\{x_{1}, \ldots, x_{n}\right\} \in \mathcal{X}\)</span> , <span class="math inline">\(X\)</span>表示领域中的所有数据，也就是表示在 <span class="math inline">\(X\)</span> 中的第 <span class="math inline">\(x_{i}\)</span> 的样本数据都属于参数空间中。</p><p>通常来说两个不同的domain，他们有不同的特征空间和不同的边缘概率分布。给定一个特定的domain，<span class="math inline">\(\mathcal{D}=\{\mathcal{X}, P(X)\}\)</span> ，一个任务 <span class="math inline">\(\mathcal{T}\)</span> 可以分为两个部分，标签空间 <span class="math inline">\(\mathcal{Y}\)</span> 和目标函数 <span class="math inline">\(f(.)\)</span> 。目标函数 <span class="math inline">\(f(.)\)</span> 对于x的预测，我们将 <span class="math inline">\(f(.)\)</span> 改写成 <span class="math inline">\(f(x)\)</span>。 在迁移学习中，对数据集我们将其分为source dataset $D_{s}= {({x_{S_{1}}},{y_{S_{1}}}),({x_{S_{2}}},{y_{s_{2}}}),({X_{s_{3}}},{Y_{s_{3}}})} $ 和target dataset <span class="math inline">\(D_{t}=\left\{\left\{x_{T_{1}},y_{T_{1}}\right\},\left\{x_{T_{2}},y_{T_{2}}\right\},\left\{x_{T_{n}},y_{T_{n}}\right\}\right\}\)</span> 。表示在source dataset中，每一个在S<sub>1</sub> 中的x对应着一个label-y in S<sub>1</sub>。 而target 同样如此。通常情况下，<span class="math inline">\(n_{T}&lt;&lt;n_{S}\)</span> 。</p><h4 id="数学定义">数学定义：</h4><p>用数学对迁移学习做定义如下：</p><p><img src="迁移学习在计算机视觉中的应用/image-20200220184841364.png" alt="image-20200220184841364" style="zoom:50%;"></p><p>其中最后一项都相等的话，那就是机器学习的研究。因此迁移学习实际上是对机器学习的一个扩展。其研究的范围非常广。</p><p>对定义做详细说明：</p><ul><li>当 <span class="math inline">\(\mathcal{D}_{S} \neq \mathcal{D}_{T}\)</span> ，<span class="math inline">\(\mathcal{T}_{S}=\mathcal{T}_{T}\)</span> 时，此类迁移学习问题是领域上的不同，可以来自：1. 两个domain的X特征空间不一样；or 2. 两个domain的边缘概率分布不一样。</li><li>当 <span class="math inline">\(\mathcal{D}_{S} = \mathcal{D}_{T}\)</span>，<span class="math inline">\(\mathcal{T}_{S} \neq \mathcal{T}_{T}\)</span> 时，此类迁移学习问题是在任务上的不同，可以来自：1. 两个task的 <span class="math inline">\(Y\)</span>的特征空间不一样；or 2. 两个task的条件概率不一样 <span class="math inline">\(P_{S}(y|x) \neq P_{T}(y|x)\)</span></li></ul><p>在论文中，以文本分类为例，分别对上面的两种情况举了两个例子：</p><blockquote><ul><li><p>case 1 corresponds to when the two sets of documents are described in different languages, and case 2 may correspond to when the source domain documents and the targetdomain documents focus on different topics. （当 <span class="math inline">\(\mathcal{D}_{S} \neq \mathcal{D}_{T}\)</span> ，<span class="math inline">\(\mathcal{T}_{S}=\mathcal{T}_{T}\)</span> 时）</p></li><li><p>case 1 corresponds to the situation where source domain has binary document classes, whereas the target domain has 10 classes to classify the documents to. Case 2 corresponds to the situation where the source and target documents are very unbalanced in terms of the userdefined classes. 当 <span class="math inline">\(\mathcal{D}_{S} = \mathcal{D}_{T}\)</span>，<span class="math inline">\(\mathcal{T}_{S} \neq \mathcal{T}_{T}\)</span> 时</p></li></ul></blockquote><p>根据此，还可以引入另一个非常热的方向：领域自适应问题实际上解决的是两个特征空间相同的数据集边缘分布不同(正常的数据集应该是独立同分布的)，用现有的数据去学习一个function来预测label的准确性。</p><p><img src="迁移学习在计算机视觉中的应用/image-20200221002110406.png" alt="image-20200221002110406" style="zoom: 67%;"></p><h4 id="迁移学习的研究领域">迁移学习的研究领域：</h4><p><img src="迁移学习在计算机视觉中的应用/image-20200221002150365.png" alt="image-20200221002150365" style="zoom:50%;"></p><h4 id="迁移学习的分类">迁移学习的分类</h4><p>Different Settings of Transfer Learning</p><figure><img src="迁移学习在计算机视觉中的应用/image-20200220225555100.png" alt="image-20200220225555100"><figcaption aria-hidden="true">image-20200220225555100</figcaption></figure><p><img src="迁移学习在计算机视觉中的应用/image-20200220225419493.png" alt="image-20200220225419493" style="zoom: 25%;"></p><h4 id="深度迁移学习的实现finetune">深度迁移学习的实现——finetune</h4><p>NIPS2014的一篇文章成功地将迁移学习引入到CNN卷积神经网络中，使用了大量地实验告诉我们AlexNet实际是如何对图像进行特征提取的。在了解了迁移学习的方式，通过对其中层中的参数进行调整，就可以得到更好的结果。</p><p>现在用于计算机视觉的finetune已经可以在很大程度上使得结果变得更好：可以参见 <a href>斯坦福CS231_transfer-learning_toturial</a></p><p>用pytorch上的一个例子来学习下迁移学习具体的实现方式。对迁移学习最主要的理解就是通过在大型的数据集上进行训练得到一个网络结构，重置其中的参数，经过微调用来满足在其他数据集上的学习。</p><p>接下来看代码：一个二分类问题，想训练一个resnet18，train_model是方便后面直接调用模型来train CNN这个model。需要输入的参数有model，criterion判别，optimizer优化器，epochs。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, criterion, optimizer, scheduler, num_epochs=<span class="number">25</span>)</span>:</span></span><br><span class="line">    since = time.time()</span><br><span class="line"></span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        print(<span class="string">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch, num_epochs - <span class="number">1</span>))</span><br><span class="line">        print(<span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Each epoch has a training and validation phase</span></span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                model.train()  <span class="comment"># Set model to training mode</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.eval()   <span class="comment"># Set model to evaluate mode</span></span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Iterate over data.</span></span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># zero the parameter gradients</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># forward</span></span><br><span class="line">                <span class="comment"># track history if only in train</span></span><br><span class="line">                <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">'train'</span>):</span><br><span class="line">                    outputs = model(inputs)</span><br><span class="line">                    _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># backward + optimize only if in training phase</span></span><br><span class="line">                    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># statistics</span></span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">                running_corrects += torch.sum(preds == labels.data)</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                scheduler.step()</span><br><span class="line"></span><br><span class="line">            epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">            epoch_acc = running_corrects.double() / dataset_sizes[phase]</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;'</span>.format(</span><br><span class="line">                phase, epoch_loss, epoch_acc))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># deep copy the model</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'val'</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    print(<span class="string">'Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s'</span>.format(</span><br><span class="line">        time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</span><br><span class="line">    print(<span class="string">'Best val Acc: &#123;:4f&#125;'</span>.format(best_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load best model weights</span></span><br><span class="line">    model.load_state_dict(best_model_wts)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h4 id="finetuning-the-convnet">Finetuning the convnet</h4><p>Load a pretrained model and reset final fully connected layer.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_ft = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">num_ftrs = model_ft.fc.in_features</span><br><span class="line"><span class="comment"># Here the size of each output sample is set to 2.</span></span><br><span class="line"><span class="comment"># Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).</span></span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that all parameters are being optimized</span></span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">                       num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure><p>model_conv是finetune的网络结构，单独设置了全连接层，最后输出两个label出来。在训练的过程中，conv层和fc层都会发生参数更新，等于在resneet18的参数基础上训练了整个网络，参数不是从初始开始更新的。</p><h4 id="convnet-as-fixed-feature-extractor">ConvNet as fixed feature extractor</h4><p>Here, we need to freeze all the network except the final layer.</p><p>训练的时候model_conv是不会发生变化的，只是fc层会发生变化。只训练了fc层的参数，conv层作为提取器不发生任何变化。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_conv = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters of newly constructed modules have requires_grad=True by default</span></span><br><span class="line">num_ftrs = model_conv.fc.in_features</span><br><span class="line">model_conv.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_conv = model_conv.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that only parameters of final layer are being optimized as</span></span><br><span class="line"><span class="comment"># opposed to before.</span></span><br><span class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_conv = train_model(model_conv, criterion, optimizer_conv,</span><br><span class="line">                         exp_lr_scheduler, num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure><p>最终结果是fc层的结果更精确，更快速。</p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transfer-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Meta-learning tutorial</title>
      <link href="2020/01/12/meta-learning/"/>
      <url>2020/01/12/meta-learning/</url>
      
        <content type="html"><![CDATA[<p>乘着假期期间，把meta-learning部分的内容全部过一遍。把最近的工作做一些简单的总结，顺便做了一份tutorial方便更多的同学对这个领域quickview。以及提供一部分代码进行实践，深入了解目前元学习领域解决的各种问题。以及非常流行的MAML和Reptile两种模型无关的元学习方法。并尝试从理论上对元学习模型进行解释，learn to learn对于few-shot，zero-shot，one-shot learning都有很好的应用，同时需要掌握的概念有迁移学习和表示学习的概念，了解神经网络学习的主要特征有哪些。本tutorial的参考资料都列到了最后的附录中。</p><h4 id="meta-learning的两类观点">Meta-learning的两类观点</h4><ul><li>一种是Mechanistic view: 可以读取整个数据集并预测新数据点的深度神经网络模型。训练该网络使用一个元数据集，该元数据集本身包含许多数据集，每个数据集用于不同的任务。此观点使实现meta-learning算法更加容易。</li><li>另一种是Probabilistic view：从一组（元训练meta-training）任务中提取先验信息，从而高效地学习新任务学习新任务时, 使用此先验和（小）训练集来推断最有可能的后验参数。此观点使理解元学习算法更加容易。</li></ul><h4 id="元学习的数学定义">元学习的数学定义</h4><p>通过学习一类任务，得到一个通用的参数<span class="math inline">\(\theta ^*\)</span>,通过固定参数<span class="math inline">\(\theta\)</span> ，在每个训练集的任务上用该task运用该task可以得到<span class="math inline">\(\phi\)</span>的训练参数。</p><h5 id="监督学习">监督学习：</h5><p>目标是:<span class="math inline">\(\arg \max _{\phi} \log p(\phi | D)\)</span> ，其中<span class="math inline">\(\mathcal{D}=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{k}, y_{k}\right)\right\}\)</span></p><p>其中，xi表示input，比如image，而y表示label。</p><p><span class="math inline">\(=\arg \max \log p(D | \phi)+\log p(\phi)\)</span></p><p><span class="math inline">\(=\arg \max \sum_{i} \log p\left(y_{i} | x_{i}, \phi\right)+\log p(\phi)\)</span></p><p>上式可以理解为寻找最大的<span class="math inline">\(\phi\)</span> , 使得学习出来的输出和真实的输出在相似度(极大似然)上最大。其中<span class="math inline">\(\log p(\phi)\)</span> 可以理解为在训练过程中加的正则项，为了防止训练的时候出现过拟合。</p><p>对于监督学习来说，存在的问题是</p><ul><li><p>如果想训练出一个非常强大的模型需要大量的数据，</p></li><li><p>对于一些任务来说label data 非常有限</p></li></ul><h5 id="元学习">元学习</h5><p>实际上，我们可以将数据集分成多个dataset，其中每一个dataset含有不同张图片，我们把这个新的dataset称作是meta-train数据集。</p><p><img src="\image\图片1.png" alt="图片1" style="zoom: 50%;"><img src="meta-learning/image-20200220032921498.png" alt="image-20200220032921498"></p><p>这样就是在每一个Di上都可以学习到需要的参数，最后综合在不同的dataset上学习到的参数作为<span class="math inline">\(\phi\)</span>使得其最大。数学上的定义就是：</p><p><span class="math inline">\(\arg \max _{\phi} \log p\left(\phi | \mathcal{D}, \mathcal{D}_{\text {meta-train }}\right)\)</span></p><p>meta-learning的问题就建立在这样的数据集的基础上，其中<span class="math inline">\(\mathcal{D}=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{k}, y_{k}\right)\right\}\)</span> <span class="math inline">\(\mathcal{D}_{\text {meta-train }}=\left\{\mathcal{D}_{1}, \ldots, \mathcal{D}_{n}\right\}\)</span> ，<span class="math inline">\(\mathcal{D}_{i}=\left\{\left(x_{1}^{i}, y_{1}^{i}\right), \ldots,\left(x_{k}^{i}, y_{k}^{i}\right)\right\}\)</span> .</p><p>因此, 假定<span class="math inline">\(\phi \perp \mathcal{D}_{\text {meta-train }} | \theta\)</span>, 表示$ $ 与 <span class="math inline">\(\mathcal{D}_{meta-train}\)</span> 和 $$ 是独立无关的, 可以推导出下面的式子</p><p><span class="math inline">\(\log p\left(\phi | \mathcal{D}, \mathcal{D}_{\mathrm{meta}-\mathrm{train}}\right)=\log \int_{\Theta} p(\phi | \mathcal{D}, \theta) p\left(\theta | \mathcal{D}_{\mathrm{meta}-\mathrm{train}}\right) d \theta\)</span></p><p><span class="math inline">\(\approx \log p\left(\phi | \mathcal{D}, \theta^{\star}\right)+\log p\left(\theta^{\star} | \mathcal{D}_{\text {meta-train }}\right)\)</span></p><p>第二行是找到一个平均值<span class="math inline">\(\theta^{\star}\)</span>对积分做一个平均得到。其中，第二项中不包含变量<span class="math inline">\(\phi\)</span>，为常数项，因此最后问题就变成了：</p><p><span class="math inline">\(\arg \max _{\phi} \log p\left(\phi | \mathcal{D}, \mathcal{D}_{\text {meta-train }}\right) \approx \arg \max _{\phi} \log p\left(\phi | \mathcal{D}, \theta^{\star}\right)\)</span></p><p>数学定义完全可以看明白。上式称为是<strong>元学习(meta-learning)需要解决的问题</strong>。要先找到一个<span class="math inline">\(\theta^{*}\)</span> ，再找到最好的<span class="math inline">\(\phi\)</span> 使得能够得到最大的p。</p><h5 id="元学习解决思路及术语介绍">元学习解决思路及术语介绍：</h5><p>要想使得上式能够得到求解，我们看一下如何进行完整的optimization 过程。优化问题可以分为两部分：</p><ul><li><p>1.第一部分：meta-learning，即<span class="math inline">\(\theta^{\star}=\arg \max _{\theta} \log p\left(\theta | \mathcal{D}_{\text {meta-train }}\right)\)</span></p></li><li><p>2.第二部分：adaptation. 即 <span class="math inline">\(\phi^{\star}=\arg \max _{\phi} \log p\left(\phi | \mathcal{D}^{\operatorname{tr}}, \theta^{\star}\right)\)</span></p></li></ul><p>我们将meta-train数据集进行完整的划分：</p><p><span class="math inline">\(\begin{array}{l}{\mathcal{D}_{\text {meta-train }}=\left\{\left(\mathcal{D}_{1}^{\text {tr }}, \mathcal{D}_{1}^{\text {ts }}\right), \ldots,\left(\mathcal{D}_{n}^{\text {tr }}, \mathcal{D}_{n}^{\text {ts }}\right)\right\}} \\ {D_{i}^{\text {tr }}=\left\{\left(x_{1}^{i}, y_{1}^{i}\right), \ldots,\left(x_{k}^{i}, y_{k}^{i}\right)\right\}} \\ {\mathcal{D}_{i}^{\text {ts }}=\left\{\left(x_{1}^{i}, y_{1}^{i}\right), \ldots,\left(x_{l}^{i}, y_{l}^{i}\right)\right\}}\end{array}\)</span></p><p>因此，可以将<span class="math inline">\(\phi^{\star}=\arg \max _{\phi} \log p\left(\phi | \mathcal{D}^{\operatorname{tr}}, \theta^{\star}\right)\)</span> 进行训练，得到的参数<span class="math inline">\(\phi\)</span> 即可以表示<span class="math inline">\(\phi^{\star}=f_{\theta^{\star}}\left(\mathcal{D}^{\mathrm{tr}}\right)\)</span> 下的<span class="math inline">\(\theta\)</span> ，通过进行更新参数<span class="math inline">\(\theta\)</span> ，验证在<span class="math inline">\(D^{tr}\)</span>得到结果。</p><p>以上可以认为是一个完整的训练过程，其训练<span class="math inline">\(\theta\)</span> 优化数学表达为：</p><p><span class="math inline">\(\theta^{\star}=\max _{\theta} \sum_{i=1}^{n} \log p\left(\phi_{i} | \mathcal{D}_{\mathcal{i}}^{(\mathrm{ts})}\right)\)</span></p><p>where <span class="math inline">\(\phi_{i}= f_{\theta}(D_{i}^{tr})\)</span></p><p>要在训练集上最优，使得<span class="math inline">\(\phi=f_{\theta}\left(\mathcal{D}^{\mathrm{tr}}\right)\)</span> 学习到的<span class="math inline">\(\theta\)</span>。</p><p>总结来说， 直观上<span class="math inline">\(\phi\)</span> 是在训练任务<span class="math inline">\(D^{tr}\)</span> 中学习得到的通用的经验，我们称它为<span class="math inline">\(f_{\theta}(D_{i}^{tr})\)</span> , 下标表示含有<span class="math inline">\(\theta\)</span> 这个参数，最终目的是要使得在<span class="math inline">\(D_{i}^{ts}\)</span> 上学习到最好的结果——p最大。这样就完成了在少样本测试的情况下给出了最好的分类性能。实现了meta-learning的目的。</p><p>对于数据集中的训练集和测试集，需要重新定义如下图：</p><figure><img src="meta-learning/image-20200220032944265.png" alt="image-20200220032944265"><figcaption aria-hidden="true">image-20200220032944265</figcaption></figure><p>note：meta-learning的范式也有一些与其非常相关的问题，比如多任务学习(multi-task learning)，或者超参数优化和automl问题。</p><h5 id="元学习常用的几类方法">元学习常用的几类方法：</h5><p>有四类方法，并不打算过多地展开讲，可以参见我另外一篇博客中使用模型无关的思想做了图像生成的工作，可以说对maml的理解比较深入了。也确实明白这是一个非常不错的工作。这里参考了ICML2019 tutorial的分类，也可以参考 https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html 的分类，主流方法还是MAML的各种改进。</p><h6 id="black-box-adaptation">1.Black-box adaptation</h6><figure><img src="meta-learning/image-20200220033004659.png" alt="image-20200220033004659"><figcaption aria-hidden="true">image-20200220033004659</figcaption></figure><h6 id="opamizaaon-based-inference">2.Opamizaaon-based inference</h6><p>其中，最经典的莫过于MAML。</p><p>其思路可以直接用下图表示，即让这个loss最小。深入的理解，需要从论文出发。</p><figure><img src="meta-learning/image-20200220033024315.png" alt="image-20200220033024315"><figcaption aria-hidden="true">image-20200220033024315</figcaption></figure><p><img src="\image\2020-02-16_00-46-53.jpg" alt="2020-02-16_00-46-53" style="zoom: 50%;"><img src="meta-learning/image-20200220033039938.png" alt="image-20200220033039938"></p><h6 id="non-parametric-methods">3.Non-parametric methods</h6><figure><img src="meta-learning/image-20200220033058725.png" alt="image-20200220033058725"><figcaption aria-hidden="true">image-20200220033058725</figcaption></figure><h6 id="bayesian-meta-learning">4.Bayesian meta-learning</h6><figure><img src="meta-learning/image-20200220033114720.png" alt="image-20200220033114720"><figcaption aria-hidden="true">image-20200220033114720</figcaption></figure><h5 id="元学习的应用">元学习的应用：</h5><p>参考ICML2019tutorial，在语言和图像上都有应用，包括现在比较火的reinforcement learning以及robtic learning。</p><p><img src="meta-learning/image-20200220033126015.png" alt="image-20200220033126015" style="zoom:50%;"></p><p><img src="meta-learning/image-20200220033139928.png" alt="image-20200220033139928">：</p><p>上述应用还是在比较传统的领域，实际上，更多的工作开始围绕着强化学习展开，由于强化学习方面的了解的不够深入，今后会继续了解imitation learning相关的工作成果。</p><p>很喜欢这个图：元学习的终极目标，作为本篇文章的结尾！</p><figure><img src="meta-learning/image-20200220033150499.png" alt="image-20200220033150499"><figcaption aria-hidden="true">image-20200220033150499</figcaption></figure><h4 id="参考资料">参考资料</h4><ul><li>ICML2019tutorial上的，Chelsea Finn和Sergey Levine做的lecuture。 https://sites.google.com/view/icml19metalearning (google site) https://youtube.videoken.com/embed/DijI4XrhqNo</li><li>hungyi-Lee，台大，http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pptx ppt, https://www.youtube.com/watch?v=EkAqYbpCYAc&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=33&amp;t=0s (video)</li><li>meta-learning比较好的overview: https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html</li><li>meta-transfer-learning: https://yyliu.net/files/meta-transfer-learning-slides.pdf (slides) https://github.com/yaoyao-liu/meta-transfer-learning (code)</li><li>few-shot image generation with reptile: https://github.com/LuEE-C/FIGR (code) https://arxiv.org/abs/1901.02199 (paper)</li><li>meta-transfer-learning-gan: https://yyliu.net/files/meta-transfer-learning-slides.pdf (pdf) https://arxiv.org/pdf/1812.02391 (paper)</li></ul>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> meta-learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
